{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC,LinearSVR\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import lightgbm as lgb\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "import datetime\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import tqdm\n",
    "import datetime\n",
    "import keras\n",
    "import numpy as np\n",
    "#test getWindowedValue\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv3D,Conv2D,MaxPooling1D,MaxPooling2D,MaxPooling3D,Conv1D\n",
    "from keras.layers import Lambda,Multiply ,TimeDistributed\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras import initializers\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.layers import Dense,Dropout,Flatten,Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam,SGD\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import Adam,RMSprop\n",
    "from keras.activations import tanh,relu\n",
    "#from keras.utils import multi_gpu_model\n",
    "from keras.layers.advanced_activations import PReLU,LeakyReLU,ELU\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,LSTM,GRU,Input,InputLayer,Activation, Input,Conv1D,MaxPooling1D,GlobalAveragePooling1D\n",
    "from keras.layers import Convolution2D, MaxPooling2D,TimeDistributed,Convolution1D,MaxPooling1D,concatenate, Average,BatchNormalization,GlobalMaxPool1D\n",
    "from keras.utils import np_utils\n",
    "from keras import losses\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from IPython.display import SVG\n",
    "\n",
    "import keras\n",
    "#from fastText import train_unsupervised\n",
    "\n",
    "LengthOfInputSequences=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileTrain='./train.txt'\n",
    "fileDev='./dev.txt'\n",
    "fileTest='./dfs-test.txt'\n",
    "fileTest='../vardial2018gold/DFS/dfs-gold.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n",
    "    # Clean the text, with the option to remove stop_words and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    #text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\0k \", \"0000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\"china\", \"China\", text)\n",
    "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\"quora\", \"Quora\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"America\", text)\n",
    "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
    "    text = re.sub(r\"Method\", \"method\", text)\n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(fname):\n",
    "    return pd.read_csv(fname, sep='\\t',header=-1)\n",
    "\n",
    "\n",
    "#22ste\n",
    "\n",
    "def fitlerLine(lin):\n",
    "    lin=lin.lower()\n",
    "    for k in ['.',',','?',\"'s\",\"'t\",',']:\n",
    "        lin=lin.replace(k,' '+k+' ')\n",
    "    #for k in ['.',',','?','!']:\n",
    "    #    lin=lin.replace(k,' ')\n",
    "    while '  ' in lin:\n",
    "        lin=lin.replace('  ',' ')\n",
    "    return lin.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=loadData(fileTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                                         0    1\n",
       "0       Dat zeg ik liever niet. Ik moet een beslissing...  BEL\n",
       "1       De problemen van de westerse wereld zijn in ve...  BEL\n",
       "2       Uw werkgever krijgt 't moeilijk met mogelijke ...  BEL\n",
       "3       Kijk eens aan. Die man schildert ten behoeve v...  BEL\n",
       "4       Nooit. Ik weet het niet. Misschien was je me w...  BEL\n",
       "5       Je staat voor schut, en je verknalt je hele le...  BEL\n",
       "6       All in. Dit verbaast me. Perrault re raisede M...  BEL\n",
       "7       Broadway is een prachtig Engels dorpje bij de ...  BEL\n",
       "8       Maar ze was niet thuis. Wie niet? Alison. Mijn...  BEL\n",
       "9       Gaat hij goed, de Story? Nee? Ik denk dat het ...  BEL\n",
       "10      Hebben we hun lijnen? Neem ze af. Waar is de v...  BEL\n",
       "11      Ik heb uren op de grond gelegen voor de poten ...  BEL\n",
       "12      Niet bij mij. Bij mij schept hij niet op. Toch...  BEL\n",
       "13      De zwelling is hier minder. Al die vloeistof v...  BEL\n",
       "14      Ga jij maar. Ik blijf hier wachten tot ze bijk...  BEL\n",
       "15      Dat weet je niet omdat ik het je nog niet heb ...  BEL\n",
       "16      Clarence, blijf je terugtrekken. Daaronder. Ik...  BEL\n",
       "17      Ik ben teruggekeerd, ik ben zelfs nooit binnen...  BEL\n",
       "18      Wat voor zoete cr medrankjes heeft u? Bedankt ...  BEL\n",
       "19      Ik heb al te veel dode mensen gezien. Ik wil h...  BEL\n",
       "20      Maar ik zit hopeloos vast. Ik heb geen werk en...  BEL\n",
       "21      Er zijn er altijd die daar misbruik van maken ...  BEL\n",
       "22      Ik stelde mijn idee voor aan Andy Dranatelli m...  BEL\n",
       "23      Ze is met mij uitgegaan. Wacht, ik maak u een ...  BEL\n",
       "24      We legden takken op hem, legden hem neer en na...  BEL\n",
       "25      De jongens verwachten een warme douche. Hallo,...  BEL\n",
       "26      Volgens het handboek is alles in orde als de v...  BEL\n",
       "27      Dat blokkeert al mijn chakra's. Ik vind het zo...  BEL\n",
       "28      Ik wil die training gebruiken om een mijl te l...  BEL\n",
       "29      Dana was lesbisch. Excuseer me. Werden we expl...  BEL\n",
       "...                                                   ...  ...\n",
       "299970  Dat is ze niet. Waarom dan dat verbandje? Is d...  DUT\n",
       "299971  En daarom snoei ik nu de heg. Ik ga alleen met...  DUT\n",
       "299972  Hij heeft bijna een nieuw record. Het tweede p...  DUT\n",
       "299973  De weersomstandigheden ook. Geduld, wachten, e...  DUT\n",
       "299974  We gaan geen virussen verspreiden. Wat vind je...  DUT\n",
       "299975  Hij was na een paar minuten dood. Heel erg bed...  DUT\n",
       "299976  Met zulke prachtige ingredi nten verwacht Marc...  DUT\n",
       "299977  We blijven gewoon schrijvers. Die blonde doet ...  DUT\n",
       "299978  We beginnen zo open en spontaan, als individu....  DUT\n",
       "299979  Je weet het echt niet meer? Nee, echt niet. Te...  DUT\n",
       "299980  Vangen. , hoe gaat het? Ryan? Wat is er? Ben j...  DUT\n",
       "299981  Het is heel dun. Het loopt gewoon helemaal uit...  DUT\n",
       "299982  Je moeder heeft gelijk, het is je eigen schuld...  DUT\n",
       "299983  U hebt het vorige week gekocht? Ja, maar ze ha...  DUT\n",
       "299984  We moeten naar een cyclus toe. Omstreeks 8 uur...  DUT\n",
       "299985  Gingen jullie weleens een stukje varen? In twe...  DUT\n",
       "299986  Nu ben ik de eiwitten kwijt die het hadden moe...  DUT\n",
       "299987  Uw stem in ruil voor de helft van Trini's leve...  DUT\n",
       "299988  Een ondernemende Duitse journalist had het zie...  DUT\n",
       "299989  De agenten achtervolgen de rechter en filmen h...  DUT\n",
       "299990  Paula heeft Ragolia nooit huur betaald. En Rag...  DUT\n",
       "299991  Het thema van deze week is superhelden. Die me...  DUT\n",
       "299992  Ze komt heel oprecht over. , de kleinzoon van ...  DUT\n",
       "299993  Alleen omdat ik in een stoel terechtkwam. We h...  DUT\n",
       "299994  Ik heb niets kunnen vinden. Ga naar de site va...  DUT\n",
       "299995  Het dessert. Mooi. Dat ziet er mooi uit. STEM ...  DUT\n",
       "299996  Jullie hebben de gevarenzone betreden. Twee va...  DUT\n",
       "299997  En dat weet u. Vuile smeerlap. , kent u het Pe...  DUT\n",
       "299998  Aan zijn titel zijn landrechten verbonden. Hij...  DUT\n",
       "299999  Zo. Niet schrikken. Ik bedek de kaas met een f...  DUT\n",
       "\n",
       "[300000 rows x 2 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainxRaw=list(map(fitlerLine,df[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dat zeg ik liever niet . ik moet een beslissing nemen voor het welzijn van dit meisje . dimeola zal een exorcisme zijn psychisch zieke dochter schaden . is dat waar of niet ?', 'de problemen van de westerse wereld zijn in veel opzichten anders dan 2 . 000 jaar geleden . maar onze burgerplicht blijft dezelfde . ons erfgoed verdedigen tegen wie het wil verdelen en vernietigen .']\n"
     ]
    }
   ],
   "source": [
    "print(trainxRaw[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000 ['BEL' 'BEL' 'BEL' 'BEL' 'BEL' 'BEL' 'BEL' 'BEL' 'BEL' 'BEL']\n"
     ]
    }
   ],
   "source": [
    "trainyRaw=df[1].values\n",
    "print(len(trainyRaw),trainyRaw[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500 ['BEL' 'BEL' 'BEL' 'BEL' 'BEL' 'BEL' 'BEL' 'BEL' 'BEL' 'BEL']\n"
     ]
    }
   ],
   "source": [
    "devdata=loadData(fileDev)\n",
    "devxRaw=list(map(fitlerLine,devdata[0]))\n",
    "devyRaw=devdata[1].values\n",
    "print(len(devxRaw))\n",
    "print(len(devyRaw),devyRaw[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdata=loadData(fileTest)\n",
    "testy=testdata[1].values\n",
    "testxRaw=list(map(fitlerLine,testdata[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waar ze biologische producten verbouwen . ze geniet van het buitenleven en haar dieren . tim van 30 woont in hilversum , is vrijgezel . in het dagelijks leven is hij online marketeer en hij besteed elke vrije minuut aan zijn boy toys .', 'en dan ga je mee . als ik echt vond dat het tijd werd voor iets anders , zou ik dat zeggen . als je een relatie hebt moet je daarover kunnen praten .', 'ik ben niet op zoek naar acteurs die goed spelen , maar eerder acteurs die in de huid kruipen van het personage . ik stel zware eisen aan de acteurs . ze moeten zich inleven ,', \"olds escorteerde bommenwerpers boven europa . een keer werden 52 b 17 's neergeschoten door messerschmitt 's in drie minuten . het was vreselijk voor olds om dit te aanzien . de b 17 's waren machteloos .\", 'helemaal mee eens . en ze hebben mij uitgekotst . dus ik doe mee . ik wil ze best op straat gooien . je twijfelt toch niet aan mijn loyaliteit ? hoe zit het met eric ?', 'wat naar , zeg . waar lijdt ze aan ? het is een natuurlijke dood . natuurlijk ? hoe oud is die meid ? ze is 88 jaar . doe je het met een vrouw van 88 ?', \"al die nep glimlachjes en dat 'alles goed , debbie ? ' ze pakten mijn kind van me af , en toen zat ik weer in mijn rotflatje . dat laat ik me niet nog eens gebeuren .\", 'wij willen weten wie je bent , waar je vandaan komt en wat je doet . dit gaat niet over jou , maar over ons . onze favoriete clip deze week waren de skaters van arick arthur .', 'ik hoop dat iedereen met dit probleem het niet opgeeft . we zijn ook niet opeens zo geworden . maar ik ga de goede kant op . het lukt wel . ze behaalt kleine overwinningen .', 'zet er dus een punt achter . donna , wat is er ? je kunt me alles vertellen . wat is er aan de hand ? nee , er is niets hoor . jawel , er is iets .']\n"
     ]
    }
   ],
   "source": [
    "testdata.describe\n",
    "print(testxRaw[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from langid.langid import LanguageIdentifier, model\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for r in devxRaw[10:]:\n",
    "    print(r,identifier.classify(r))\n",
    "    break;\n",
    "import langid\n",
    "langid.set_languages(['de','fr','it','en'])\n",
    "idscores=[langid.classify(r) for r in devxRaw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(([s[0] for s in idscores]))\n",
    "print(devyRaw)\n",
    "#print(([s[1] for s in idscores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "a='1 2  3'\n",
    "print(a.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mytoken(lin):\n",
    "    return lin.split()\n",
    "tfv = TfidfVectorizer(min_df=2,use_idf=1,\n",
    "                      smooth_idf=1,ngram_range=(1,3),\n",
    "                     )#analyzer='char_wb') #,stop_words='english')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tsvd=TruncatedSVD(n_components=400,random_state=2016)   # this gives similar results as to Semeval , try n_components=600\n",
    "#trainx=tsvd.fit_transform(trainx)\n",
    "#evalx=tsvd.transform(evalx)\n",
    "#clf=LinearDiscriminantAnalysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '00 24', '00 28', '00 29', '00 36', '00 52', '00 55', '00 de', '00 en', '00 is']\n",
      "['zwommen', 'zwommen in', 'zwommen met', 'zwommen met de', 'zx', 'zybutz', 'zz', 'zz top', 'zzp', 'zzyzx']\n",
      "(300000, 1449159) (500, 1449159)\n"
     ]
    }
   ],
   "source": [
    "trainx=tfv.fit_transform(trainxRaw)\n",
    "evalx=tfv.transform(devxRaw)\n",
    "print (tfv.get_feature_names()[:10])\n",
    "print (tfv.get_feature_names()[-10:])\n",
    "print (trainx.shape,evalx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainy=trainyRaw\n",
    "evaly=devyRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('features.txt','w') as fout:\n",
    "    fout.writelines('\\n'.join(tfv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[170  80]\n",
      " [ 76 174]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        BEL       0.69      0.68      0.69       250\n",
      "        DUT       0.69      0.70      0.69       250\n",
      "\n",
      "avg / total       0.69      0.69      0.69       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf=LinearSVC()\n",
    "\n",
    "clf.fit(trainx,trainy)\n",
    "predictValue=clf.predict(evalx)\n",
    "print(confusion_matrix(evaly,predictValue))\n",
    "print(classification_report(evaly,predictValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n",
      "300500\n"
     ]
    }
   ],
   "source": [
    "#tfidf learned from train and dev\n",
    "tfv = TfidfVectorizer(min_df=2,use_idf=1,\n",
    "                      smooth_idf=1,ngram_range=(1,3),\n",
    "                     )\n",
    "tempTrain=trainxRaw.copy()\n",
    "print(len(tempTrain))\n",
    "tempTrain.extend(devxRaw)\n",
    "print(len(tempTrain))\n",
    "tfv.fit(tempTrain)\n",
    "tempTrain=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1453534\n"
     ]
    }
   ],
   "source": [
    "#exame tfv features\n",
    "print(len(tfv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainx=tfv.transform(trainxRaw)\n",
    "evalx=tfv.transform(devxRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1449159)\n"
     ]
    }
   ],
   "source": [
    "#load test file\n",
    "fileTest='./dfs-test.txt'\n",
    "testdata=loadData(fileTest)\n",
    "testxRaw=list(map(fitlerLine,testdata[0]))\n",
    "testx=tfv.transform(testxRaw)\n",
    "print(testx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.0'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.69054\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.688307\n",
      "[3]\tvalid_0's binary_logloss: 0.686218\n",
      "[4]\tvalid_0's binary_logloss: 0.684618\n",
      "[5]\tvalid_0's binary_logloss: 0.683382\n",
      "[6]\tvalid_0's binary_logloss: 0.682077\n",
      "[7]\tvalid_0's binary_logloss: 0.680795\n",
      "[8]\tvalid_0's binary_logloss: 0.679673\n",
      "[9]\tvalid_0's binary_logloss: 0.678634\n",
      "[10]\tvalid_0's binary_logloss: 0.677843\n",
      "[11]\tvalid_0's binary_logloss: 0.676642\n",
      "[12]\tvalid_0's binary_logloss: 0.675774\n",
      "[13]\tvalid_0's binary_logloss: 0.675065\n",
      "[14]\tvalid_0's binary_logloss: 0.674461\n",
      "[15]\tvalid_0's binary_logloss: 0.673586\n",
      "[16]\tvalid_0's binary_logloss: 0.67253\n",
      "[17]\tvalid_0's binary_logloss: 0.671906\n",
      "[18]\tvalid_0's binary_logloss: 0.670814\n",
      "[19]\tvalid_0's binary_logloss: 0.670111\n",
      "[20]\tvalid_0's binary_logloss: 0.669062\n",
      "[21]\tvalid_0's binary_logloss: 0.668262\n",
      "[22]\tvalid_0's binary_logloss: 0.667294\n",
      "[23]\tvalid_0's binary_logloss: 0.666746\n",
      "[24]\tvalid_0's binary_logloss: 0.666025\n",
      "[25]\tvalid_0's binary_logloss: 0.665433\n",
      "[26]\tvalid_0's binary_logloss: 0.664808\n",
      "[27]\tvalid_0's binary_logloss: 0.664022\n",
      "[28]\tvalid_0's binary_logloss: 0.663328\n",
      "[29]\tvalid_0's binary_logloss: 0.662781\n",
      "[30]\tvalid_0's binary_logloss: 0.661991\n",
      "[31]\tvalid_0's binary_logloss: 0.66165\n",
      "[32]\tvalid_0's binary_logloss: 0.661131\n",
      "[33]\tvalid_0's binary_logloss: 0.660719\n",
      "[34]\tvalid_0's binary_logloss: 0.660206\n",
      "[35]\tvalid_0's binary_logloss: 0.659898\n",
      "[36]\tvalid_0's binary_logloss: 0.659365\n",
      "[37]\tvalid_0's binary_logloss: 0.658924\n",
      "[38]\tvalid_0's binary_logloss: 0.658396\n",
      "[39]\tvalid_0's binary_logloss: 0.657787\n",
      "[40]\tvalid_0's binary_logloss: 0.657602\n",
      "[41]\tvalid_0's binary_logloss: 0.657004\n",
      "[42]\tvalid_0's binary_logloss: 0.656428\n",
      "[43]\tvalid_0's binary_logloss: 0.655987\n",
      "[44]\tvalid_0's binary_logloss: 0.655428\n",
      "[45]\tvalid_0's binary_logloss: 0.654969\n",
      "[46]\tvalid_0's binary_logloss: 0.654509\n",
      "[47]\tvalid_0's binary_logloss: 0.654257\n",
      "[48]\tvalid_0's binary_logloss: 0.65379\n",
      "[49]\tvalid_0's binary_logloss: 0.653236\n",
      "[50]\tvalid_0's binary_logloss: 0.65296\n",
      "[51]\tvalid_0's binary_logloss: 0.652778\n",
      "[52]\tvalid_0's binary_logloss: 0.652523\n",
      "[53]\tvalid_0's binary_logloss: 0.652166\n",
      "[54]\tvalid_0's binary_logloss: 0.651787\n",
      "[55]\tvalid_0's binary_logloss: 0.651496\n",
      "[56]\tvalid_0's binary_logloss: 0.651273\n",
      "[57]\tvalid_0's binary_logloss: 0.651107\n",
      "[58]\tvalid_0's binary_logloss: 0.650771\n",
      "[59]\tvalid_0's binary_logloss: 0.650345\n",
      "[60]\tvalid_0's binary_logloss: 0.649941\n",
      "[61]\tvalid_0's binary_logloss: 0.649754\n",
      "[62]\tvalid_0's binary_logloss: 0.649805\n",
      "[63]\tvalid_0's binary_logloss: 0.649463\n",
      "[64]\tvalid_0's binary_logloss: 0.649442\n",
      "[65]\tvalid_0's binary_logloss: 0.64929\n",
      "[66]\tvalid_0's binary_logloss: 0.649177\n",
      "[67]\tvalid_0's binary_logloss: 0.64896\n",
      "[68]\tvalid_0's binary_logloss: 0.648757\n",
      "[69]\tvalid_0's binary_logloss: 0.648482\n",
      "[70]\tvalid_0's binary_logloss: 0.648158\n",
      "[71]\tvalid_0's binary_logloss: 0.648076\n",
      "[72]\tvalid_0's binary_logloss: 0.647881\n",
      "[73]\tvalid_0's binary_logloss: 0.647807\n",
      "[74]\tvalid_0's binary_logloss: 0.64761\n",
      "[75]\tvalid_0's binary_logloss: 0.647364\n",
      "[76]\tvalid_0's binary_logloss: 0.647193\n",
      "[77]\tvalid_0's binary_logloss: 0.646932\n",
      "[78]\tvalid_0's binary_logloss: 0.646892\n",
      "[79]\tvalid_0's binary_logloss: 0.646714\n",
      "[80]\tvalid_0's binary_logloss: 0.646483\n",
      "[81]\tvalid_0's binary_logloss: 0.646379\n",
      "[82]\tvalid_0's binary_logloss: 0.646197\n",
      "[83]\tvalid_0's binary_logloss: 0.646071\n",
      "[84]\tvalid_0's binary_logloss: 0.645938\n",
      "[85]\tvalid_0's binary_logloss: 0.645675\n",
      "[86]\tvalid_0's binary_logloss: 0.645533\n",
      "[87]\tvalid_0's binary_logloss: 0.645434\n",
      "[88]\tvalid_0's binary_logloss: 0.645267\n",
      "[89]\tvalid_0's binary_logloss: 0.645271\n",
      "[90]\tvalid_0's binary_logloss: 0.645258\n",
      "[91]\tvalid_0's binary_logloss: 0.64518\n",
      "[92]\tvalid_0's binary_logloss: 0.644912\n",
      "[93]\tvalid_0's binary_logloss: 0.644732\n",
      "[94]\tvalid_0's binary_logloss: 0.644597\n",
      "[95]\tvalid_0's binary_logloss: 0.644536\n",
      "[96]\tvalid_0's binary_logloss: 0.644366\n",
      "[97]\tvalid_0's binary_logloss: 0.644224\n",
      "[98]\tvalid_0's binary_logloss: 0.644123\n",
      "[99]\tvalid_0's binary_logloss: 0.644091\n",
      "[100]\tvalid_0's binary_logloss: 0.644081\n",
      "[101]\tvalid_0's binary_logloss: 0.643841\n",
      "[102]\tvalid_0's binary_logloss: 0.643659\n",
      "[103]\tvalid_0's binary_logloss: 0.643619\n",
      "[104]\tvalid_0's binary_logloss: 0.64368\n",
      "[105]\tvalid_0's binary_logloss: 0.643647\n",
      "[106]\tvalid_0's binary_logloss: 0.643475\n",
      "[107]\tvalid_0's binary_logloss: 0.64331\n",
      "[108]\tvalid_0's binary_logloss: 0.643174\n",
      "[109]\tvalid_0's binary_logloss: 0.643119\n",
      "[110]\tvalid_0's binary_logloss: 0.643006\n",
      "[111]\tvalid_0's binary_logloss: 0.642916\n",
      "[112]\tvalid_0's binary_logloss: 0.642879\n",
      "[113]\tvalid_0's binary_logloss: 0.642813\n",
      "[114]\tvalid_0's binary_logloss: 0.642671\n",
      "[115]\tvalid_0's binary_logloss: 0.642495\n",
      "[116]\tvalid_0's binary_logloss: 0.642416\n",
      "[117]\tvalid_0's binary_logloss: 0.642346\n",
      "[118]\tvalid_0's binary_logloss: 0.642191\n",
      "[119]\tvalid_0's binary_logloss: 0.641983\n",
      "[120]\tvalid_0's binary_logloss: 0.641909\n",
      "[121]\tvalid_0's binary_logloss: 0.641833\n",
      "[122]\tvalid_0's binary_logloss: 0.641785\n",
      "[123]\tvalid_0's binary_logloss: 0.641874\n",
      "[124]\tvalid_0's binary_logloss: 0.641738\n",
      "[125]\tvalid_0's binary_logloss: 0.641566\n",
      "[126]\tvalid_0's binary_logloss: 0.641448\n",
      "[127]\tvalid_0's binary_logloss: 0.641366\n",
      "[128]\tvalid_0's binary_logloss: 0.641148\n",
      "[129]\tvalid_0's binary_logloss: 0.641202\n",
      "[130]\tvalid_0's binary_logloss: 0.641205\n",
      "[131]\tvalid_0's binary_logloss: 0.641218\n",
      "[132]\tvalid_0's binary_logloss: 0.641237\n",
      "[133]\tvalid_0's binary_logloss: 0.641186\n",
      "[134]\tvalid_0's binary_logloss: 0.641067\n",
      "[135]\tvalid_0's binary_logloss: 0.641004\n",
      "[136]\tvalid_0's binary_logloss: 0.641072\n",
      "[137]\tvalid_0's binary_logloss: 0.641096\n",
      "[138]\tvalid_0's binary_logloss: 0.641232\n",
      "[139]\tvalid_0's binary_logloss: 0.6412\n",
      "[140]\tvalid_0's binary_logloss: 0.641119\n",
      "[141]\tvalid_0's binary_logloss: 0.641052\n",
      "[142]\tvalid_0's binary_logloss: 0.64106\n",
      "[143]\tvalid_0's binary_logloss: 0.640927\n",
      "[144]\tvalid_0's binary_logloss: 0.640811\n",
      "[145]\tvalid_0's binary_logloss: 0.640765\n",
      "[146]\tvalid_0's binary_logloss: 0.64073\n",
      "[147]\tvalid_0's binary_logloss: 0.640648\n",
      "[148]\tvalid_0's binary_logloss: 0.640569\n",
      "[149]\tvalid_0's binary_logloss: 0.640432\n",
      "[150]\tvalid_0's binary_logloss: 0.640431\n",
      "[151]\tvalid_0's binary_logloss: 0.64037\n",
      "[152]\tvalid_0's binary_logloss: 0.64033\n",
      "[153]\tvalid_0's binary_logloss: 0.640247\n",
      "[154]\tvalid_0's binary_logloss: 0.640268\n",
      "[155]\tvalid_0's binary_logloss: 0.640212\n",
      "[156]\tvalid_0's binary_logloss: 0.640256\n",
      "[157]\tvalid_0's binary_logloss: 0.640364\n",
      "[158]\tvalid_0's binary_logloss: 0.640305\n",
      "[159]\tvalid_0's binary_logloss: 0.640076\n",
      "[160]\tvalid_0's binary_logloss: 0.639978\n",
      "[161]\tvalid_0's binary_logloss: 0.639922\n",
      "[162]\tvalid_0's binary_logloss: 0.639725\n",
      "[163]\tvalid_0's binary_logloss: 0.639667\n",
      "[164]\tvalid_0's binary_logloss: 0.639638\n",
      "[165]\tvalid_0's binary_logloss: 0.639577\n",
      "[166]\tvalid_0's binary_logloss: 0.639637\n",
      "[167]\tvalid_0's binary_logloss: 0.639616\n",
      "[168]\tvalid_0's binary_logloss: 0.639698\n",
      "[169]\tvalid_0's binary_logloss: 0.639696\n",
      "[170]\tvalid_0's binary_logloss: 0.639684\n",
      "[171]\tvalid_0's binary_logloss: 0.639666\n",
      "[172]\tvalid_0's binary_logloss: 0.639614\n",
      "[173]\tvalid_0's binary_logloss: 0.639464\n",
      "[174]\tvalid_0's binary_logloss: 0.639491\n",
      "[175]\tvalid_0's binary_logloss: 0.639454\n",
      "[176]\tvalid_0's binary_logloss: 0.639457\n",
      "[177]\tvalid_0's binary_logloss: 0.639366\n",
      "[178]\tvalid_0's binary_logloss: 0.639308\n",
      "[179]\tvalid_0's binary_logloss: 0.63946\n",
      "[180]\tvalid_0's binary_logloss: 0.639465\n",
      "[181]\tvalid_0's binary_logloss: 0.639383\n",
      "[182]\tvalid_0's binary_logloss: 0.639291\n",
      "[183]\tvalid_0's binary_logloss: 0.639178\n",
      "[184]\tvalid_0's binary_logloss: 0.639173\n",
      "[185]\tvalid_0's binary_logloss: 0.639039\n",
      "[186]\tvalid_0's binary_logloss: 0.638955\n",
      "[187]\tvalid_0's binary_logloss: 0.638946\n",
      "[188]\tvalid_0's binary_logloss: 0.639038\n",
      "[189]\tvalid_0's binary_logloss: 0.639037\n",
      "[190]\tvalid_0's binary_logloss: 0.638924\n",
      "[191]\tvalid_0's binary_logloss: 0.638933\n",
      "[192]\tvalid_0's binary_logloss: 0.638936\n",
      "[193]\tvalid_0's binary_logloss: 0.63884\n",
      "[194]\tvalid_0's binary_logloss: 0.638793\n",
      "[195]\tvalid_0's binary_logloss: 0.638828\n",
      "[196]\tvalid_0's binary_logloss: 0.638903\n",
      "[197]\tvalid_0's binary_logloss: 0.638994\n",
      "[198]\tvalid_0's binary_logloss: 0.639062\n",
      "[199]\tvalid_0's binary_logloss: 0.639047\n",
      "[200]\tvalid_0's binary_logloss: 0.639118\n",
      "[201]\tvalid_0's binary_logloss: 0.639112\n",
      "[202]\tvalid_0's binary_logloss: 0.639032\n",
      "[203]\tvalid_0's binary_logloss: 0.639035\n",
      "[204]\tvalid_0's binary_logloss: 0.639002\n",
      "[205]\tvalid_0's binary_logloss: 0.638993\n",
      "[206]\tvalid_0's binary_logloss: 0.638991\n",
      "[207]\tvalid_0's binary_logloss: 0.638937\n",
      "[208]\tvalid_0's binary_logloss: 0.638851\n",
      "[209]\tvalid_0's binary_logloss: 0.638853\n",
      "[210]\tvalid_0's binary_logloss: 0.638793\n",
      "[211]\tvalid_0's binary_logloss: 0.638797\n",
      "[212]\tvalid_0's binary_logloss: 0.63872\n",
      "[213]\tvalid_0's binary_logloss: 0.638634\n",
      "[214]\tvalid_0's binary_logloss: 0.638572\n",
      "[215]\tvalid_0's binary_logloss: 0.638532\n",
      "[216]\tvalid_0's binary_logloss: 0.6384\n",
      "[217]\tvalid_0's binary_logloss: 0.638406\n",
      "[218]\tvalid_0's binary_logloss: 0.638434\n",
      "[219]\tvalid_0's binary_logloss: 0.638536\n",
      "[220]\tvalid_0's binary_logloss: 0.638436\n",
      "[221]\tvalid_0's binary_logloss: 0.638469\n",
      "[222]\tvalid_0's binary_logloss: 0.63845\n",
      "[223]\tvalid_0's binary_logloss: 0.638392\n",
      "[224]\tvalid_0's binary_logloss: 0.638483\n",
      "[225]\tvalid_0's binary_logloss: 0.6385\n",
      "[226]\tvalid_0's binary_logloss: 0.638507\n",
      "[227]\tvalid_0's binary_logloss: 0.638529\n",
      "[228]\tvalid_0's binary_logloss: 0.638499\n",
      "[229]\tvalid_0's binary_logloss: 0.63842\n",
      "[230]\tvalid_0's binary_logloss: 0.638419\n",
      "[231]\tvalid_0's binary_logloss: 0.638506\n",
      "[232]\tvalid_0's binary_logloss: 0.638423\n",
      "[233]\tvalid_0's binary_logloss: 0.638339\n",
      "[234]\tvalid_0's binary_logloss: 0.638237\n",
      "[235]\tvalid_0's binary_logloss: 0.638295\n",
      "[236]\tvalid_0's binary_logloss: 0.638184\n",
      "[237]\tvalid_0's binary_logloss: 0.638212\n",
      "[238]\tvalid_0's binary_logloss: 0.638308\n",
      "[239]\tvalid_0's binary_logloss: 0.638266\n",
      "[240]\tvalid_0's binary_logloss: 0.638213\n",
      "[241]\tvalid_0's binary_logloss: 0.638185\n",
      "[242]\tvalid_0's binary_logloss: 0.638251\n",
      "[243]\tvalid_0's binary_logloss: 0.638308\n",
      "[244]\tvalid_0's binary_logloss: 0.638289\n",
      "[245]\tvalid_0's binary_logloss: 0.638303\n",
      "[246]\tvalid_0's binary_logloss: 0.638261\n",
      "[247]\tvalid_0's binary_logloss: 0.638214\n",
      "[248]\tvalid_0's binary_logloss: 0.638147\n",
      "[249]\tvalid_0's binary_logloss: 0.638145\n",
      "[250]\tvalid_0's binary_logloss: 0.638098\n",
      "[251]\tvalid_0's binary_logloss: 0.63799\n",
      "[252]\tvalid_0's binary_logloss: 0.637936\n",
      "[253]\tvalid_0's binary_logloss: 0.63791\n",
      "[254]\tvalid_0's binary_logloss: 0.637934\n",
      "[255]\tvalid_0's binary_logloss: 0.637887\n",
      "[256]\tvalid_0's binary_logloss: 0.637824\n",
      "[257]\tvalid_0's binary_logloss: 0.637767\n",
      "[258]\tvalid_0's binary_logloss: 0.637688\n",
      "[259]\tvalid_0's binary_logloss: 0.637701\n",
      "[260]\tvalid_0's binary_logloss: 0.637711\n",
      "[261]\tvalid_0's binary_logloss: 0.637702\n",
      "[262]\tvalid_0's binary_logloss: 0.637651\n",
      "[263]\tvalid_0's binary_logloss: 0.637649\n",
      "[264]\tvalid_0's binary_logloss: 0.637787\n",
      "[265]\tvalid_0's binary_logloss: 0.637776\n",
      "[266]\tvalid_0's binary_logloss: 0.637772\n",
      "[267]\tvalid_0's binary_logloss: 0.637812\n",
      "[268]\tvalid_0's binary_logloss: 0.637811\n",
      "[269]\tvalid_0's binary_logloss: 0.637807\n",
      "[270]\tvalid_0's binary_logloss: 0.637865\n",
      "[271]\tvalid_0's binary_logloss: 0.637892\n",
      "[272]\tvalid_0's binary_logloss: 0.637977\n",
      "[273]\tvalid_0's binary_logloss: 0.63783\n",
      "[274]\tvalid_0's binary_logloss: 0.637867\n",
      "[275]\tvalid_0's binary_logloss: 0.637851\n",
      "[276]\tvalid_0's binary_logloss: 0.637808\n",
      "[277]\tvalid_0's binary_logloss: 0.637691\n",
      "[278]\tvalid_0's binary_logloss: 0.637656\n",
      "[279]\tvalid_0's binary_logloss: 0.637647\n",
      "[280]\tvalid_0's binary_logloss: 0.637604\n",
      "[281]\tvalid_0's binary_logloss: 0.637555\n",
      "[282]\tvalid_0's binary_logloss: 0.637535\n",
      "[283]\tvalid_0's binary_logloss: 0.637419\n",
      "[284]\tvalid_0's binary_logloss: 0.63725\n",
      "[285]\tvalid_0's binary_logloss: 0.637154\n",
      "[286]\tvalid_0's binary_logloss: 0.637167\n",
      "[287]\tvalid_0's binary_logloss: 0.637189\n",
      "[288]\tvalid_0's binary_logloss: 0.637163\n",
      "[289]\tvalid_0's binary_logloss: 0.637146\n",
      "[290]\tvalid_0's binary_logloss: 0.637037\n",
      "[291]\tvalid_0's binary_logloss: 0.636944\n",
      "[292]\tvalid_0's binary_logloss: 0.63693\n",
      "[293]\tvalid_0's binary_logloss: 0.637057\n",
      "[294]\tvalid_0's binary_logloss: 0.637113\n",
      "[295]\tvalid_0's binary_logloss: 0.63709\n",
      "[296]\tvalid_0's binary_logloss: 0.637112\n",
      "[297]\tvalid_0's binary_logloss: 0.637087\n",
      "[298]\tvalid_0's binary_logloss: 0.637034\n",
      "[299]\tvalid_0's binary_logloss: 0.637167\n",
      "[300]\tvalid_0's binary_logloss: 0.637051\n",
      "[301]\tvalid_0's binary_logloss: 0.636984\n",
      "[302]\tvalid_0's binary_logloss: 0.63689\n",
      "[303]\tvalid_0's binary_logloss: 0.636901\n",
      "[304]\tvalid_0's binary_logloss: 0.636894\n",
      "[305]\tvalid_0's binary_logloss: 0.636889\n",
      "[306]\tvalid_0's binary_logloss: 0.636853\n",
      "[307]\tvalid_0's binary_logloss: 0.63675\n",
      "[308]\tvalid_0's binary_logloss: 0.636689\n",
      "[309]\tvalid_0's binary_logloss: 0.63653\n",
      "[310]\tvalid_0's binary_logloss: 0.636527\n",
      "[311]\tvalid_0's binary_logloss: 0.636564\n",
      "[312]\tvalid_0's binary_logloss: 0.636647\n",
      "[313]\tvalid_0's binary_logloss: 0.63665\n",
      "[314]\tvalid_0's binary_logloss: 0.636608\n",
      "[315]\tvalid_0's binary_logloss: 0.636539\n",
      "[316]\tvalid_0's binary_logloss: 0.63657\n",
      "[317]\tvalid_0's binary_logloss: 0.636585\n",
      "[318]\tvalid_0's binary_logloss: 0.636546\n",
      "[319]\tvalid_0's binary_logloss: 0.636504\n",
      "[320]\tvalid_0's binary_logloss: 0.636591\n",
      "[321]\tvalid_0's binary_logloss: 0.636465\n",
      "[322]\tvalid_0's binary_logloss: 0.636522\n",
      "[323]\tvalid_0's binary_logloss: 0.636502\n",
      "[324]\tvalid_0's binary_logloss: 0.636609\n",
      "[325]\tvalid_0's binary_logloss: 0.636538\n",
      "[326]\tvalid_0's binary_logloss: 0.63657\n",
      "[327]\tvalid_0's binary_logloss: 0.636494\n",
      "[328]\tvalid_0's binary_logloss: 0.636445\n",
      "[329]\tvalid_0's binary_logloss: 0.636385\n",
      "[330]\tvalid_0's binary_logloss: 0.636362\n",
      "[331]\tvalid_0's binary_logloss: 0.636272\n",
      "[332]\tvalid_0's binary_logloss: 0.636214\n",
      "[333]\tvalid_0's binary_logloss: 0.63626\n",
      "[334]\tvalid_0's binary_logloss: 0.636267\n",
      "[335]\tvalid_0's binary_logloss: 0.636215\n",
      "[336]\tvalid_0's binary_logloss: 0.636251\n",
      "[337]\tvalid_0's binary_logloss: 0.636318\n",
      "[338]\tvalid_0's binary_logloss: 0.636322\n",
      "[339]\tvalid_0's binary_logloss: 0.636456\n",
      "[340]\tvalid_0's binary_logloss: 0.636522\n",
      "[341]\tvalid_0's binary_logloss: 0.63642\n",
      "[342]\tvalid_0's binary_logloss: 0.63639\n",
      "[343]\tvalid_0's binary_logloss: 0.636451\n",
      "[344]\tvalid_0's binary_logloss: 0.636489\n",
      "[345]\tvalid_0's binary_logloss: 0.636403\n",
      "[346]\tvalid_0's binary_logloss: 0.636296\n",
      "[347]\tvalid_0's binary_logloss: 0.636309\n",
      "[348]\tvalid_0's binary_logloss: 0.636465\n",
      "[349]\tvalid_0's binary_logloss: 0.636511\n",
      "[350]\tvalid_0's binary_logloss: 0.636665\n",
      "[351]\tvalid_0's binary_logloss: 0.636729\n",
      "[352]\tvalid_0's binary_logloss: 0.63671\n",
      "Early stopping, best iteration is:\n",
      "[332]\tvalid_0's binary_logloss: 0.636214\n",
      "[[1860 1140]\n",
      " [1114 1886]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        BEL       0.63      0.62      0.62      3000\n",
      "        DUT       0.62      0.63      0.63      3000\n",
      "\n",
      "avg / total       0.62      0.62      0.62      6000\n",
      "\n",
      "[1]\tvalid_0's binary_logloss: 0.690368\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.688126\n",
      "[3]\tvalid_0's binary_logloss: 0.686218\n",
      "[4]\tvalid_0's binary_logloss: 0.684609\n",
      "[5]\tvalid_0's binary_logloss: 0.683153\n",
      "[6]\tvalid_0's binary_logloss: 0.681855\n",
      "[7]\tvalid_0's binary_logloss: 0.680591\n",
      "[8]\tvalid_0's binary_logloss: 0.679435\n",
      "[9]\tvalid_0's binary_logloss: 0.678603\n",
      "[10]\tvalid_0's binary_logloss: 0.677661\n",
      "[11]\tvalid_0's binary_logloss: 0.676825\n",
      "[12]\tvalid_0's binary_logloss: 0.675774\n",
      "[13]\tvalid_0's binary_logloss: 0.675124\n",
      "[14]\tvalid_0's binary_logloss: 0.674274\n",
      "[15]\tvalid_0's binary_logloss: 0.673179\n",
      "[16]\tvalid_0's binary_logloss: 0.672428\n",
      "[17]\tvalid_0's binary_logloss: 0.671836\n",
      "[18]\tvalid_0's binary_logloss: 0.670863\n",
      "[19]\tvalid_0's binary_logloss: 0.670219\n",
      "[20]\tvalid_0's binary_logloss: 0.669432\n",
      "[21]\tvalid_0's binary_logloss: 0.668626\n",
      "[22]\tvalid_0's binary_logloss: 0.667877\n",
      "[23]\tvalid_0's binary_logloss: 0.667183\n",
      "[24]\tvalid_0's binary_logloss: 0.666323\n",
      "[25]\tvalid_0's binary_logloss: 0.665511\n",
      "[26]\tvalid_0's binary_logloss: 0.66497\n",
      "[27]\tvalid_0's binary_logloss: 0.664311\n",
      "[28]\tvalid_0's binary_logloss: 0.663946\n",
      "[29]\tvalid_0's binary_logloss: 0.663436\n",
      "[30]\tvalid_0's binary_logloss: 0.66265\n",
      "[31]\tvalid_0's binary_logloss: 0.661972\n",
      "[32]\tvalid_0's binary_logloss: 0.661483\n",
      "[33]\tvalid_0's binary_logloss: 0.660793\n",
      "[34]\tvalid_0's binary_logloss: 0.660157\n",
      "[35]\tvalid_0's binary_logloss: 0.659525\n",
      "[36]\tvalid_0's binary_logloss: 0.659221\n",
      "[37]\tvalid_0's binary_logloss: 0.658564\n",
      "[38]\tvalid_0's binary_logloss: 0.658439\n",
      "[39]\tvalid_0's binary_logloss: 0.657919\n",
      "[40]\tvalid_0's binary_logloss: 0.657573\n",
      "[41]\tvalid_0's binary_logloss: 0.656994\n",
      "[42]\tvalid_0's binary_logloss: 0.656655\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fa2c70de6728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     clf.fit(cvtrainx,cvtrainy,early_stopping_rounds=20,\n\u001b[1;32m     22\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvdevx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcvdevy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         verbose=True)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#score1=clf.score(cvdevx,cvdevy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    667\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    456\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    197\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1434\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1435\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1437\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sf=sklearn.model_selection.StratifiedKFold(50)\n",
    "from sklearn.datasets import make_classification\n",
    "#from xgboost import XGBClassifer\n",
    "from sklearn.ensemble import AdaBoostClassifier,BaggingClassifier\n",
    "cvcount=0\n",
    "allPredicts=[]\n",
    "import os\n",
    "os.system('mkdir -p models')\n",
    "\n",
    "#trainx,trainy=make_classification(100000)\n",
    "#print(trainx.shape,trainy.shape)\n",
    "for trainindex,devindex in sf.split(trainx,trainy):\n",
    "    cvtrainx,cvtrainy=trainx[trainindex],trainy[trainindex]\n",
    "    cvdevx,cvdevy=trainx[devindex],trainy[devindex]\n",
    "    #clf=LinearSVC()   #BaggingClassifier(base_estimator=LinearSVC(),n_estimators=3,n_jobs=-1)\n",
    "    #clf.fit(cvtrainx,cvtrainy)\n",
    "    \n",
    "    clf=lgb.LGBMClassifier(num_leaves=100,n_jobs=12,\n",
    "                                learning_rate=0.1,n_estimators=1000,silent=False)\n",
    "    \n",
    "    clf.fit(cvtrainx,cvtrainy,early_stopping_rounds=20,\n",
    "        eval_set=(cvdevx,cvdevy),\n",
    "        verbose=True)\n",
    "    \n",
    "    #score1=clf.score(cvdevx,cvdevy)\n",
    "    #score2=clf.score(evalx,evaly)\n",
    "    predictValue=clf.predict(cvdevx)\n",
    "    #print('===============\\t%d\\t%f\\t%f'%(cvcount,score1,score2))\n",
    "    \n",
    "    print(confusion_matrix(cvdevy,predictValue))\n",
    "    print(classification_report(cvdevy,predictValue))\n",
    "    #predictValue=clf.predict(evalx)    \n",
    "    #print(confusion_matrix(evaly,predictValue))\n",
    "    #print(classification_report(evaly,predictValue))\n",
    "    #clf=LinearSVC()\n",
    "    #print(type(cvtrainy),type(evaly))\n",
    "    #clf.fit(vstack((cvtrainx,evalx)),np.concatenate((cvtrainy,evaly)))\n",
    "    '''\n",
    "    clf=lgb.LGBMClassifier(num_leaves=100,n_jobs=12,\n",
    "                                learning_rate=0.1,n_estimators=1000,silent=False)\n",
    "\n",
    "    clf.fit(vstack((cvtrainx,evalx)),np.concatenate((cvtrainy,evaly)),early_stopping_rounds=20,\n",
    "        eval_set=(cvdevx,cvdevy),\n",
    "        verbose=True)\n",
    "    \n",
    "    score1=clf.score(cvdevx,cvdevy)\n",
    "    score2=clf.score(evalx,evaly)\n",
    "    #predictValue=clf.predict(cvdevx)\n",
    "    print('===============\\t%d\\t%f\\t%f'%(cvcount,score1,score2))\n",
    "    \n",
    "    modelName='./models/%02d_%f_%f.lgbm'%(cvcount,score1,score2)\n",
    "\n",
    "    pickle.dump(clf,open(modelName,'wb'))\n",
    "    testPredict=clf.predict(testx)\n",
    "    predictFile='./models/%02d_%f_%f.lgbm.predict'%(cvcount,score1,score2)\n",
    "    with open(predictFile,'w') as fout:\n",
    "        fout.write(' '.join(testPredict))\n",
    "    allPredicts.append(testPredict)\n",
    "    '''\n",
    "    cvcount+=1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "'''\n",
    "#light boost\n",
    "\n",
    "[[1860 1140]\n",
    " [1114 1886]]\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        BEL       0.63      0.62      0.62      3000\n",
    "        DUT       0.62      0.63      0.63      3000\n",
    "\n",
    "avg / total       0.62      0.62      0.62      6000\n",
    "\n",
    "[1]\tvalid_0's binary_logloss: 0.690368\n",
    "Training until validation scores don't improve for 20 rounds.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(len(allPredicts))\n",
    "print(len(allPredicts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate final submission\n",
    "finalPredict=np.array(allPredicts).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(finalPredict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "DUT\n",
      "BEL\n",
      "BEL\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "with open('submit.txt','w') as fout:\n",
    "    for l in finalPredict:\n",
    "        print(Counter(l).most_common()[0][0])\n",
    "        fout.write(Counter(l).most_common()[0][0]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 644564) (300000,) (500, 644564) (500,)\n",
      "[1]\tvalid_0's binary_logloss: 0.692145\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.690955\n",
      "[3]\tvalid_0's binary_logloss: 0.690054\n",
      "[4]\tvalid_0's binary_logloss: 0.68992\n",
      "[5]\tvalid_0's binary_logloss: 0.689989\n",
      "[6]\tvalid_0's binary_logloss: 0.689144\n",
      "[7]\tvalid_0's binary_logloss: 0.688645\n",
      "[8]\tvalid_0's binary_logloss: 0.688399\n",
      "[9]\tvalid_0's binary_logloss: 0.688552\n",
      "[10]\tvalid_0's binary_logloss: 0.68827\n",
      "[11]\tvalid_0's binary_logloss: 0.686846\n",
      "[12]\tvalid_0's binary_logloss: 0.686406\n",
      "[13]\tvalid_0's binary_logloss: 0.686123\n",
      "[14]\tvalid_0's binary_logloss: 0.686474\n",
      "[15]\tvalid_0's binary_logloss: 0.686478\n",
      "[16]\tvalid_0's binary_logloss: 0.685755\n",
      "[17]\tvalid_0's binary_logloss: 0.68581\n",
      "[18]\tvalid_0's binary_logloss: 0.685611\n",
      "[19]\tvalid_0's binary_logloss: 0.684769\n",
      "[20]\tvalid_0's binary_logloss: 0.684982\n",
      "[21]\tvalid_0's binary_logloss: 0.684513\n",
      "[22]\tvalid_0's binary_logloss: 0.683972\n",
      "[23]\tvalid_0's binary_logloss: 0.683867\n",
      "[24]\tvalid_0's binary_logloss: 0.683702\n",
      "[25]\tvalid_0's binary_logloss: 0.682916\n",
      "[26]\tvalid_0's binary_logloss: 0.682204\n",
      "[27]\tvalid_0's binary_logloss: 0.681778\n",
      "[28]\tvalid_0's binary_logloss: 0.680908\n",
      "[29]\tvalid_0's binary_logloss: 0.679947\n",
      "[30]\tvalid_0's binary_logloss: 0.680066\n",
      "[31]\tvalid_0's binary_logloss: 0.679826\n",
      "[32]\tvalid_0's binary_logloss: 0.679419\n",
      "[33]\tvalid_0's binary_logloss: 0.67923\n",
      "[34]\tvalid_0's binary_logloss: 0.678575\n",
      "[35]\tvalid_0's binary_logloss: 0.6782\n",
      "[36]\tvalid_0's binary_logloss: 0.678104\n",
      "[37]\tvalid_0's binary_logloss: 0.677456\n",
      "[38]\tvalid_0's binary_logloss: 0.677254\n",
      "[39]\tvalid_0's binary_logloss: 0.677049\n",
      "[40]\tvalid_0's binary_logloss: 0.676596\n",
      "[41]\tvalid_0's binary_logloss: 0.676515\n",
      "[42]\tvalid_0's binary_logloss: 0.676254\n",
      "[43]\tvalid_0's binary_logloss: 0.675795\n",
      "[44]\tvalid_0's binary_logloss: 0.674644\n",
      "[45]\tvalid_0's binary_logloss: 0.675088\n",
      "[46]\tvalid_0's binary_logloss: 0.674181\n",
      "[47]\tvalid_0's binary_logloss: 0.674169\n",
      "[48]\tvalid_0's binary_logloss: 0.673828\n",
      "[49]\tvalid_0's binary_logloss: 0.673358\n",
      "[50]\tvalid_0's binary_logloss: 0.6726\n",
      "[51]\tvalid_0's binary_logloss: 0.67273\n",
      "[52]\tvalid_0's binary_logloss: 0.672427\n",
      "[53]\tvalid_0's binary_logloss: 0.672409\n",
      "[54]\tvalid_0's binary_logloss: 0.672399\n",
      "[55]\tvalid_0's binary_logloss: 0.67201\n",
      "[56]\tvalid_0's binary_logloss: 0.671298\n",
      "[57]\tvalid_0's binary_logloss: 0.671381\n",
      "[58]\tvalid_0's binary_logloss: 0.671876\n",
      "[59]\tvalid_0's binary_logloss: 0.671676\n",
      "[60]\tvalid_0's binary_logloss: 0.671794\n",
      "[61]\tvalid_0's binary_logloss: 0.671829\n",
      "[62]\tvalid_0's binary_logloss: 0.67147\n",
      "[63]\tvalid_0's binary_logloss: 0.671426\n",
      "[64]\tvalid_0's binary_logloss: 0.671003\n",
      "[65]\tvalid_0's binary_logloss: 0.670558\n",
      "[66]\tvalid_0's binary_logloss: 0.669879\n",
      "[67]\tvalid_0's binary_logloss: 0.669654\n",
      "[68]\tvalid_0's binary_logloss: 0.669673\n",
      "[69]\tvalid_0's binary_logloss: 0.669717\n",
      "[70]\tvalid_0's binary_logloss: 0.669952\n",
      "[71]\tvalid_0's binary_logloss: 0.669969\n",
      "[72]\tvalid_0's binary_logloss: 0.669673\n",
      "[73]\tvalid_0's binary_logloss: 0.670175\n",
      "[74]\tvalid_0's binary_logloss: 0.670193\n",
      "[75]\tvalid_0's binary_logloss: 0.669998\n",
      "[76]\tvalid_0's binary_logloss: 0.670173\n",
      "[77]\tvalid_0's binary_logloss: 0.670062\n",
      "[78]\tvalid_0's binary_logloss: 0.670344\n",
      "[79]\tvalid_0's binary_logloss: 0.670347\n",
      "[80]\tvalid_0's binary_logloss: 0.67038\n",
      "[81]\tvalid_0's binary_logloss: 0.670256\n",
      "[82]\tvalid_0's binary_logloss: 0.670604\n",
      "[83]\tvalid_0's binary_logloss: 0.670453\n",
      "[84]\tvalid_0's binary_logloss: 0.670777\n",
      "[85]\tvalid_0's binary_logloss: 0.671026\n",
      "[86]\tvalid_0's binary_logloss: 0.670924\n",
      "[87]\tvalid_0's binary_logloss: 0.670578\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid_0's binary_logloss: 0.669654\n",
      "[[152  98]\n",
      " [110 140]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        BEL       0.58      0.61      0.59       250\n",
      "        DUT       0.59      0.56      0.57       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(trainx.shape,trainy.shape,evalx.shape,evaly.shape)\n",
    "clf=lgb.LGBMClassifier(num_leaves=150,n_jobs=12,\n",
    "                                learning_rate=0.1,n_estimators=1000,silent=False)\n",
    "\n",
    "clf.fit(trainx,trainy,early_stopping_rounds=20,\n",
    "        eval_set=(evalx,evaly),\n",
    "        verbose=True)\n",
    "predictValue=clf.predict(evalx)\n",
    "print(confusion_matrix(evaly,predictValue))\n",
    "print(classification_report(evaly,predictValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fasttext\n",
    "import fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainyRaw),type(trainxRaw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdutFile='./data/rawdut.txt'\n",
    "rawbelFile='./data/rawbel.txt'\n",
    "trainlabeled='./data/trainlabeled.txt'\n",
    "devlabeled='./data/devlabeled.txt'\n",
    "preFiexed='__label__'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(trainlabeled,'w') as fout:\n",
    "    for x,y in zip(trainxRaw,trainy):\n",
    "        fout.write('__label__{} {}\\n'.format(y,x))\n",
    "\n",
    "with open(devlabeled,'w') as fout:\n",
    "    for x,y in zip(devxRaw,devyRaw):\n",
    "        fout.write('__label__{} {}\\n'.format(y,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "rawDu=np.array(trainxRaw)[trainyRaw=='DUT']\n",
    "os.system('mkdir -p data')\n",
    "with open(rawdutFile,'w') as fout:\n",
    "    fout.writelines('\\n'.join(map(str,rawDu)))\n",
    "    \n",
    "rawBel=np.array(trainxRaw)[trainyRaw=='BEL']\n",
    "os.system('mkdir -p data')\n",
    "with open('./data/rawbel.txt','w') as fout:\n",
    "    fout.writelines('\\n'.join(map(str,rawBel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOW',\n",
       " 'EOS',\n",
       " 'EOW',\n",
       " 'FastText',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'absolute_import',\n",
       " 'division',\n",
       " 'load_model',\n",
       " 'print_function',\n",
       " 'tokenize',\n",
       " 'train_supervised',\n",
       " 'train_unsupervised',\n",
       " 'unicode_literals']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(fastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = fastText.train_supervised(trainlabeled)\n",
    "\n",
    "result = classifier.test(devlabeled)\n",
    "#print ('P@1:', result.precision)\n",
    "#print ('R@1:', result.recall)\n",
    "#print ('Number of examples:', result.nexamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 0.586, 0.586)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41255\n"
     ]
    }
   ],
   "source": [
    "#print(trainx[0])\n",
    "#get vocab by tfidf get_feature_names\n",
    "tfv = TfidfVectorizer(min_df=5,use_idf=1,\n",
    "                      smooth_idf=1,ngram_range=(1,1),\n",
    "                     )\n",
    "tfv.fit(trainxRaw)\n",
    "print(len(tfv.get_feature_names()))\n",
    "tfidfDict={s:sindex for sindex,  s in enumerate(  tfv.get_feature_names())}\n",
    "#for s in trainxRaw[0].split():\n",
    "#    if s in tfidfDict:\n",
    "#        print(s,tfidfDict[s])\n",
    "\n",
    "def toCnnIndexAllinOne(lin):\n",
    "    rv=[]\n",
    "    for s in lin.split()[:LengthOfInputSequences]:\n",
    "        if s in tfidfDict:\n",
    "            rv.append(tfidfDict[s])\n",
    "    while(len(rv)<LengthOfInputSequences):\n",
    "        rv.append(0)\n",
    "    return rv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_unsupervised' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-87c50ae52ae7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#build w2v and use the w2v index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#train a w2v and change the trainCnnIndex/devCnnIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodelW2V\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_unsupervised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/rawtrainNoLb.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skipgram'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminCount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtfidfDict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msindex\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0mmodelW2V\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#for s in trainxRaw[0].split():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_unsupervised' is not defined"
     ]
    }
   ],
   "source": [
    "#build w2v and use the w2v index\n",
    "#train a w2v and change the trainCnnIndex/devCnnIndex\n",
    "modelW2V=train_unsupervised('./data/rawtrainNoLb.txt',model='skipgram',minCount=10,dim=10)\n",
    "tfidfDict={s:sindex for sindex,  s in enumerate(  modelW2V.get_words())}\n",
    "#for s in trainxRaw[0].split():\n",
    "#    if s in tfidfDict:\n",
    "#        print(s,tfidfDict[s])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidfDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-ceda0256c57b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainCnnIndex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoCnnIndexAllinOne\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainxRaw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdevCnnIndex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoCnnIndexAllinOne\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevxRaw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-ceda0256c57b>\u001b[0m in \u001b[0;36mtoCnnIndexAllinOne\u001b[0;34m(lin)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLengthOfInputSequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfidfDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidfDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mLengthOfInputSequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidfDict' is not defined"
     ]
    }
   ],
   "source": [
    "def toCnnIndexAllinOne(lin):\n",
    "    rv=[]\n",
    "    for s in lin.split()[:LengthOfInputSequences]:\n",
    "        if s in tfidfDict:\n",
    "            rv.append(tfidfDict[s])\n",
    "    while(len(rv)<LengthOfInputSequences):\n",
    "        rv.append(0)\n",
    "    return rv\n",
    "\n",
    "trainCnnIndex=list(map(toCnnIndexAllinOne,trainxRaw))\n",
    "devCnnIndex=list(map(toCnnIndexAllinOne,devxRaw))\n",
    "testCnnIndex=list(map(toCnnIndexAllinOne,testxRaw))\n",
    "print(modelW2V.get_output_matrix().shape[0])  #default minCount=5 42836    minCount=3 61282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 144, 1, 503, 12, 0, 1, 40, 10, 1376, 278, 26, 6, 9156, 15, 33, 384, 0, 90, 10, 25186, 19, 11422, 4553, 447, 11025, 0, 8, 9, 70, 61, 12, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5, 444, 15, 5, 7917, 353, 19, 13, 80, 11622, 155, 31, 546, 0, 319, 129, 359, 0, 17, 152, 317, 604, 0, 84, 15027, 2841, 132, 106, 6, 46, 5849, 11, 2786, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(trainCnnIndex[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 60)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 60, 20)        825100      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 60, 10)        210         embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistribu (None, 60, 1)         11          time_distributed_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 60, 1)         0           time_distributed_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)            (None, 60, 20)        0           activation_1[0][0]               \n",
      "                                                                   embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 56, 64)        6464        multiply_1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling1d_1 (AveragePool (None, 18, 64)        0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 14, 64)        20544       average_pooling1d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling1d_2 (AveragePool (None, 4, 64)         0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling1d_3 (AveragePool (None, 1, 20)         0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 256)           0           average_pooling1d_2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 20)            0           average_pooling1d_3[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 276)           0           flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 200)           55400       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             402         dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 908,131\n",
      "Trainable params: 908,131\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1134pt\" viewBox=\"0.00 0.00 1151.00 1134.00\" width=\"1151pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1130)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-1130 1147,-1130 1147,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139867035243352 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139867035243352</title>\n",
       "<polygon fill=\"none\" points=\"295.5,-1079.5 295.5,-1125.5 615.5,-1125.5 615.5,-1079.5 295.5,-1079.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"375.5\" y=\"-1098.8\">input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"455.5,-1079.5 455.5,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"489.5\" y=\"-1110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"455.5,-1102.5 523.5,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"489.5\" y=\"-1087.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"523.5,-1079.5 523.5,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"569.5\" y=\"-1110.3\">(None, 60)</text>\n",
       "<polyline fill=\"none\" points=\"523.5,-1102.5 615.5,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"569.5\" y=\"-1087.3\">(None, 60)</text>\n",
       "</g>\n",
       "<!-- 139867035242960 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139867035242960</title>\n",
       "<polygon fill=\"none\" points=\"260.5,-996.5 260.5,-1042.5 650.5,-1042.5 650.5,-996.5 260.5,-996.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"362\" y=\"-1015.8\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"463.5,-996.5 463.5,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"497.5\" y=\"-1027.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"463.5,-1019.5 531.5,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"497.5\" y=\"-1004.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"531.5,-996.5 531.5,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"591\" y=\"-1027.3\">(None, 60)</text>\n",
       "<polyline fill=\"none\" points=\"531.5,-1019.5 650.5,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"591\" y=\"-1004.3\">(None, 60, 20)</text>\n",
       "</g>\n",
       "<!-- 139867035243352&#45;&gt;139867035242960 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139867035243352-&gt;139867035242960</title>\n",
       "<path d=\"M455.5,-1079.37C455.5,-1071.15 455.5,-1061.66 455.5,-1052.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"459,-1052.61 455.5,-1042.61 452,-1052.61 459,-1052.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139867431827160 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139867431827160</title>\n",
       "<polygon fill=\"none\" points=\"0,-913.5 0,-959.5 589,-959.5 589,-913.5 0,-913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201\" y=\"-932.8\">time_distributed_1(dense_1): TimeDistributed(Dense)</text>\n",
       "<polyline fill=\"none\" points=\"402,-913.5 402,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"402,-936.5 470,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"470,-913.5 470,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"529.5\" y=\"-944.3\">(None, 60, 20)</text>\n",
       "<polyline fill=\"none\" points=\"470,-936.5 589,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"529.5\" y=\"-921.3\">(None, 60, 10)</text>\n",
       "</g>\n",
       "<!-- 139867035242960&#45;&gt;139867431827160 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139867035242960-&gt;139867431827160</title>\n",
       "<path d=\"M411.506,-996.366C391.717,-986.41 368.194,-974.576 347.448,-964.138\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"348.947,-960.975 338.441,-959.607 345.801,-967.228 348.947,-960.975\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866812270688 -->\n",
       "<g class=\"node\" id=\"node6\"><title>139866812270688</title>\n",
       "<polygon fill=\"none\" points=\"386,-664.5 386,-710.5 849,-710.5 849,-664.5 386,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"467\" y=\"-683.8\">multiply_1: Multiply</text>\n",
       "<polyline fill=\"none\" points=\"548,-664.5 548,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"582\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"548,-687.5 616,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"582\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"616,-664.5 616,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"732.5\" y=\"-695.3\">[(None, 60, 1), (None, 60, 20)]</text>\n",
       "<polyline fill=\"none\" points=\"616,-687.5 849,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"732.5\" y=\"-672.3\">(None, 60, 20)</text>\n",
       "</g>\n",
       "<!-- 139867035242960&#45;&gt;139866812270688 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>139867035242960-&gt;139866812270688</title>\n",
       "<path d=\"M526.658,-996.443C550.393,-987.247 576.195,-975.128 597.5,-960 608.409,-952.254 605.801,-943.993 617.5,-937.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M617.5,-935.5C635.924,-925.274 626.056,-785.139 620.48,-720.613\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"623.96,-720.228 619.594,-710.575 616.987,-720.844 623.96,-720.228\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866816556112 -->\n",
       "<g class=\"node\" id=\"node11\"><title>139866816556112</title>\n",
       "<polygon fill=\"none\" points=\"646,-830.5 646,-876.5 1143,-876.5 1143,-830.5 646,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"801\" y=\"-849.8\">average_pooling1d_3: AveragePooling1D</text>\n",
       "<polyline fill=\"none\" points=\"956,-830.5 956,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"990\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"956,-853.5 1024,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"990\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1024,-830.5 1024,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1083.5\" y=\"-861.3\">(None, 60, 20)</text>\n",
       "<polyline fill=\"none\" points=\"1024,-853.5 1143,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1083.5\" y=\"-838.3\">(None, 1, 20)</text>\n",
       "</g>\n",
       "<!-- 139867035242960&#45;&gt;139866816556112 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>139867035242960-&gt;139866816556112</title>\n",
       "<path d=\"M617.5,-935.5C663.609,-909.909 718.625,-891.669 767.43,-879.062\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"768.587,-882.38 777.424,-876.536 766.871,-875.593 768.587,-882.38\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866812270072 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139866812270072</title>\n",
       "<polygon fill=\"none\" points=\"0,-830.5 0,-876.5 589,-876.5 589,-830.5 0,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201\" y=\"-849.8\">time_distributed_2(dense_2): TimeDistributed(Dense)</text>\n",
       "<polyline fill=\"none\" points=\"402,-830.5 402,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"402,-853.5 470,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"470,-830.5 470,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"529.5\" y=\"-861.3\">(None, 60, 10)</text>\n",
       "<polyline fill=\"none\" points=\"470,-853.5 589,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"529.5\" y=\"-838.3\">(None, 60, 1)</text>\n",
       "</g>\n",
       "<!-- 139867431827160&#45;&gt;139866812270072 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139867431827160-&gt;139866812270072</title>\n",
       "<path d=\"M294.5,-913.366C294.5,-905.152 294.5,-895.658 294.5,-886.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"298,-886.607 294.5,-876.607 291,-886.607 298,-886.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866812269736 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139866812269736</title>\n",
       "<polygon fill=\"none\" points=\"169,-747.5 169,-793.5 534,-793.5 534,-747.5 169,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.5\" y=\"-766.8\">activation_1: Activation</text>\n",
       "<polyline fill=\"none\" points=\"356,-747.5 356,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"390\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"356,-770.5 424,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"390\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"424,-747.5 424,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479\" y=\"-778.3\">(None, 60, 1)</text>\n",
       "<polyline fill=\"none\" points=\"424,-770.5 534,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479\" y=\"-755.3\">(None, 60, 1)</text>\n",
       "</g>\n",
       "<!-- 139866812270072&#45;&gt;139866812269736 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139866812270072-&gt;139866812269736</title>\n",
       "<path d=\"M310.076,-830.366C316.233,-821.616 323.412,-811.414 330.048,-801.985\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"333.051,-803.799 335.943,-793.607 327.326,-799.771 333.051,-803.799\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866812269736&#45;&gt;139866812270688 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>139866812269736-&gt;139866812270688</title>\n",
       "<path d=\"M423.834,-747.473C458.287,-736.982 499.651,-724.386 535.29,-713.534\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"536.633,-716.784 545.18,-710.522 534.594,-710.087 536.633,-716.784\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866812272312 -->\n",
       "<g class=\"node\" id=\"node7\"><title>139866812272312</title>\n",
       "<polygon fill=\"none\" points=\"448.5,-581.5 448.5,-627.5 786.5,-627.5 786.5,-581.5 448.5,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524\" y=\"-600.8\">conv1d_1: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"599.5,-581.5 599.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"633.5\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"599.5,-604.5 667.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"633.5\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"667.5,-581.5 667.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"727\" y=\"-612.3\">(None, 60, 20)</text>\n",
       "<polyline fill=\"none\" points=\"667.5,-604.5 786.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"727\" y=\"-589.3\">(None, 56, 64)</text>\n",
       "</g>\n",
       "<!-- 139866812270688&#45;&gt;139866812272312 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>139866812270688-&gt;139866812272312</title>\n",
       "<path d=\"M617.5,-664.366C617.5,-656.152 617.5,-646.658 617.5,-637.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"621,-637.607 617.5,-627.607 614,-637.607 621,-637.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866722078848 -->\n",
       "<g class=\"node\" id=\"node8\"><title>139866722078848</title>\n",
       "<polygon fill=\"none\" points=\"369,-498.5 369,-544.5 866,-544.5 866,-498.5 369,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524\" y=\"-517.8\">average_pooling1d_1: AveragePooling1D</text>\n",
       "<polyline fill=\"none\" points=\"679,-498.5 679,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"713\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"679,-521.5 747,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"713\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"747,-498.5 747,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"806.5\" y=\"-529.3\">(None, 56, 64)</text>\n",
       "<polyline fill=\"none\" points=\"747,-521.5 866,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"806.5\" y=\"-506.3\">(None, 18, 64)</text>\n",
       "</g>\n",
       "<!-- 139866812272312&#45;&gt;139866722078848 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>139866812272312-&gt;139866722078848</title>\n",
       "<path d=\"M617.5,-581.366C617.5,-573.152 617.5,-563.658 617.5,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"621,-554.607 617.5,-544.607 614,-554.607 621,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139867222262560 -->\n",
       "<g class=\"node\" id=\"node9\"><title>139867222262560</title>\n",
       "<polygon fill=\"none\" points=\"448.5,-415.5 448.5,-461.5 786.5,-461.5 786.5,-415.5 448.5,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524\" y=\"-434.8\">conv1d_2: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"599.5,-415.5 599.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"633.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"599.5,-438.5 667.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"633.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"667.5,-415.5 667.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"727\" y=\"-446.3\">(None, 18, 64)</text>\n",
       "<polyline fill=\"none\" points=\"667.5,-438.5 786.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"727\" y=\"-423.3\">(None, 14, 64)</text>\n",
       "</g>\n",
       "<!-- 139866722078848&#45;&gt;139867222262560 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>139866722078848-&gt;139867222262560</title>\n",
       "<path d=\"M617.5,-498.366C617.5,-490.152 617.5,-480.658 617.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"621,-471.607 617.5,-461.607 614,-471.607 621,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866816403776 -->\n",
       "<g class=\"node\" id=\"node10\"><title>139866816403776</title>\n",
       "<polygon fill=\"none\" points=\"369,-332.5 369,-378.5 866,-378.5 866,-332.5 369,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524\" y=\"-351.8\">average_pooling1d_2: AveragePooling1D</text>\n",
       "<polyline fill=\"none\" points=\"679,-332.5 679,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"713\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"679,-355.5 747,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"713\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"747,-332.5 747,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"806.5\" y=\"-363.3\">(None, 14, 64)</text>\n",
       "<polyline fill=\"none\" points=\"747,-355.5 866,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"806.5\" y=\"-340.3\">(None, 4, 64)</text>\n",
       "</g>\n",
       "<!-- 139867222262560&#45;&gt;139866816403776 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>139867222262560-&gt;139866816403776</title>\n",
       "<path d=\"M617.5,-415.366C617.5,-407.152 617.5,-397.658 617.5,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"621,-388.607 617.5,-378.607 614,-388.607 621,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866816437720 -->\n",
       "<g class=\"node\" id=\"node12\"><title>139866816437720</title>\n",
       "<polygon fill=\"none\" points=\"503,-249.5 503,-295.5 822,-295.5 822,-249.5 503,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"573.5\" y=\"-268.8\">flatten_1: Flatten</text>\n",
       "<polyline fill=\"none\" points=\"644,-249.5 644,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"678\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"644,-272.5 712,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"678\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"712,-249.5 712,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"767\" y=\"-280.3\">(None, 4, 64)</text>\n",
       "<polyline fill=\"none\" points=\"712,-272.5 822,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"767\" y=\"-257.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 139866816403776&#45;&gt;139866816437720 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>139866816403776-&gt;139866816437720</title>\n",
       "<path d=\"M629.796,-332.366C634.559,-323.794 640.094,-313.83 645.243,-304.563\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"648.421,-306.048 650.218,-295.607 642.302,-302.649 648.421,-306.048\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866816555888 -->\n",
       "<g class=\"node\" id=\"node13\"><title>139866816555888</title>\n",
       "<polygon fill=\"none\" points=\"735,-747.5 735,-793.5 1054,-793.5 1054,-747.5 735,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"805.5\" y=\"-766.8\">flatten_2: Flatten</text>\n",
       "<polyline fill=\"none\" points=\"876,-747.5 876,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"876,-770.5 944,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"944,-747.5 944,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"999\" y=\"-778.3\">(None, 1, 20)</text>\n",
       "<polyline fill=\"none\" points=\"944,-770.5 1054,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"999\" y=\"-755.3\">(None, 20)</text>\n",
       "</g>\n",
       "<!-- 139866816556112&#45;&gt;139866816555888 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>139866816556112-&gt;139866816555888</title>\n",
       "<path d=\"M894.5,-830.366C894.5,-822.152 894.5,-812.658 894.5,-803.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"898,-803.607 894.5,-793.607 891,-803.607 898,-803.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866615743880 -->\n",
       "<g class=\"node\" id=\"node14\"><title>139866615743880</title>\n",
       "<polygon fill=\"none\" points=\"535.5,-166.5 535.5,-212.5 1021.5,-212.5 1021.5,-166.5 535.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"646\" y=\"-185.8\">concatenate_1: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"756.5,-166.5 756.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"790.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"756.5,-189.5 824.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"790.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"824.5,-166.5 824.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"923\" y=\"-197.3\">[(None, 256), (None, 20)]</text>\n",
       "<polyline fill=\"none\" points=\"824.5,-189.5 1021.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"923\" y=\"-174.3\">(None, 276)</text>\n",
       "</g>\n",
       "<!-- 139866816437720&#45;&gt;139866615743880 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>139866816437720-&gt;139866615743880</title>\n",
       "<path d=\"M694.198,-249.366C707.88,-239.812 724.04,-228.528 738.532,-218.409\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"740.646,-221.202 746.841,-212.607 736.638,-215.462 740.646,-221.202\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866816555888&#45;&gt;139866615743880 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>139866816555888-&gt;139866615743880</title>\n",
       "<path d=\"M894.5,-747.483C894.5,-716.005 894.5,-656.323 894.5,-605.5 894.5,-605.5 894.5,-605.5 894.5,-354.5 894.5,-298.968 849.62,-249.068 815.692,-219.279\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"817.647,-216.346 807.772,-212.509 813.099,-221.667 817.647,-216.346\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866615743152 -->\n",
       "<g class=\"node\" id=\"node15\"><title>139866615743152</title>\n",
       "<polygon fill=\"none\" points=\"630,-83.5 630,-129.5 927,-129.5 927,-83.5 630,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"694\" y=\"-102.8\">dense_3: Dense</text>\n",
       "<polyline fill=\"none\" points=\"758,-83.5 758,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"792\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"758,-106.5 826,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"792\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"826,-83.5 826,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"876.5\" y=\"-114.3\">(None, 276)</text>\n",
       "<polyline fill=\"none\" points=\"826,-106.5 927,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"876.5\" y=\"-91.3\">(None, 200)</text>\n",
       "</g>\n",
       "<!-- 139866615743880&#45;&gt;139866615743152 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>139866615743880-&gt;139866615743152</title>\n",
       "<path d=\"M778.5,-166.366C778.5,-158.152 778.5,-148.658 778.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"782,-139.607 778.5,-129.607 775,-139.607 782,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139866615600240 -->\n",
       "<g class=\"node\" id=\"node16\"><title>139866615600240</title>\n",
       "<polygon fill=\"none\" points=\"630,-0.5 630,-46.5 927,-46.5 927,-0.5 630,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"694\" y=\"-19.8\">dense_4: Dense</text>\n",
       "<polyline fill=\"none\" points=\"758,-0.5 758,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"792\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"758,-23.5 826,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"792\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"826,-0.5 826,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"876.5\" y=\"-31.3\">(None, 200)</text>\n",
       "<polyline fill=\"none\" points=\"826,-23.5 927,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"876.5\" y=\"-8.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 139866615743152&#45;&gt;139866615600240 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>139866615743152-&gt;139866615600240</title>\n",
       "<path d=\"M778.5,-83.3664C778.5,-75.1516 778.5,-65.6579 778.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"782,-56.6068 778.5,-46.6068 775,-56.6069 782,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#CNN\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "\n",
    "#https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py \n",
    "\n",
    "def getCNN():\n",
    "    input1=keras.layers.Input(shape=(LengthOfInputSequences,))    \n",
    "    \n",
    "    kernel_size = 3\n",
    "    filters = 64\n",
    "    pool_size = 3\n",
    "    lstm_output_size=64\n",
    "    '''\n",
    "    x1=Embedding(input_dim=len(tfidfDict), #modelW2V.get_output_matrix().shape[0],\n",
    "                input_length=LengthOfInputSequences,\n",
    "                output_dim=100,     #modelW2V.get_output_matrix().shape[1],\n",
    "                 trainable=True,\n",
    "                #weights=[modelW2V.get_output_matrix()],\n",
    "                #\n",
    "               )(input1)\n",
    "               '''\n",
    "    x1=Embedding(input_dim=modelW2V.get_output_matrix().shape[0],\n",
    "                input_length=LengthOfInputSequences,\n",
    "                output_dim=modelW2V.get_output_matrix().shape[1],      \n",
    "                 weights=[modelW2V.get_output_matrix()],\n",
    "               )(input1)\n",
    "    #x1=Dropout(0.2)(x1)\n",
    "    x1=Conv1D(filters=filters,kernel_size=kernel_size)(x1)\n",
    "    #x1=Dropout(0.2)(x1)\n",
    "    #x1=Conv1D(filters=filters,kernel_size=kernel_size)(x1)\n",
    "    #x1=Dropout(0.2)(x1)\n",
    "    #x1=Conv1D(filters=filters,kernel_size=kernel_size)(x1)\n",
    "    x1=MaxPooling1D(pool_size=pool_size)(x1)\n",
    "    x1=Conv1D(filters=filters,kernel_size=kernel_size)(x1)\n",
    "    x1=MaxPooling1D(pool_size=pool_size)(x1)\n",
    "    #x1=Dropout(0.2)(x1)\n",
    "    x1=LSTM(lstm_output_size)(x1)\n",
    "    #x1=keras.layers.\n",
    "    #x1=Flatten()(x1)\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #x1=Dropout(0.2)(x1)\n",
    "    addLayer=Dense(512)(x1)\n",
    "    output=Dense(2)(addLayer)\n",
    "    output=Activation('softmax')(output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    seq=keras.models.Model(inputs=[input1],outputs=output)\n",
    "    \n",
    "    seq.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    seq.summary()\n",
    "    return seq\n",
    "\n",
    "\n",
    "def getSparseMLP():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2048))\n",
    "    return model\n",
    "    pass\n",
    "\n",
    "def getMLP():\n",
    "    input1=keras.layers.Input(shape=(LengthOfInputSequences,))    \n",
    "    \n",
    "    kernel_size = 5\n",
    "    filters = 32\n",
    "    pool_size = 4\n",
    "    lstm_output_size=512\n",
    "    '''\n",
    "    x1=Embedding(input_dim=len(tfidfDict), #modelW2V.get_output_matrix().shape[0],\n",
    "                input_length=LengthOfInputSequences,\n",
    "                output_dim=100,     #modelW2V.get_output_matrix().shape[1],\n",
    "                 trainable=True,\n",
    "                #weights=[modelW2V.get_output_matrix()],\n",
    "                #\n",
    "               )(input1)\n",
    "               '''\n",
    "    \n",
    "    x1=Embedding(input_dim=modelW2V.get_output_matrix().shape[0],\n",
    "                input_length=LengthOfInputSequences,\n",
    "                output_dim=modelW2V.get_output_matrix().shape[1],      \n",
    "                weights=[modelW2V.get_output_matrix()],\n",
    "                #trainable=False,\n",
    "                 )(input1)\n",
    "\n",
    "   \n",
    "    #x1=Flatten()(x1)\n",
    "    #x1=LSTM(512)(x1)\n",
    "    #x1=Dropout(0.2)(x1)\n",
    "    #addLayer=Dense(512)(x1)\n",
    "    output=Dense(2)(x1)\n",
    "    output=Activation('softmax')(output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    seq=keras.models.Model(inputs=[input1],outputs=output)\n",
    "    \n",
    "    #seq.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    seq.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    seq.summary()\n",
    "    return seq\n",
    "def getCNNIMDB2():\n",
    "    model = Sequential()\n",
    "    kernel_size = 5\n",
    "    filters = 64\n",
    "    pool_size = 4\n",
    "    lstm_output_size=70\n",
    "    '''\n",
    "    model.add(Embedding(modelW2V.get_output_matrix().shape[0], \n",
    "                        modelW2V.get_output_matrix().shape[1], \n",
    "                        input_length=LengthOfInputSequences,\n",
    "                        weights=[modelW2V.get_output_matrix()]\n",
    "                       ))\n",
    "     '''            \n",
    "    model.add(Embedding(input_dim=len(tfidfDict),\n",
    "                input_length=LengthOfInputSequences,\n",
    "                output_dim=20,      \n",
    "                #weights=[modelW2V.get_output_matrix()],\n",
    "                #trainable=False,\n",
    "               ))\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(lstm_output_size))\n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def getCNNIMDB3():\n",
    "    model = Sequential()\n",
    "    kernel_size = 5\n",
    "    filters = 64\n",
    "    pool_size = 4\n",
    "    lstm_output_size=70\n",
    "       \n",
    "    model.add(Embedding(input_dim=len(tfidfDict),\n",
    "                input_length=LengthOfInputSequences,\n",
    "                output_dim=20,      \n",
    "                #weights=[modelW2V.get_output_matrix()],\n",
    "                #trainable=False,\n",
    "               ))\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.add(LSTM(lstm_output_size))\n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def getAvgWordVector():\n",
    "    model = Sequential()\n",
    "    kernel_size = 5\n",
    "    filters = 64\n",
    "    pool_size = 4\n",
    "    lstm_output_size=70\n",
    "    input1=Input(shape=(60,))   \n",
    "    x1=Embedding(input_dim=len(tfidfDict),\n",
    "                input_length=LengthOfInputSequences,\n",
    "                output_dim=20,   )(input1)\n",
    "    #x2=Conv1D(filters=64,kernel_size=kernel_size)(x1)\n",
    "    x2=keras.layers.AveragePooling1D(60)(x1)\n",
    "    x2=Flatten()(x2)\n",
    "    #x2=LSTM(50)(x2)\n",
    "    #x2=MaxPooling1D(pool_size=4)(x2)\n",
    "    #x2=LSTM(50)(x2)\n",
    "    \n",
    "    #conv=keras.layers.Concatenate()([conv,x2])\n",
    "    conv=x2\n",
    "    conv=Dense(200)(conv)\n",
    "    output=Dense(2)(conv)\n",
    "    \n",
    "    seq=keras.models.Model(inputs=[input1],outputs=output)\n",
    "    \n",
    "    seq.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    seq.summary()\n",
    "    return seq\n",
    "\n",
    "\n",
    "def getLSTM():\n",
    "    model = Sequential()\n",
    "    kernel_size = 5\n",
    "    filters = 64\n",
    "    pool_size = 4\n",
    "    lstm_output_size=70\n",
    "    input1=Input(shape=(60,))   \n",
    "    x1=Embedding(input_dim=len(tfidfDict),\n",
    "                input_length=LengthOfInputSequences,\n",
    "                output_dim=20,   )(input1)\n",
    "\n",
    "    x1=keras.layers.Bidirectional(LSTM(50))(x1)\n",
    "    #only lstm ok\n",
    "    #test with blstm  ok\n",
    "    \n",
    "\n",
    "    conv=Dense(200)(x1)\n",
    "    output=Dense(2)(conv)\n",
    "    \n",
    "    seq=keras.models.Model(inputs=[input1],outputs=output)\n",
    "    \n",
    "    seq.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    seq.summary()\n",
    "    return seq\n",
    "\n",
    "def getLSTMAT():\n",
    "    model = Sequential()\n",
    "    kernel_size = 5\n",
    "    filters = 64\n",
    "    pool_size = 4\n",
    "    lstm_output_size=70\n",
    "    input1=Input(shape=(60,))   \n",
    "    x1=Embedding(input_dim=len(tfidfDict),\n",
    "                input_length=LengthOfInputSequences,\n",
    "                output_dim=20,   )(input1)\n",
    "\n",
    "    '''attentation'''\n",
    "    \n",
    "    #https://stackoverflow.com/questions/42918446/how-to-add-an-attention-mechanism-in-keras\n",
    "    #https://github.com/keras-team/keras/issues/9658\n",
    "    #https://github.com/keras-team/keras/issues/2403\n",
    "    attention=TimeDistributed(Dense(10))(x1)\n",
    "    attention=TimeDistributed(Dense(1))(attention)\n",
    "    attention=Activation('softmax')(attention)\n",
    "    \n",
    "    x1=Multiply()([attention,x1])\n",
    "    x1=keras.layers.Bidirectional(LSTM(50))(x1)\n",
    "    #only lstm ok\n",
    "    #test with blstm  ok\n",
    "    \n",
    "    \n",
    "    \n",
    "    #x1=keras.layers.Bidirectional(LSTM(50))(x1)\n",
    "    conv=Dense(200)(x1)\n",
    "    output=Dense(2)(conv)\n",
    "    \n",
    "    seq=keras.models.Model(inputs=[input1],outputs=output)\n",
    "    \n",
    "    seq.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    seq.summary()\n",
    "    return seq\n",
    "\n",
    "\n",
    "def getCNNAT():\n",
    "    model = Sequential()\n",
    "    kernel_size = 5\n",
    "    filters = 64\n",
    "    pool_size = 4\n",
    "    lstm_output_size=70\n",
    "    input1=Input(shape=(60,))   \n",
    "    em=Embedding(input_dim=len(tfidfDict),\n",
    "                input_length=LengthOfInputSequences,\n",
    "                output_dim=20,   )(input1)\n",
    "\n",
    "    '''attentation'''\n",
    "    \n",
    "    #https://stackoverflow.com/questions/42918446/how-to-add-an-attention-mechanism-in-keras\n",
    "    #https://github.com/keras-team/keras/issues/9658\n",
    "    #https://github.com/keras-team/keras/issues/2403\n",
    "    attention=TimeDistributed(Dense(10))(em)\n",
    "    attention=TimeDistributed(Dense(1))(attention)\n",
    "    attention=Activation('softmax')(attention)\n",
    "    \n",
    "    x1=Multiply()([attention,em])\n",
    "    x1=Conv1D(filters=64,kernel_size=kernel_size)(x1)\n",
    "    x1=keras.layers.AveragePooling1D(3)(x1)\n",
    "    x1=Conv1D(filters=64,kernel_size=kernel_size)(x1)\n",
    "    x1=keras.layers.AveragePooling1D(3)(x1)\n",
    "    x1=Flatten()(x1)\n",
    "    x2=keras.layers.AveragePooling1D(60)(em)\n",
    "    x2=Flatten()(x2)\n",
    "    x1=keras.layers.Concatenate()([x1,x2])\n",
    "    #only lstm ok\n",
    "    #test with blstm  ok\n",
    "    \n",
    "    \n",
    "    \n",
    "    #x1=keras.layers.Bidirectional(LSTM(50))(x1)\n",
    "    conv=Dense(200)(x1)\n",
    "    output=Dense(2)(conv)\n",
    "    \n",
    "    seq=keras.models.Model(inputs=[input1],outputs=output)\n",
    "    \n",
    "    seq.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    seq.summary()\n",
    "    return seq\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "crf=getCNNAT()\n",
    "\n",
    "#plot_model(crf, to_file='lstm-mlp.png',show_shapes=True)\n",
    "SVG(model_to_dot(crf, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LengthOfInputSequences=60\n",
    "trainCnnIndex=list(map(toCnnIndexAllinOne,trainxRaw))\n",
    "devCnnIndex=list(map(toCnnIndexAllinOne,devxRaw))\n",
    "testCnnIndex=list(map(toCnnIndexAllinOne,testxRaw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 60)\n",
      "(15000, 60)\n",
      "(20000, 60)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 60)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 60, 20)        825100      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 60, 10)        210         embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistribu (None, 60, 1)         11          time_distributed_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 60, 1)         0           time_distributed_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)            (None, 60, 20)        0           activation_1[0][0]               \n",
      "                                                                   embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 56, 64)        6464        multiply_1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling1d_1 (AveragePool (None, 18, 64)        0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 14, 64)        20544       average_pooling1d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling1d_2 (AveragePool (None, 4, 64)         0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling1d_3 (AveragePool (None, 1, 20)         0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 256)           0           average_pooling1d_2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 20)            0           average_pooling1d_3[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 276)           0           flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 200)           55400       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             402         dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 908,131\n",
      "Trainable params: 908,131\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 285000 samples, validate on 15000 samples\n",
      "Epoch 1/100\n",
      "284928/285000 [============================>.] - ETA: 0s - loss: 0.6662 - acc: 0.5941Epoch 00000: val_loss improved from inf to 0.64833, saving model to ./models/cnn_0.hdf5\n",
      "285000/285000 [==============================] - 12s - loss: 0.6662 - acc: 0.5941 - val_loss: 0.6483 - val_acc: 0.6205\n",
      "Epoch 2/100\n",
      "284416/285000 [============================>.] - ETA: 0s - loss: 0.6115 - acc: 0.6627\n",
      "Epoch 00001: reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 00001: val_loss did not improve\n",
      "285000/285000 [==============================] - 11s - loss: 0.6115 - acc: 0.6627 - val_loss: 0.6749 - val_acc: 0.6312\n",
      "Epoch 3/100\n",
      "284672/285000 [============================>.] - ETA: 0s - loss: 0.5179 - acc: 0.7378\n",
      "Epoch 00002: reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00002: val_loss did not improve\n",
      "285000/285000 [==============================] - 11s - loss: 0.5179 - acc: 0.7378 - val_loss: 0.8383 - val_acc: 0.6077\n",
      "Epoch 4/100\n",
      "283904/285000 [============================>.] - ETA: 0s - loss: 0.4863 - acc: 0.7564\n",
      "Epoch 00003: reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00003: val_loss did not improve\n",
      "285000/285000 [==============================] - 11s - loss: 0.4863 - acc: 0.7563 - val_loss: 0.8825 - val_acc: 0.6077\n",
      "Epoch 00003: early stopping\n",
      "------------0------CROSS-------\n",
      "[[4242 3258]\n",
      " [2350 5150]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.57      0.60      7500\n",
      "          1       0.61      0.69      0.65      7500\n",
      "\n",
      "avg / total       0.63      0.63      0.62     15000\n",
      "\n",
      "------------0------TEST-----------\n",
      "[[5507 4493]\n",
      " [3192 6808]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.55      0.59     10000\n",
      "          1       0.60      0.68      0.64     10000\n",
      "\n",
      "avg / total       0.62      0.62      0.61     20000\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 60)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 60, 20)        825100      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 60, 10)        210         embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistribu (None, 60, 1)         11          time_distributed_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 60, 1)         0           time_distributed_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)            (None, 60, 20)        0           activation_1[0][0]               \n",
      "                                                                   embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 56, 64)        6464        multiply_1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling1d_1 (AveragePool (None, 18, 64)        0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 14, 64)        20544       average_pooling1d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling1d_2 (AveragePool (None, 4, 64)         0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling1d_3 (AveragePool (None, 1, 20)         0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 256)           0           average_pooling1d_2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 20)            0           average_pooling1d_3[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 276)           0           flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 200)           55400       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             402         dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 908,131\n",
      "Trainable params: 908,131\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 285000 samples, validate on 15000 samples\n",
      "Epoch 1/100\n",
      "239872/285000 [========================>.....] - ETA: 1s - loss: 0.6816 - acc: 0.5823"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-e2687df1608c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainCnnIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnnTrainyCat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             )\n\u001b[1;32m     54\u001b[0m     \u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1180\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \"\"\"Called right before processing a batch.\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainCnnIndex=np.array(trainCnnIndex)\n",
    "print(trainCnnIndex.shape)\n",
    "devCnnIndex=np.array(devCnnIndex)\n",
    "print(devCnnIndex.shape)\n",
    "testCnnIndex=np.array(testCnnIndex)\n",
    "print(testCnnIndex.shape)\n",
    "\n",
    "cnnTrainy=[0 if s=='DUT' else 1 for s in trainy]\n",
    "cnnDevy=[0 if s=='DUT' else 1 for s in evaly]\n",
    "cnnTesty=[0 if s=='DUT' else 1 for s in testy]\n",
    "cnnTrainyCat=keras.utils.to_categorical(cnnTrainy)\n",
    "cnnDevyCat= keras.utils.to_categorical(cnnDevy)\n",
    "import os\n",
    "os.system('mkdir -p models')\n",
    "#fullTrainX=np.vstack((trainCnnIndex,devCnnIndex))\n",
    "#fullTrainY=np.concatenate((cnnTrainyCat,cnnDevyCat))\n",
    "sf=sklearn.model_selection.StratifiedKFold(20)\n",
    "cnnmodelCount=0\n",
    "for trainindex,devindex in sf.split(trainCnnIndex,cnnTrainy):\n",
    "    K.clear_session()\n",
    "    crf=getCNNAT()\n",
    "    modelFile='./models/cnn_%d.hdf5'%(cnnmodelCount)\n",
    "    callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                               patience=2,\n",
    "                               verbose=1,\n",
    "                               min_delta=0.01,\n",
    "                               mode='min'),\n",
    "                 ReduceLROnPlateau(monitor='val_loss',\n",
    "                                   factor=0.1,\n",
    "                                   patience=0,\n",
    "                                   verbose=1,\n",
    "                                   epsilon=0.0001,\n",
    "                                   mode='min'),\n",
    "                 ModelCheckpoint(monitor='val_loss',\n",
    "                                 filepath=modelFile,\n",
    "                                 save_best_only=True,\n",
    "                                 save_weights_only=True,\n",
    "                                 verbose=1,\n",
    "                                 mode='min'),\n",
    "                 ]\n",
    "    '''\n",
    "    crf.fit(trainCnnIndex,\n",
    "            cnnTrainyCat,\n",
    "            validation_data=(devCnnIndex,cnnDevyCat),\n",
    "            epochs=100,batch_size=256,\n",
    "            callbacks=callbacks)\n",
    "            '''\n",
    "    crf.fit(trainCnnIndex[trainindex],\n",
    "            cnnTrainyCat[trainindex],       \n",
    "            validation_data=(trainCnnIndex[devindex],cnnTrainyCat[devindex]),\n",
    "            epochs=100,batch_size=256,\n",
    "            callbacks=callbacks,\n",
    "            )\n",
    "    crf.load_weights(modelFile)\n",
    "    devCnnIndex=trainCnnIndex[devindex]\n",
    "    cnnDevy= np.argmax( cnnTrainyCat[devindex],axis=1)\n",
    "    predictValue=crf.predict(devCnnIndex)\n",
    "    \n",
    "    #predict=[0 if s <th else 1 for s in predictValue]\n",
    "    predict=np.argmax(predictValue,axis=1)\n",
    "    print('------------%d------CROSS-------'%(cnnmodelCount))\n",
    "    print(confusion_matrix(cnnDevy,predict))\n",
    "    print(classification_report(cnnDevy,predict))\n",
    "    \n",
    "    \n",
    "    #check out the real test\n",
    "    predictValue=crf.predict(testCnnIndex)\n",
    "    \n",
    "    #predict=[0 if s <th else 1 for s in predictValue]\n",
    "    predict=np.argmax(predictValue,axis=1)\n",
    "    print('------------%d------TEST-----------'%(cnnmodelCount))\n",
    "    print(confusion_matrix(cnnTesty,predict))\n",
    "    print(classification_report(cnnTesty,predict))\n",
    "    #print(predictValue.shape)\n",
    "    #fpr, tpr, thresholds = metrics.roc_curve(cnnDevy, predictValue)\n",
    "    #print(thresholds)\n",
    "    #print(i,metrics.auc(fpr, tpr))\n",
    "    cnnmodelCount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------0--0.031269---------------\n",
      "[[  0 250]\n",
      " [  0 250]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       250\n",
      "          1       0.50      1.00      0.67       250\n",
      "\n",
      "avg / total       0.25      0.50      0.33       500\n",
      "\n",
      "------------0--0.034581---------------\n",
      "[[  1 249]\n",
      " [  0 250]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.00      0.01       250\n",
      "          1       0.50      1.00      0.67       250\n",
      "\n",
      "avg / total       0.75      0.50      0.34       500\n",
      "\n",
      "------------0--0.045967---------------\n",
      "[[  2 248]\n",
      " [  0 250]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.01      0.02       250\n",
      "          1       0.50      1.00      0.67       250\n",
      "\n",
      "avg / total       0.75      0.50      0.34       500\n",
      "\n",
      "------------0--0.056271---------------\n",
      "[[  2 248]\n",
      " [  1 249]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.01      0.02       250\n",
      "          1       0.50      1.00      0.67       250\n",
      "\n",
      "avg / total       0.58      0.50      0.34       500\n",
      "\n",
      "------------0--0.059169---------------\n",
      "[[  3 247]\n",
      " [  1 249]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.01      0.02       250\n",
      "          1       0.50      1.00      0.67       250\n",
      "\n",
      "avg / total       0.63      0.50      0.35       500\n",
      "\n",
      "------------0--0.059399---------------\n",
      "[[  4 246]\n",
      " [  1 249]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.02      0.03       250\n",
      "          1       0.50      1.00      0.67       250\n",
      "\n",
      "avg / total       0.65      0.51      0.35       500\n",
      "\n",
      "------------0--0.061201---------------\n",
      "[[  5 245]\n",
      " [  1 249]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.02      0.04       250\n",
      "          1       0.50      1.00      0.67       250\n",
      "\n",
      "avg / total       0.67      0.51      0.35       500\n",
      "\n",
      "------------0--0.062802---------------\n",
      "[[  6 244]\n",
      " [  1 249]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.02      0.05       250\n",
      "          1       0.51      1.00      0.67       250\n",
      "\n",
      "avg / total       0.68      0.51      0.36       500\n",
      "\n",
      "------------0--0.063212---------------\n",
      "[[  6 244]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.02      0.05       250\n",
      "          1       0.50      0.99      0.67       250\n",
      "\n",
      "avg / total       0.63      0.51      0.36       500\n",
      "\n",
      "------------0--0.072930---------------\n",
      "[[  7 243]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.03      0.05       250\n",
      "          1       0.51      0.99      0.67       250\n",
      "\n",
      "avg / total       0.64      0.51      0.36       500\n",
      "\n",
      "------------0--0.073111---------------\n",
      "[[  8 242]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.03      0.06       250\n",
      "          1       0.51      0.99      0.67       250\n",
      "\n",
      "avg / total       0.65      0.51      0.37       500\n",
      "\n",
      "------------0--0.074242---------------\n",
      "[[  9 241]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.04      0.07       250\n",
      "          1       0.51      0.99      0.67       250\n",
      "\n",
      "avg / total       0.66      0.51      0.37       500\n",
      "\n",
      "------------0--0.074587---------------\n",
      "[[ 10 240]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.04      0.08       250\n",
      "          1       0.51      0.99      0.67       250\n",
      "\n",
      "avg / total       0.67      0.52      0.37       500\n",
      "\n",
      "------------0--0.075284---------------\n",
      "[[ 11 239]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.04      0.08       250\n",
      "          1       0.51      0.99      0.67       250\n",
      "\n",
      "avg / total       0.68      0.52      0.38       500\n",
      "\n",
      "------------0--0.077179---------------\n",
      "[[ 12 238]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.05      0.09       250\n",
      "          1       0.51      0.99      0.67       250\n",
      "\n",
      "avg / total       0.68      0.52      0.38       500\n",
      "\n",
      "------------0--0.079632---------------\n",
      "[[ 13 237]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.05      0.10       250\n",
      "          1       0.51      0.99      0.67       250\n",
      "\n",
      "avg / total       0.69      0.52      0.39       500\n",
      "\n",
      "------------0--0.086272---------------\n",
      "[[ 14 236]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.06      0.11       250\n",
      "          1       0.51      0.99      0.68       250\n",
      "\n",
      "avg / total       0.69      0.52      0.39       500\n",
      "\n",
      "------------0--0.086903---------------\n",
      "[[ 15 235]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.06      0.11       250\n",
      "          1       0.51      0.99      0.68       250\n",
      "\n",
      "avg / total       0.70      0.53      0.39       500\n",
      "\n",
      "------------0--0.088185---------------\n",
      "[[ 16 234]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.06      0.12       250\n",
      "          1       0.51      0.99      0.68       250\n",
      "\n",
      "avg / total       0.70      0.53      0.40       500\n",
      "\n",
      "------------0--0.095380---------------\n",
      "[[ 17 233]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.07      0.13       250\n",
      "          1       0.52      0.99      0.68       250\n",
      "\n",
      "avg / total       0.71      0.53      0.40       500\n",
      "\n",
      "------------0--0.095393---------------\n",
      "[[ 18 232]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.07      0.13       250\n",
      "          1       0.52      0.99      0.68       250\n",
      "\n",
      "avg / total       0.71      0.53      0.41       500\n",
      "\n",
      "------------0--0.095531---------------\n",
      "[[ 19 231]\n",
      " [  2 248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.08      0.14       250\n",
      "          1       0.52      0.99      0.68       250\n",
      "\n",
      "avg / total       0.71      0.53      0.41       500\n",
      "\n",
      "------------0--0.100094---------------\n",
      "[[ 19 231]\n",
      " [  3 247]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.08      0.14       250\n",
      "          1       0.52      0.99      0.68       250\n",
      "\n",
      "avg / total       0.69      0.53      0.41       500\n",
      "\n",
      "------------0--0.108099---------------\n",
      "[[ 20 230]\n",
      " [  3 247]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.08      0.15       250\n",
      "          1       0.52      0.99      0.68       250\n",
      "\n",
      "avg / total       0.69      0.53      0.41       500\n",
      "\n",
      "------------0--0.108769---------------\n",
      "[[ 21 229]\n",
      " [  3 247]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.08      0.15       250\n",
      "          1       0.52      0.99      0.68       250\n",
      "\n",
      "avg / total       0.70      0.54      0.42       500\n",
      "\n",
      "------------0--0.115624---------------\n",
      "[[ 22 228]\n",
      " [  3 247]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.09      0.16       250\n",
      "          1       0.52      0.99      0.68       250\n",
      "\n",
      "avg / total       0.70      0.54      0.42       500\n",
      "\n",
      "------------0--0.117321---------------\n",
      "[[ 23 227]\n",
      " [  3 247]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.09      0.17       250\n",
      "          1       0.52      0.99      0.68       250\n",
      "\n",
      "avg / total       0.70      0.54      0.42       500\n",
      "\n",
      "------------0--0.118355---------------\n",
      "[[ 24 226]\n",
      " [  3 247]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.10      0.17       250\n",
      "          1       0.52      0.99      0.68       250\n",
      "\n",
      "avg / total       0.71      0.54      0.43       500\n",
      "\n",
      "------------0--0.122744---------------\n",
      "[[ 25 225]\n",
      " [  3 247]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.10      0.18       250\n",
      "          1       0.52      0.99      0.68       250\n",
      "\n",
      "avg / total       0.71      0.54      0.43       500\n",
      "\n",
      "------------0--0.123245---------------\n",
      "[[ 26 224]\n",
      " [  3 247]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.10      0.19       250\n",
      "          1       0.52      0.99      0.69       250\n",
      "\n",
      "avg / total       0.71      0.55      0.44       500\n",
      "\n",
      "------------0--0.128864---------------\n",
      "[[ 26 224]\n",
      " [  4 246]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.10      0.19       250\n",
      "          1       0.52      0.98      0.68       250\n",
      "\n",
      "avg / total       0.70      0.54      0.43       500\n",
      "\n",
      "------------0--0.131601---------------\n",
      "[[ 26 224]\n",
      " [  5 245]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.10      0.19       250\n",
      "          1       0.52      0.98      0.68       250\n",
      "\n",
      "avg / total       0.68      0.54      0.43       500\n",
      "\n",
      "------------0--0.140288---------------\n",
      "[[ 27 223]\n",
      " [  5 245]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.11      0.19       250\n",
      "          1       0.52      0.98      0.68       250\n",
      "\n",
      "avg / total       0.68      0.54      0.44       500\n",
      "\n",
      "------------0--0.140483---------------\n",
      "[[ 28 222]\n",
      " [  5 245]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.11      0.20       250\n",
      "          1       0.52      0.98      0.68       250\n",
      "\n",
      "avg / total       0.69      0.55      0.44       500\n",
      "\n",
      "------------0--0.145764---------------\n",
      "[[ 29 221]\n",
      " [  5 245]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.12      0.20       250\n",
      "          1       0.53      0.98      0.68       250\n",
      "\n",
      "avg / total       0.69      0.55      0.44       500\n",
      "\n",
      "------------0--0.145856---------------\n",
      "[[ 30 220]\n",
      " [  5 245]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.12      0.21       250\n",
      "          1       0.53      0.98      0.69       250\n",
      "\n",
      "avg / total       0.69      0.55      0.45       500\n",
      "\n",
      "------------0--0.147122---------------\n",
      "[[ 31 219]\n",
      " [  5 245]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.12      0.22       250\n",
      "          1       0.53      0.98      0.69       250\n",
      "\n",
      "avg / total       0.69      0.55      0.45       500\n",
      "\n",
      "------------0--0.149855---------------\n",
      "[[ 32 218]\n",
      " [  5 245]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.13      0.22       250\n",
      "          1       0.53      0.98      0.69       250\n",
      "\n",
      "avg / total       0.70      0.55      0.46       500\n",
      "\n",
      "------------0--0.151496---------------\n",
      "[[ 32 218]\n",
      " [  6 244]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.13      0.22       250\n",
      "          1       0.53      0.98      0.69       250\n",
      "\n",
      "avg / total       0.69      0.55      0.45       500\n",
      "\n",
      "------------0--0.153167---------------\n",
      "[[ 33 217]\n",
      " [  6 244]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.13      0.23       250\n",
      "          1       0.53      0.98      0.69       250\n",
      "\n",
      "avg / total       0.69      0.55      0.46       500\n",
      "\n",
      "------------0--0.153367---------------\n",
      "[[ 33 217]\n",
      " [  7 243]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.13      0.23       250\n",
      "          1       0.53      0.97      0.68       250\n",
      "\n",
      "avg / total       0.68      0.55      0.46       500\n",
      "\n",
      "------------0--0.154256---------------\n",
      "[[ 33 217]\n",
      " [  8 242]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.13      0.23       250\n",
      "          1       0.53      0.97      0.68       250\n",
      "\n",
      "avg / total       0.67      0.55      0.45       500\n",
      "\n",
      "------------0--0.155035---------------\n",
      "[[ 33 217]\n",
      " [  9 241]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.13      0.23       250\n",
      "          1       0.53      0.96      0.68       250\n",
      "\n",
      "avg / total       0.66      0.55      0.45       500\n",
      "\n",
      "------------0--0.163944---------------\n",
      "[[ 33 217]\n",
      " [ 10 240]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.13      0.23       250\n",
      "          1       0.53      0.96      0.68       250\n",
      "\n",
      "avg / total       0.65      0.55      0.45       500\n",
      "\n",
      "------------0--0.167185---------------\n",
      "[[ 33 217]\n",
      " [ 11 239]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.13      0.22       250\n",
      "          1       0.52      0.96      0.68       250\n",
      "\n",
      "avg / total       0.64      0.54      0.45       500\n",
      "\n",
      "------------0--0.168591---------------\n",
      "[[ 33 217]\n",
      " [ 12 238]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.13      0.22       250\n",
      "          1       0.52      0.95      0.68       250\n",
      "\n",
      "avg / total       0.63      0.54      0.45       500\n",
      "\n",
      "------------0--0.171897---------------\n",
      "[[ 34 216]\n",
      " [ 12 238]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.14      0.23       250\n",
      "          1       0.52      0.95      0.68       250\n",
      "\n",
      "avg / total       0.63      0.54      0.45       500\n",
      "\n",
      "------------0--0.173299---------------\n",
      "[[ 35 215]\n",
      " [ 12 238]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.14      0.24       250\n",
      "          1       0.53      0.95      0.68       250\n",
      "\n",
      "avg / total       0.64      0.55      0.46       500\n",
      "\n",
      "------------0--0.173957---------------\n",
      "[[ 36 214]\n",
      " [ 12 238]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.14      0.24       250\n",
      "          1       0.53      0.95      0.68       250\n",
      "\n",
      "avg / total       0.64      0.55      0.46       500\n",
      "\n",
      "------------0--0.174321---------------\n",
      "[[ 36 214]\n",
      " [ 13 237]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.14      0.24       250\n",
      "          1       0.53      0.95      0.68       250\n",
      "\n",
      "avg / total       0.63      0.55      0.46       500\n",
      "\n",
      "------------0--0.174871---------------\n",
      "[[ 37 213]\n",
      " [ 13 237]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.15      0.25       250\n",
      "          1       0.53      0.95      0.68       250\n",
      "\n",
      "avg / total       0.63      0.55      0.46       500\n",
      "\n",
      "------------0--0.176718---------------\n",
      "[[ 38 212]\n",
      " [ 13 237]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.15      0.25       250\n",
      "          1       0.53      0.95      0.68       250\n",
      "\n",
      "avg / total       0.64      0.55      0.47       500\n",
      "\n",
      "------------0--0.179949---------------\n",
      "[[ 39 211]\n",
      " [ 13 237]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.16      0.26       250\n",
      "          1       0.53      0.95      0.68       250\n",
      "\n",
      "avg / total       0.64      0.55      0.47       500\n",
      "\n",
      "------------0--0.182875---------------\n",
      "[[ 39 211]\n",
      " [ 14 236]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.16      0.26       250\n",
      "          1       0.53      0.94      0.68       250\n",
      "\n",
      "avg / total       0.63      0.55      0.47       500\n",
      "\n",
      "------------0--0.185313---------------\n",
      "[[ 40 210]\n",
      " [ 14 236]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.16      0.26       250\n",
      "          1       0.53      0.94      0.68       250\n",
      "\n",
      "avg / total       0.63      0.55      0.47       500\n",
      "\n",
      "------------0--0.188505---------------\n",
      "[[ 41 209]\n",
      " [ 14 236]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.16      0.27       250\n",
      "          1       0.53      0.94      0.68       250\n",
      "\n",
      "avg / total       0.64      0.55      0.47       500\n",
      "\n",
      "------------0--0.190070---------------\n",
      "[[ 42 208]\n",
      " [ 14 236]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.17      0.27       250\n",
      "          1       0.53      0.94      0.68       250\n",
      "\n",
      "avg / total       0.64      0.56      0.48       500\n",
      "\n",
      "------------0--0.195282---------------\n",
      "[[ 43 207]\n",
      " [ 14 236]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.17      0.28       250\n",
      "          1       0.53      0.94      0.68       250\n",
      "\n",
      "avg / total       0.64      0.56      0.48       500\n",
      "\n",
      "------------0--0.195553---------------\n",
      "[[ 44 206]\n",
      " [ 14 236]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.18      0.29       250\n",
      "          1       0.53      0.94      0.68       250\n",
      "\n",
      "avg / total       0.65      0.56      0.48       500\n",
      "\n",
      "------------0--0.195664---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 45 205]\n",
      " [ 14 236]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.18      0.29       250\n",
      "          1       0.54      0.94      0.68       250\n",
      "\n",
      "avg / total       0.65      0.56      0.49       500\n",
      "\n",
      "------------0--0.195845---------------\n",
      "[[ 46 204]\n",
      " [ 14 236]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.18      0.30       250\n",
      "          1       0.54      0.94      0.68       250\n",
      "\n",
      "avg / total       0.65      0.56      0.49       500\n",
      "\n",
      "------------0--0.196674---------------\n",
      "[[ 46 204]\n",
      " [ 15 235]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.18      0.30       250\n",
      "          1       0.54      0.94      0.68       250\n",
      "\n",
      "avg / total       0.64      0.56      0.49       500\n",
      "\n",
      "------------0--0.197124---------------\n",
      "[[ 47 203]\n",
      " [ 15 235]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.19      0.30       250\n",
      "          1       0.54      0.94      0.68       250\n",
      "\n",
      "avg / total       0.65      0.56      0.49       500\n",
      "\n",
      "------------0--0.202631---------------\n",
      "[[ 48 202]\n",
      " [ 15 235]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.19      0.31       250\n",
      "          1       0.54      0.94      0.68       250\n",
      "\n",
      "avg / total       0.65      0.57      0.50       500\n",
      "\n",
      "------------0--0.204023---------------\n",
      "[[ 49 201]\n",
      " [ 15 235]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.20      0.31       250\n",
      "          1       0.54      0.94      0.69       250\n",
      "\n",
      "avg / total       0.65      0.57      0.50       500\n",
      "\n",
      "------------0--0.204903---------------\n",
      "[[ 50 200]\n",
      " [ 15 235]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.20      0.32       250\n",
      "          1       0.54      0.94      0.69       250\n",
      "\n",
      "avg / total       0.65      0.57      0.50       500\n",
      "\n",
      "------------0--0.205272---------------\n",
      "[[ 51 199]\n",
      " [ 15 235]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.20      0.32       250\n",
      "          1       0.54      0.94      0.69       250\n",
      "\n",
      "avg / total       0.66      0.57      0.50       500\n",
      "\n",
      "------------0--0.208082---------------\n",
      "[[ 52 198]\n",
      " [ 15 235]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.21      0.33       250\n",
      "          1       0.54      0.94      0.69       250\n",
      "\n",
      "avg / total       0.66      0.57      0.51       500\n",
      "\n",
      "------------0--0.211898---------------\n",
      "[[ 52 198]\n",
      " [ 16 234]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.21      0.33       250\n",
      "          1       0.54      0.94      0.69       250\n",
      "\n",
      "avg / total       0.65      0.57      0.51       500\n",
      "\n",
      "------------0--0.213628---------------\n",
      "[[ 53 197]\n",
      " [ 16 234]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.21      0.33       250\n",
      "          1       0.54      0.94      0.69       250\n",
      "\n",
      "avg / total       0.66      0.57      0.51       500\n",
      "\n",
      "------------0--0.213866---------------\n",
      "[[ 54 196]\n",
      " [ 16 234]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.22      0.34       250\n",
      "          1       0.54      0.94      0.69       250\n",
      "\n",
      "avg / total       0.66      0.58      0.51       500\n",
      "\n",
      "------------0--0.216125---------------\n",
      "[[ 55 195]\n",
      " [ 16 234]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.22      0.34       250\n",
      "          1       0.55      0.94      0.69       250\n",
      "\n",
      "avg / total       0.66      0.58      0.52       500\n",
      "\n",
      "------------0--0.217964---------------\n",
      "[[ 56 194]\n",
      " [ 16 234]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.22      0.35       250\n",
      "          1       0.55      0.94      0.69       250\n",
      "\n",
      "avg / total       0.66      0.58      0.52       500\n",
      "\n",
      "------------0--0.218030---------------\n",
      "[[ 56 194]\n",
      " [ 17 233]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.22      0.35       250\n",
      "          1       0.55      0.93      0.69       250\n",
      "\n",
      "avg / total       0.66      0.58      0.52       500\n",
      "\n",
      "------------0--0.218435---------------\n",
      "[[ 56 194]\n",
      " [ 18 232]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.22      0.35       250\n",
      "          1       0.54      0.93      0.69       250\n",
      "\n",
      "avg / total       0.65      0.58      0.52       500\n",
      "\n",
      "------------0--0.219129---------------\n",
      "[[ 57 193]\n",
      " [ 18 232]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.23      0.35       250\n",
      "          1       0.55      0.93      0.69       250\n",
      "\n",
      "avg / total       0.65      0.58      0.52       500\n",
      "\n",
      "------------0--0.220109---------------\n",
      "[[ 58 192]\n",
      " [ 18 232]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.23      0.36       250\n",
      "          1       0.55      0.93      0.69       250\n",
      "\n",
      "avg / total       0.66      0.58      0.52       500\n",
      "\n",
      "------------0--0.221760---------------\n",
      "[[ 58 192]\n",
      " [ 19 231]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.23      0.35       250\n",
      "          1       0.55      0.92      0.69       250\n",
      "\n",
      "avg / total       0.65      0.58      0.52       500\n",
      "\n",
      "------------0--0.224511---------------\n",
      "[[ 59 191]\n",
      " [ 19 231]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.24      0.36       250\n",
      "          1       0.55      0.92      0.69       250\n",
      "\n",
      "avg / total       0.65      0.58      0.52       500\n",
      "\n",
      "------------0--0.224577---------------\n",
      "[[ 60 190]\n",
      " [ 19 231]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.24      0.36       250\n",
      "          1       0.55      0.92      0.69       250\n",
      "\n",
      "avg / total       0.65      0.58      0.53       500\n",
      "\n",
      "------------0--0.224790---------------\n",
      "[[ 61 189]\n",
      " [ 19 231]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.24      0.37       250\n",
      "          1       0.55      0.92      0.69       250\n",
      "\n",
      "avg / total       0.66      0.58      0.53       500\n",
      "\n",
      "------------0--0.225314---------------\n",
      "[[ 62 188]\n",
      " [ 19 231]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.25      0.37       250\n",
      "          1       0.55      0.92      0.69       250\n",
      "\n",
      "avg / total       0.66      0.59      0.53       500\n",
      "\n",
      "------------0--0.226232---------------\n",
      "[[ 63 187]\n",
      " [ 19 231]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.25      0.38       250\n",
      "          1       0.55      0.92      0.69       250\n",
      "\n",
      "avg / total       0.66      0.59      0.54       500\n",
      "\n",
      "------------0--0.226289---------------\n",
      "[[ 64 186]\n",
      " [ 19 231]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.26      0.38       250\n",
      "          1       0.55      0.92      0.69       250\n",
      "\n",
      "avg / total       0.66      0.59      0.54       500\n",
      "\n",
      "------------0--0.228198---------------\n",
      "[[ 65 185]\n",
      " [ 19 231]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.26      0.39       250\n",
      "          1       0.56      0.92      0.69       250\n",
      "\n",
      "avg / total       0.66      0.59      0.54       500\n",
      "\n",
      "------------0--0.229064---------------\n",
      "[[ 65 185]\n",
      " [ 20 230]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.26      0.39       250\n",
      "          1       0.55      0.92      0.69       250\n",
      "\n",
      "avg / total       0.66      0.59      0.54       500\n",
      "\n",
      "------------0--0.234478---------------\n",
      "[[ 65 185]\n",
      " [ 21 229]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.26      0.39       250\n",
      "          1       0.55      0.92      0.69       250\n",
      "\n",
      "avg / total       0.65      0.59      0.54       500\n",
      "\n",
      "------------0--0.234529---------------\n",
      "[[ 65 185]\n",
      " [ 22 228]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.26      0.39       250\n",
      "          1       0.55      0.91      0.69       250\n",
      "\n",
      "avg / total       0.65      0.59      0.54       500\n",
      "\n",
      "------------0--0.235177---------------\n",
      "[[ 66 184]\n",
      " [ 22 228]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.26      0.39       250\n",
      "          1       0.55      0.91      0.69       250\n",
      "\n",
      "avg / total       0.65      0.59      0.54       500\n",
      "\n",
      "------------0--0.236951---------------\n",
      "[[ 66 184]\n",
      " [ 23 227]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.26      0.39       250\n",
      "          1       0.55      0.91      0.69       250\n",
      "\n",
      "avg / total       0.65      0.59      0.54       500\n",
      "\n",
      "------------0--0.237814---------------\n",
      "[[ 66 184]\n",
      " [ 24 226]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.26      0.39       250\n",
      "          1       0.55      0.90      0.68       250\n",
      "\n",
      "avg / total       0.64      0.58      0.54       500\n",
      "\n",
      "------------0--0.238645---------------\n",
      "[[ 67 183]\n",
      " [ 24 226]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.27      0.39       250\n",
      "          1       0.55      0.90      0.69       250\n",
      "\n",
      "avg / total       0.64      0.59      0.54       500\n",
      "\n",
      "------------0--0.240132---------------\n",
      "[[ 68 182]\n",
      " [ 24 226]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.27      0.40       250\n",
      "          1       0.55      0.90      0.69       250\n",
      "\n",
      "avg / total       0.65      0.59      0.54       500\n",
      "\n",
      "------------0--0.241280---------------\n",
      "[[ 69 181]\n",
      " [ 24 226]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.28      0.40       250\n",
      "          1       0.56      0.90      0.69       250\n",
      "\n",
      "avg / total       0.65      0.59      0.55       500\n",
      "\n",
      "------------0--0.246642---------------\n",
      "[[ 69 181]\n",
      " [ 25 225]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.28      0.40       250\n",
      "          1       0.55      0.90      0.69       250\n",
      "\n",
      "avg / total       0.64      0.59      0.54       500\n",
      "\n",
      "------------0--0.247110---------------\n",
      "[[ 70 180]\n",
      " [ 25 225]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.28      0.41       250\n",
      "          1       0.56      0.90      0.69       250\n",
      "\n",
      "avg / total       0.65      0.59      0.55       500\n",
      "\n",
      "------------0--0.248423---------------\n",
      "[[ 71 179]\n",
      " [ 25 225]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.28      0.41       250\n",
      "          1       0.56      0.90      0.69       250\n",
      "\n",
      "avg / total       0.65      0.59      0.55       500\n",
      "\n",
      "------------0--0.251843---------------\n",
      "[[ 71 179]\n",
      " [ 26 224]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.28      0.41       250\n",
      "          1       0.56      0.90      0.69       250\n",
      "\n",
      "avg / total       0.64      0.59      0.55       500\n",
      "\n",
      "------------0--0.252917---------------\n",
      "[[ 71 179]\n",
      " [ 27 223]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.28      0.41       250\n",
      "          1       0.55      0.89      0.68       250\n",
      "\n",
      "avg / total       0.64      0.59      0.55       500\n",
      "\n",
      "------------0--0.253575---------------\n",
      "[[ 72 178]\n",
      " [ 27 223]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.29      0.41       250\n",
      "          1       0.56      0.89      0.69       250\n",
      "\n",
      "avg / total       0.64      0.59      0.55       500\n",
      "\n",
      "------------0--0.253850---------------\n",
      "[[ 73 177]\n",
      " [ 27 223]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.29      0.42       250\n",
      "          1       0.56      0.89      0.69       250\n",
      "\n",
      "avg / total       0.64      0.59      0.55       500\n",
      "\n",
      "------------0--0.254498---------------\n",
      "[[ 74 176]\n",
      " [ 27 223]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.30      0.42       250\n",
      "          1       0.56      0.89      0.69       250\n",
      "\n",
      "avg / total       0.65      0.59      0.55       500\n",
      "\n",
      "------------0--0.258728---------------\n",
      "[[ 74 176]\n",
      " [ 28 222]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.30      0.42       250\n",
      "          1       0.56      0.89      0.69       250\n",
      "\n",
      "avg / total       0.64      0.59      0.55       500\n",
      "\n",
      "------------0--0.261053---------------\n",
      "[[ 75 175]\n",
      " [ 28 222]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.30      0.42       250\n",
      "          1       0.56      0.89      0.69       250\n",
      "\n",
      "avg / total       0.64      0.59      0.56       500\n",
      "\n",
      "------------0--0.261986---------------\n",
      "[[ 75 175]\n",
      " [ 29 221]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.30      0.42       250\n",
      "          1       0.56      0.88      0.68       250\n",
      "\n",
      "avg / total       0.64      0.59      0.55       500\n",
      "\n",
      "------------0--0.262188---------------\n",
      "[[ 75 175]\n",
      " [ 30 220]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.30      0.42       250\n",
      "          1       0.56      0.88      0.68       250\n",
      "\n",
      "avg / total       0.64      0.59      0.55       500\n",
      "\n",
      "------------0--0.263333---------------\n",
      "[[ 76 174]\n",
      " [ 30 220]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.30      0.43       250\n",
      "          1       0.56      0.88      0.68       250\n",
      "\n",
      "avg / total       0.64      0.59      0.56       500\n",
      "\n",
      "------------0--0.264351---------------\n",
      "[[ 77 173]\n",
      " [ 30 220]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.31      0.43       250\n",
      "          1       0.56      0.88      0.68       250\n",
      "\n",
      "avg / total       0.64      0.59      0.56       500\n",
      "\n",
      "------------0--0.264440---------------\n",
      "[[ 78 172]\n",
      " [ 30 220]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.31      0.44       250\n",
      "          1       0.56      0.88      0.69       250\n",
      "\n",
      "avg / total       0.64      0.60      0.56       500\n",
      "\n",
      "------------0--0.265639---------------\n",
      "[[ 79 171]\n",
      " [ 30 220]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.32      0.44       250\n",
      "          1       0.56      0.88      0.69       250\n",
      "\n",
      "avg / total       0.64      0.60      0.56       500\n",
      "\n",
      "------------0--0.266923---------------\n",
      "[[ 79 171]\n",
      " [ 31 219]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.32      0.44       250\n",
      "          1       0.56      0.88      0.68       250\n",
      "\n",
      "avg / total       0.64      0.60      0.56       500\n",
      "\n",
      "------------0--0.267142---------------\n",
      "[[ 79 171]\n",
      " [ 32 218]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.32      0.44       250\n",
      "          1       0.56      0.87      0.68       250\n",
      "\n",
      "avg / total       0.64      0.59      0.56       500\n",
      "\n",
      "------------0--0.267477---------------\n",
      "[[ 79 171]\n",
      " [ 33 217]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.32      0.44       250\n",
      "          1       0.56      0.87      0.68       250\n",
      "\n",
      "avg / total       0.63      0.59      0.56       500\n",
      "\n",
      "------------0--0.269801---------------\n",
      "[[ 79 171]\n",
      " [ 34 216]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.32      0.44       250\n",
      "          1       0.56      0.86      0.68       250\n",
      "\n",
      "avg / total       0.63      0.59      0.56       500\n",
      "\n",
      "------------0--0.271339---------------\n",
      "[[ 80 170]\n",
      " [ 34 216]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.32      0.44       250\n",
      "          1       0.56      0.86      0.68       250\n",
      "\n",
      "avg / total       0.63      0.59      0.56       500\n",
      "\n",
      "------------0--0.271393---------------\n",
      "[[ 80 170]\n",
      " [ 35 215]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.32      0.44       250\n",
      "          1       0.56      0.86      0.68       250\n",
      "\n",
      "avg / total       0.63      0.59      0.56       500\n",
      "\n",
      "------------0--0.274622---------------\n",
      "[[ 81 169]\n",
      " [ 35 215]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.32      0.44       250\n",
      "          1       0.56      0.86      0.68       250\n",
      "\n",
      "avg / total       0.63      0.59      0.56       500\n",
      "\n",
      "------------0--0.275167---------------\n",
      "[[ 82 168]\n",
      " [ 35 215]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.33      0.45       250\n",
      "          1       0.56      0.86      0.68       250\n",
      "\n",
      "avg / total       0.63      0.59      0.56       500\n",
      "\n",
      "------------0--0.277023---------------\n",
      "[[ 83 167]\n",
      " [ 35 215]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.33      0.45       250\n",
      "          1       0.56      0.86      0.68       250\n",
      "\n",
      "avg / total       0.63      0.60      0.57       500\n",
      "\n",
      "------------0--0.277963---------------\n",
      "[[ 83 167]\n",
      " [ 36 214]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.33      0.45       250\n",
      "          1       0.56      0.86      0.68       250\n",
      "\n",
      "avg / total       0.63      0.59      0.56       500\n",
      "\n",
      "------------0--0.279440---------------\n",
      "[[ 83 167]\n",
      " [ 37 213]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.33      0.45       250\n",
      "          1       0.56      0.85      0.68       250\n",
      "\n",
      "avg / total       0.63      0.59      0.56       500\n",
      "\n",
      "------------0--0.282059---------------\n",
      "[[ 84 166]\n",
      " [ 37 213]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.34      0.45       250\n",
      "          1       0.56      0.85      0.68       250\n",
      "\n",
      "avg / total       0.63      0.59      0.57       500\n",
      "\n",
      "------------0--0.282067---------------\n",
      "[[ 84 166]\n",
      " [ 38 212]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.34      0.45       250\n",
      "          1       0.56      0.85      0.68       250\n",
      "\n",
      "avg / total       0.62      0.59      0.56       500\n",
      "\n",
      "------------0--0.284023---------------\n",
      "[[ 84 166]\n",
      " [ 39 211]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.34      0.45       250\n",
      "          1       0.56      0.84      0.67       250\n",
      "\n",
      "avg / total       0.62      0.59      0.56       500\n",
      "\n",
      "------------0--0.284876---------------\n",
      "[[ 84 166]\n",
      " [ 40 210]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.34      0.45       250\n",
      "          1       0.56      0.84      0.67       250\n",
      "\n",
      "avg / total       0.62      0.59      0.56       500\n",
      "\n",
      "------------0--0.286024---------------\n",
      "[[ 85 165]\n",
      " [ 40 210]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.34      0.45       250\n",
      "          1       0.56      0.84      0.67       250\n",
      "\n",
      "avg / total       0.62      0.59      0.56       500\n",
      "\n",
      "------------0--0.288103---------------\n",
      "[[ 85 165]\n",
      " [ 41 209]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.34      0.45       250\n",
      "          1       0.56      0.84      0.67       250\n",
      "\n",
      "avg / total       0.62      0.59      0.56       500\n",
      "\n",
      "------------0--0.288522---------------\n",
      "[[ 86 164]\n",
      " [ 41 209]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.34      0.46       250\n",
      "          1       0.56      0.84      0.67       250\n",
      "\n",
      "avg / total       0.62      0.59      0.56       500\n",
      "\n",
      "------------0--0.290375---------------\n",
      "[[ 86 164]\n",
      " [ 42 208]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.34      0.46       250\n",
      "          1       0.56      0.83      0.67       250\n",
      "\n",
      "avg / total       0.62      0.59      0.56       500\n",
      "\n",
      "------------0--0.290916---------------\n",
      "[[ 87 163]\n",
      " [ 42 208]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.35      0.46       250\n",
      "          1       0.56      0.83      0.67       250\n",
      "\n",
      "avg / total       0.62      0.59      0.56       500\n",
      "\n",
      "------------0--0.291924---------------\n",
      "[[ 88 162]\n",
      " [ 42 208]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.35      0.46       250\n",
      "          1       0.56      0.83      0.67       250\n",
      "\n",
      "avg / total       0.62      0.59      0.57       500\n",
      "\n",
      "------------0--0.292603---------------\n",
      "[[ 88 162]\n",
      " [ 43 207]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.35      0.46       250\n",
      "          1       0.56      0.83      0.67       250\n",
      "\n",
      "avg / total       0.62      0.59      0.57       500\n",
      "\n",
      "------------0--0.293245---------------\n",
      "[[ 88 162]\n",
      " [ 44 206]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.35      0.46       250\n",
      "          1       0.56      0.82      0.67       250\n",
      "\n",
      "avg / total       0.61      0.59      0.56       500\n",
      "\n",
      "------------0--0.293417---------------\n",
      "[[ 89 161]\n",
      " [ 44 206]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.36      0.46       250\n",
      "          1       0.56      0.82      0.67       250\n",
      "\n",
      "avg / total       0.62      0.59      0.57       500\n",
      "\n",
      "------------0--0.300472---------------\n",
      "[[ 89 161]\n",
      " [ 45 205]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.36      0.46       250\n",
      "          1       0.56      0.82      0.67       250\n",
      "\n",
      "avg / total       0.61      0.59      0.56       500\n",
      "\n",
      "------------0--0.300887---------------\n",
      "[[ 90 160]\n",
      " [ 45 205]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.36      0.47       250\n",
      "          1       0.56      0.82      0.67       250\n",
      "\n",
      "avg / total       0.61      0.59      0.57       500\n",
      "\n",
      "------------0--0.302420---------------\n",
      "[[ 90 160]\n",
      " [ 46 204]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.36      0.47       250\n",
      "          1       0.56      0.82      0.66       250\n",
      "\n",
      "avg / total       0.61      0.59      0.57       500\n",
      "\n",
      "------------0--0.303129---------------\n",
      "[[ 90 160]\n",
      " [ 47 203]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.36      0.47       250\n",
      "          1       0.56      0.81      0.66       250\n",
      "\n",
      "avg / total       0.61      0.59      0.56       500\n",
      "\n",
      "------------0--0.303363---------------\n",
      "[[ 90 160]\n",
      " [ 48 202]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.36      0.46       250\n",
      "          1       0.56      0.81      0.66       250\n",
      "\n",
      "avg / total       0.61      0.58      0.56       500\n",
      "\n",
      "------------0--0.304617---------------\n",
      "[[ 90 160]\n",
      " [ 49 201]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.36      0.46       250\n",
      "          1       0.56      0.80      0.66       250\n",
      "\n",
      "avg / total       0.60      0.58      0.56       500\n",
      "\n",
      "------------0--0.306623---------------\n",
      "[[ 90 160]\n",
      " [ 50 200]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.36      0.46       250\n",
      "          1       0.56      0.80      0.66       250\n",
      "\n",
      "avg / total       0.60      0.58      0.56       500\n",
      "\n",
      "------------0--0.307986---------------\n",
      "[[ 90 160]\n",
      " [ 51 199]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.36      0.46       250\n",
      "          1       0.55      0.80      0.65       250\n",
      "\n",
      "avg / total       0.60      0.58      0.56       500\n",
      "\n",
      "------------0--0.309040---------------\n",
      "[[ 90 160]\n",
      " [ 52 198]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.36      0.46       250\n",
      "          1       0.55      0.79      0.65       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.309364---------------\n",
      "[[ 91 159]\n",
      " [ 52 198]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.36      0.46       250\n",
      "          1       0.55      0.79      0.65       250\n",
      "\n",
      "avg / total       0.60      0.58      0.56       500\n",
      "\n",
      "------------0--0.310435---------------\n",
      "[[ 92 158]\n",
      " [ 52 198]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.37      0.47       250\n",
      "          1       0.56      0.79      0.65       250\n",
      "\n",
      "avg / total       0.60      0.58      0.56       500\n",
      "\n",
      "------------0--0.310659---------------\n",
      "[[ 92 158]\n",
      " [ 53 197]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.37      0.47       250\n",
      "          1       0.55      0.79      0.65       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.310687---------------\n",
      "[[ 93 157]\n",
      " [ 53 197]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.37      0.47       250\n",
      "          1       0.56      0.79      0.65       250\n",
      "\n",
      "avg / total       0.60      0.58      0.56       500\n",
      "\n",
      "------------0--0.311267---------------\n",
      "[[ 93 157]\n",
      " [ 54 196]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.37      0.47       250\n",
      "          1       0.56      0.78      0.65       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.311863---------------\n",
      "[[ 93 157]\n",
      " [ 55 195]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.37      0.47       250\n",
      "          1       0.55      0.78      0.65       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.317075---------------\n",
      "[[ 94 156]\n",
      " [ 55 195]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.38      0.47       250\n",
      "          1       0.56      0.78      0.65       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.317277---------------\n",
      "[[ 94 156]\n",
      " [ 56 194]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.38      0.47       250\n",
      "          1       0.55      0.78      0.65       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.317686---------------\n",
      "[[ 94 156]\n",
      " [ 57 193]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.38      0.47       250\n",
      "          1       0.55      0.77      0.64       250\n",
      "\n",
      "avg / total       0.59      0.57      0.56       500\n",
      "\n",
      "------------0--0.319180---------------\n",
      "[[ 95 155]\n",
      " [ 57 193]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.38      0.47       250\n",
      "          1       0.55      0.77      0.65       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.319945---------------\n",
      "[[ 96 154]\n",
      " [ 57 193]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.38      0.48       250\n",
      "          1       0.56      0.77      0.65       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.320295---------------\n",
      "[[ 96 154]\n",
      " [ 58 192]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.38      0.48       250\n",
      "          1       0.55      0.77      0.64       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.320846---------------\n",
      "[[ 96 154]\n",
      " [ 59 191]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.38      0.47       250\n",
      "          1       0.55      0.76      0.64       250\n",
      "\n",
      "avg / total       0.59      0.57      0.56       500\n",
      "\n",
      "------------0--0.321423---------------\n",
      "[[ 97 153]\n",
      " [ 59 191]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.39      0.48       250\n",
      "          1       0.56      0.76      0.64       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.321434---------------\n",
      "[[ 98 152]\n",
      " [ 59 191]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.39      0.48       250\n",
      "          1       0.56      0.76      0.64       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.322932---------------\n",
      "[[ 98 152]\n",
      " [ 60 190]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.39      0.48       250\n",
      "          1       0.56      0.76      0.64       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.323603---------------\n",
      "[[ 98 152]\n",
      " [ 61 189]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.39      0.48       250\n",
      "          1       0.55      0.76      0.64       250\n",
      "\n",
      "avg / total       0.59      0.57      0.56       500\n",
      "\n",
      "------------0--0.323878---------------\n",
      "[[ 98 152]\n",
      " [ 62 188]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.39      0.48       250\n",
      "          1       0.55      0.75      0.64       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.324165---------------\n",
      "[[ 98 152]\n",
      " [ 63 187]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.39      0.48       250\n",
      "          1       0.55      0.75      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.324297---------------\n",
      "[[ 98 152]\n",
      " [ 64 186]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.39      0.48       250\n",
      "          1       0.55      0.74      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.55       500\n",
      "\n",
      "------------0--0.324556---------------\n",
      "[[ 98 152]\n",
      " [ 65 185]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.39      0.47       250\n",
      "          1       0.55      0.74      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.55       500\n",
      "\n",
      "------------0--0.324738---------------\n",
      "[[ 99 151]\n",
      " [ 65 185]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.40      0.48       250\n",
      "          1       0.55      0.74      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.55       500\n",
      "\n",
      "------------0--0.324896---------------\n",
      "[[100 150]\n",
      " [ 65 185]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.40      0.48       250\n",
      "          1       0.55      0.74      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.325103---------------\n",
      "[[101 149]\n",
      " [ 65 185]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.40      0.49       250\n",
      "          1       0.55      0.74      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.327295---------------\n",
      "[[102 148]\n",
      " [ 65 185]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.41      0.49       250\n",
      "          1       0.56      0.74      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.327692---------------\n",
      "[[102 148]\n",
      " [ 66 184]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.41      0.49       250\n",
      "          1       0.55      0.74      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.328162---------------\n",
      "[[103 147]\n",
      " [ 66 184]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.41      0.49       250\n",
      "          1       0.56      0.74      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.330774---------------\n",
      "[[103 147]\n",
      " [ 67 183]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.41      0.49       250\n",
      "          1       0.55      0.73      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.331332---------------\n",
      "[[103 147]\n",
      " [ 68 182]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.41      0.49       250\n",
      "          1       0.55      0.73      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.332174---------------\n",
      "[[104 146]\n",
      " [ 68 182]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.42      0.49       250\n",
      "          1       0.55      0.73      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.335088---------------\n",
      "[[105 145]\n",
      " [ 68 182]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.42      0.50       250\n",
      "          1       0.56      0.73      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.336276---------------\n",
      "[[105 145]\n",
      " [ 69 181]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.42      0.50       250\n",
      "          1       0.56      0.72      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.337230---------------\n",
      "[[105 145]\n",
      " [ 70 180]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.42      0.49       250\n",
      "          1       0.55      0.72      0.63       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.337631---------------\n",
      "[[105 145]\n",
      " [ 71 179]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.42      0.49       250\n",
      "          1       0.55      0.72      0.62       250\n",
      "\n",
      "avg / total       0.57      0.57      0.56       500\n",
      "\n",
      "------------0--0.337736---------------\n",
      "[[106 144]\n",
      " [ 71 179]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.42      0.50       250\n",
      "          1       0.55      0.72      0.62       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.338003---------------\n",
      "[[106 144]\n",
      " [ 72 178]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.42      0.50       250\n",
      "          1       0.55      0.71      0.62       250\n",
      "\n",
      "avg / total       0.57      0.57      0.56       500\n",
      "\n",
      "------------0--0.342199---------------\n",
      "[[107 143]\n",
      " [ 72 178]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.43      0.50       250\n",
      "          1       0.55      0.71      0.62       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.344284---------------\n",
      "[[107 143]\n",
      " [ 73 177]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.43      0.50       250\n",
      "          1       0.55      0.71      0.62       250\n",
      "\n",
      "avg / total       0.57      0.57      0.56       500\n",
      "\n",
      "------------0--0.344888---------------\n",
      "[[108 142]\n",
      " [ 73 177]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.43      0.50       250\n",
      "          1       0.55      0.71      0.62       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.345219---------------\n",
      "[[108 142]\n",
      " [ 74 176]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.43      0.50       250\n",
      "          1       0.55      0.70      0.62       250\n",
      "\n",
      "avg / total       0.57      0.57      0.56       500\n",
      "\n",
      "------------0--0.347149---------------\n",
      "[[109 141]\n",
      " [ 74 176]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.44      0.50       250\n",
      "          1       0.56      0.70      0.62       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.351246---------------\n",
      "[[110 140]\n",
      " [ 74 176]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.44      0.51       250\n",
      "          1       0.56      0.70      0.62       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.351543---------------\n",
      "[[111 139]\n",
      " [ 74 176]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.44      0.51       250\n",
      "          1       0.56      0.70      0.62       250\n",
      "\n",
      "avg / total       0.58      0.57      0.57       500\n",
      "\n",
      "------------0--0.351624---------------\n",
      "[[112 138]\n",
      " [ 74 176]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.45      0.51       250\n",
      "          1       0.56      0.70      0.62       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.357041---------------\n",
      "[[112 138]\n",
      " [ 75 175]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.45      0.51       250\n",
      "          1       0.56      0.70      0.62       250\n",
      "\n",
      "avg / total       0.58      0.57      0.57       500\n",
      "\n",
      "------------0--0.357654---------------\n",
      "[[113 137]\n",
      " [ 75 175]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.45      0.52       250\n",
      "          1       0.56      0.70      0.62       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.357702---------------\n",
      "[[114 136]\n",
      " [ 75 175]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.46      0.52       250\n",
      "          1       0.56      0.70      0.62       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.358220---------------\n",
      "[[114 136]\n",
      " [ 76 174]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.46      0.52       250\n",
      "          1       0.56      0.70      0.62       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.358378---------------\n",
      "[[114 136]\n",
      " [ 77 173]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.46      0.52       250\n",
      "          1       0.56      0.69      0.62       250\n",
      "\n",
      "avg / total       0.58      0.57      0.57       500\n",
      "\n",
      "------------0--0.361724---------------\n",
      "[[115 135]\n",
      " [ 77 173]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.46      0.52       250\n",
      "          1       0.56      0.69      0.62       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.364022---------------\n",
      "[[116 134]\n",
      " [ 77 173]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.46      0.52       250\n",
      "          1       0.56      0.69      0.62       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.364592---------------\n",
      "[[116 134]\n",
      " [ 78 172]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.46      0.52       250\n",
      "          1       0.56      0.69      0.62       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.364710---------------\n",
      "[[116 134]\n",
      " [ 79 171]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.46      0.52       250\n",
      "          1       0.56      0.68      0.62       250\n",
      "\n",
      "avg / total       0.58      0.57      0.57       500\n",
      "\n",
      "------------0--0.365660---------------\n",
      "[[116 134]\n",
      " [ 80 170]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.46      0.52       250\n",
      "          1       0.56      0.68      0.61       250\n",
      "\n",
      "avg / total       0.58      0.57      0.57       500\n",
      "\n",
      "------------0--0.366010---------------\n",
      "[[117 133]\n",
      " [ 80 170]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.47      0.52       250\n",
      "          1       0.56      0.68      0.61       250\n",
      "\n",
      "avg / total       0.58      0.57      0.57       500\n",
      "\n",
      "------------0--0.366202---------------\n",
      "[[118 132]\n",
      " [ 80 170]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.47      0.53       250\n",
      "          1       0.56      0.68      0.62       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.366554---------------\n",
      "[[119 131]\n",
      " [ 80 170]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.48      0.53       250\n",
      "          1       0.56      0.68      0.62       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.367198---------------\n",
      "[[119 131]\n",
      " [ 81 169]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.48      0.53       250\n",
      "          1       0.56      0.68      0.61       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.371511---------------\n",
      "[[119 131]\n",
      " [ 82 168]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.48      0.53       250\n",
      "          1       0.56      0.67      0.61       250\n",
      "\n",
      "avg / total       0.58      0.57      0.57       500\n",
      "\n",
      "------------0--0.372067---------------\n",
      "[[119 131]\n",
      " [ 83 167]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.48      0.53       250\n",
      "          1       0.56      0.67      0.61       250\n",
      "\n",
      "avg / total       0.57      0.57      0.57       500\n",
      "\n",
      "------------0--0.372744---------------\n",
      "[[119 131]\n",
      " [ 84 166]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.48      0.53       250\n",
      "          1       0.56      0.66      0.61       250\n",
      "\n",
      "avg / total       0.57      0.57      0.57       500\n",
      "\n",
      "------------0--0.373088---------------\n",
      "[[119 131]\n",
      " [ 85 165]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.48      0.52       250\n",
      "          1       0.56      0.66      0.60       250\n",
      "\n",
      "avg / total       0.57      0.57      0.56       500\n",
      "\n",
      "------------0--0.373223---------------\n",
      "[[120 130]\n",
      " [ 85 165]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.48      0.53       250\n",
      "          1       0.56      0.66      0.61       250\n",
      "\n",
      "avg / total       0.57      0.57      0.57       500\n",
      "\n",
      "------------0--0.373949---------------\n",
      "[[120 130]\n",
      " [ 86 164]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.48      0.53       250\n",
      "          1       0.56      0.66      0.60       250\n",
      "\n",
      "avg / total       0.57      0.57      0.56       500\n",
      "\n",
      "------------0--0.375547---------------\n",
      "[[121 129]\n",
      " [ 86 164]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.48      0.53       250\n",
      "          1       0.56      0.66      0.60       250\n",
      "\n",
      "avg / total       0.57      0.57      0.57       500\n",
      "\n",
      "------------0--0.376982---------------\n",
      "[[121 129]\n",
      " [ 87 163]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.48      0.53       250\n",
      "          1       0.56      0.65      0.60       250\n",
      "\n",
      "avg / total       0.57      0.57      0.56       500\n",
      "\n",
      "------------0--0.377078---------------\n",
      "[[122 128]\n",
      " [ 87 163]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.49      0.53       250\n",
      "          1       0.56      0.65      0.60       250\n",
      "\n",
      "avg / total       0.57      0.57      0.57       500\n",
      "\n",
      "------------0--0.378228---------------\n",
      "[[123 127]\n",
      " [ 87 163]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.49      0.53       250\n",
      "          1       0.56      0.65      0.60       250\n",
      "\n",
      "avg / total       0.57      0.57      0.57       500\n",
      "\n",
      "------------0--0.378639---------------\n",
      "[[123 127]\n",
      " [ 88 162]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.49      0.53       250\n",
      "          1       0.56      0.65      0.60       250\n",
      "\n",
      "avg / total       0.57      0.57      0.57       500\n",
      "\n",
      "------------0--0.380225---------------\n",
      "[[124 126]\n",
      " [ 88 162]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.50      0.54       250\n",
      "          1       0.56      0.65      0.60       250\n",
      "\n",
      "avg / total       0.57      0.57      0.57       500\n",
      "\n",
      "------------0--0.380449---------------\n",
      "[[124 126]\n",
      " [ 89 161]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.50      0.54       250\n",
      "          1       0.56      0.64      0.60       250\n",
      "\n",
      "avg / total       0.57      0.57      0.57       500\n",
      "\n",
      "------------0--0.382240---------------\n",
      "[[125 125]\n",
      " [ 89 161]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.50      0.54       250\n",
      "          1       0.56      0.64      0.60       250\n",
      "\n",
      "avg / total       0.57      0.57      0.57       500\n",
      "\n",
      "------------0--0.383324---------------\n",
      "[[125 125]\n",
      " [ 90 160]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.50      0.54       250\n",
      "          1       0.56      0.64      0.60       250\n",
      "\n",
      "avg / total       0.57      0.57      0.57       500\n",
      "\n",
      "------------0--0.383886---------------\n",
      "[[126 124]\n",
      " [ 90 160]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.50      0.54       250\n",
      "          1       0.56      0.64      0.60       250\n",
      "\n",
      "avg / total       0.57      0.57      0.57       500\n",
      "\n",
      "------------0--0.384817---------------\n",
      "[[127 123]\n",
      " [ 90 160]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.51      0.54       250\n",
      "          1       0.57      0.64      0.60       250\n",
      "\n",
      "avg / total       0.58      0.57      0.57       500\n",
      "\n",
      "------------0--0.384869---------------\n",
      "[[128 122]\n",
      " [ 90 160]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.51      0.55       250\n",
      "          1       0.57      0.64      0.60       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.385429---------------\n",
      "[[129 121]\n",
      " [ 90 160]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.52      0.55       250\n",
      "          1       0.57      0.64      0.60       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.385844---------------\n",
      "[[130 120]\n",
      " [ 90 160]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.52      0.55       250\n",
      "          1       0.57      0.64      0.60       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.387051---------------\n",
      "[[131 119]\n",
      " [ 90 160]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.52      0.56       250\n",
      "          1       0.57      0.64      0.60       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.388170---------------\n",
      "[[132 118]\n",
      " [ 90 160]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.53      0.56       250\n",
      "          1       0.58      0.64      0.61       250\n",
      "\n",
      "avg / total       0.59      0.58      0.58       500\n",
      "\n",
      "------------0--0.390797---------------\n",
      "[[132 118]\n",
      " [ 91 159]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.53      0.56       250\n",
      "          1       0.57      0.64      0.60       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.391281---------------\n",
      "[[133 117]\n",
      " [ 91 159]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.53      0.56       250\n",
      "          1       0.58      0.64      0.60       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.393017---------------\n",
      "[[134 116]\n",
      " [ 91 159]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.54      0.56       250\n",
      "          1       0.58      0.64      0.61       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.394846---------------\n",
      "[[134 116]\n",
      " [ 92 158]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.54      0.56       250\n",
      "          1       0.58      0.63      0.60       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.394928---------------\n",
      "[[134 116]\n",
      " [ 93 157]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.54      0.56       250\n",
      "          1       0.58      0.63      0.60       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.395107---------------\n",
      "[[135 115]\n",
      " [ 93 157]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.54      0.56       250\n",
      "          1       0.58      0.63      0.60       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.396995---------------\n",
      "[[136 114]\n",
      " [ 93 157]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.54      0.57       250\n",
      "          1       0.58      0.63      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.397259---------------\n",
      "[[137 113]\n",
      " [ 93 157]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.55      0.57       250\n",
      "          1       0.58      0.63      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.398608---------------\n",
      "[[138 112]\n",
      " [ 93 157]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.55      0.57       250\n",
      "          1       0.58      0.63      0.61       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.399565---------------\n",
      "[[139 111]\n",
      " [ 93 157]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.56      0.58       250\n",
      "          1       0.59      0.63      0.61       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.399932---------------\n",
      "[[139 111]\n",
      " [ 94 156]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.56      0.58       250\n",
      "          1       0.58      0.62      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.400745---------------\n",
      "[[139 111]\n",
      " [ 95 155]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.56      0.57       250\n",
      "          1       0.58      0.62      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.401096---------------\n",
      "[[140 110]\n",
      " [ 95 155]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.56      0.58       250\n",
      "          1       0.58      0.62      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.402152---------------\n",
      "[[140 110]\n",
      " [ 96 154]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.56      0.58       250\n",
      "          1       0.58      0.62      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.403535---------------\n",
      "[[140 110]\n",
      " [ 97 153]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.56      0.57       250\n",
      "          1       0.58      0.61      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.403963---------------\n",
      "[[141 109]\n",
      " [ 97 153]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.56      0.58       250\n",
      "          1       0.58      0.61      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.405058---------------\n",
      "[[142 108]\n",
      " [ 97 153]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.57      0.58       250\n",
      "          1       0.59      0.61      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.405267---------------\n",
      "[[142 108]\n",
      " [ 98 152]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.57      0.58       250\n",
      "          1       0.58      0.61      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.405770---------------\n",
      "[[143 107]\n",
      " [ 98 152]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.57      0.58       250\n",
      "          1       0.59      0.61      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.406147---------------\n",
      "[[144 106]\n",
      " [ 98 152]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.58      0.59       250\n",
      "          1       0.59      0.61      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.406235---------------\n",
      "[[145 105]\n",
      " [ 98 152]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.58      0.59       250\n",
      "          1       0.59      0.61      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.408104---------------\n",
      "[[145 105]\n",
      " [ 99 151]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.58      0.59       250\n",
      "          1       0.59      0.60      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.408248---------------\n",
      "[[146 104]\n",
      " [ 99 151]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.58      0.59       250\n",
      "          1       0.59      0.60      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.408587---------------\n",
      "[[147 103]\n",
      " [ 99 151]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.59      0.59       250\n",
      "          1       0.59      0.60      0.60       250\n",
      "\n",
      "avg / total       0.60      0.60      0.60       500\n",
      "\n",
      "------------0--0.410470---------------\n",
      "[[147 103]\n",
      " [100 150]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.59      0.59       250\n",
      "          1       0.59      0.60      0.60       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.410783---------------\n",
      "[[147 103]\n",
      " [101 149]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.59      0.59       250\n",
      "          1       0.59      0.60      0.59       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.411512---------------\n",
      "[[147 103]\n",
      " [102 148]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.59      0.59       250\n",
      "          1       0.59      0.59      0.59       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.413404---------------\n",
      "[[147 103]\n",
      " [103 147]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.59      0.59       250\n",
      "          1       0.59      0.59      0.59       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.414424---------------\n",
      "[[147 103]\n",
      " [104 146]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.59      0.59       250\n",
      "          1       0.59      0.58      0.59       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.414997---------------\n",
      "[[147 103]\n",
      " [105 145]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.59      0.59       250\n",
      "          1       0.58      0.58      0.58       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.415368---------------\n",
      "[[147 103]\n",
      " [106 144]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.59      0.58       250\n",
      "          1       0.58      0.58      0.58       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.415809---------------\n",
      "[[148 102]\n",
      " [106 144]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.59      0.59       250\n",
      "          1       0.59      0.58      0.58       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.416240---------------\n",
      "[[148 102]\n",
      " [107 143]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.59      0.59       250\n",
      "          1       0.58      0.57      0.58       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.417241---------------\n",
      "[[148 102]\n",
      " [108 142]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.59      0.58       250\n",
      "          1       0.58      0.57      0.57       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.417659---------------\n",
      "[[149 101]\n",
      " [108 142]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.60      0.59       250\n",
      "          1       0.58      0.57      0.58       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.418286---------------\n",
      "[[149 101]\n",
      " [109 141]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.60      0.59       250\n",
      "          1       0.58      0.56      0.57       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.418566---------------\n",
      "[[149 101]\n",
      " [110 140]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.60      0.59       250\n",
      "          1       0.58      0.56      0.57       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.418639---------------\n",
      "[[149 101]\n",
      " [111 139]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.60      0.58       250\n",
      "          1       0.58      0.56      0.57       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.418703---------------\n",
      "[[150 100]\n",
      " [111 139]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.60      0.59       250\n",
      "          1       0.58      0.56      0.57       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.419459---------------\n",
      "[[150 100]\n",
      " [112 138]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.60      0.59       250\n",
      "          1       0.58      0.55      0.57       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.420594---------------\n",
      "[[151  99]\n",
      " [112 138]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.60      0.59       250\n",
      "          1       0.58      0.55      0.57       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.421249---------------\n",
      "[[151  99]\n",
      " [113 137]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.60      0.59       250\n",
      "          1       0.58      0.55      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.422952---------------\n",
      "[[152  98]\n",
      " [113 137]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.61      0.59       250\n",
      "          1       0.58      0.55      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.424152---------------\n",
      "[[153  97]\n",
      " [113 137]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.61      0.59       250\n",
      "          1       0.59      0.55      0.57       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.424178---------------\n",
      "[[153  97]\n",
      " [114 136]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.61      0.59       250\n",
      "          1       0.58      0.54      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.424632---------------\n",
      "[[154  96]\n",
      " [114 136]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.62      0.59       250\n",
      "          1       0.59      0.54      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.425068---------------\n",
      "[[154  96]\n",
      " [115 135]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.62      0.59       250\n",
      "          1       0.58      0.54      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.427172---------------\n",
      "[[155  95]\n",
      " [115 135]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.62      0.60       250\n",
      "          1       0.59      0.54      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.427517---------------\n",
      "[[155  95]\n",
      " [116 134]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.62      0.60       250\n",
      "          1       0.59      0.54      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.427750---------------\n",
      "[[156  94]\n",
      " [116 134]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.62      0.60       250\n",
      "          1       0.59      0.54      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.427899---------------\n",
      "[[157  93]\n",
      " [116 134]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.63      0.60       250\n",
      "          1       0.59      0.54      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.428992---------------\n",
      "[[157  93]\n",
      " [117 133]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.63      0.60       250\n",
      "          1       0.59      0.53      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.429416---------------\n",
      "[[157  93]\n",
      " [118 132]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.63      0.60       250\n",
      "          1       0.59      0.53      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.431686---------------\n",
      "[[158  92]\n",
      " [118 132]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.63      0.60       250\n",
      "          1       0.59      0.53      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.431825---------------\n",
      "[[158  92]\n",
      " [119 131]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.63      0.60       250\n",
      "          1       0.59      0.52      0.55       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.433518---------------\n",
      "[[159  91]\n",
      " [119 131]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.64      0.60       250\n",
      "          1       0.59      0.52      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.433875---------------\n",
      "[[160  90]\n",
      " [119 131]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.64      0.60       250\n",
      "          1       0.59      0.52      0.56       250\n",
      "\n",
      "avg / total       0.58      0.58      0.58       500\n",
      "\n",
      "------------0--0.434628---------------\n",
      "[[161  89]\n",
      " [119 131]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.64      0.61       250\n",
      "          1       0.60      0.52      0.56       250\n",
      "\n",
      "avg / total       0.59      0.58      0.58       500\n",
      "\n",
      "------------0--0.435369---------------\n",
      "[[162  88]\n",
      " [119 131]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.65      0.61       250\n",
      "          1       0.60      0.52      0.56       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.436514---------------\n",
      "[[163  87]\n",
      " [119 131]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.65      0.61       250\n",
      "          1       0.60      0.52      0.56       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.437626---------------\n",
      "[[164  86]\n",
      " [119 131]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.66      0.62       250\n",
      "          1       0.60      0.52      0.56       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.438670---------------\n",
      "[[165  85]\n",
      " [119 131]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.66      0.62       250\n",
      "          1       0.61      0.52      0.56       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.438706---------------\n",
      "[[165  85]\n",
      " [120 130]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.66      0.62       250\n",
      "          1       0.60      0.52      0.56       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.439324---------------\n",
      "[[166  84]\n",
      " [120 130]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.66      0.62       250\n",
      "          1       0.61      0.52      0.56       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.439430---------------\n",
      "[[166  84]\n",
      " [121 129]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.66      0.62       250\n",
      "          1       0.61      0.52      0.56       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.439548---------------\n",
      "[[167  83]\n",
      " [121 129]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.67      0.62       250\n",
      "          1       0.61      0.52      0.56       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.440101---------------\n",
      "[[167  83]\n",
      " [122 128]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.67      0.62       250\n",
      "          1       0.61      0.51      0.56       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.441321---------------\n",
      "[[168  82]\n",
      " [122 128]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.67      0.62       250\n",
      "          1       0.61      0.51      0.56       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.442886---------------\n",
      "[[168  82]\n",
      " [123 127]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.67      0.62       250\n",
      "          1       0.61      0.51      0.55       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.444015---------------\n",
      "[[168  82]\n",
      " [124 126]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.67      0.62       250\n",
      "          1       0.61      0.50      0.55       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.445208---------------\n",
      "[[169  81]\n",
      " [124 126]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.68      0.62       250\n",
      "          1       0.61      0.50      0.55       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.445494---------------\n",
      "[[169  81]\n",
      " [125 125]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.68      0.62       250\n",
      "          1       0.61      0.50      0.55       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.445997---------------\n",
      "[[170  80]\n",
      " [125 125]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.68      0.62       250\n",
      "          1       0.61      0.50      0.55       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.447251---------------\n",
      "[[171  79]\n",
      " [125 125]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.68      0.63       250\n",
      "          1       0.61      0.50      0.55       250\n",
      "\n",
      "avg / total       0.60      0.59      0.59       500\n",
      "\n",
      "------------0--0.448689---------------\n",
      "[[171  79]\n",
      " [126 124]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.68      0.63       250\n",
      "          1       0.61      0.50      0.55       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.449647---------------\n",
      "[[172  78]\n",
      " [126 124]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.69      0.63       250\n",
      "          1       0.61      0.50      0.55       250\n",
      "\n",
      "avg / total       0.60      0.59      0.59       500\n",
      "\n",
      "------------0--0.452071---------------\n",
      "[[172  78]\n",
      " [127 123]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.69      0.63       250\n",
      "          1       0.61      0.49      0.55       250\n",
      "\n",
      "avg / total       0.59      0.59      0.59       500\n",
      "\n",
      "------------0--0.452388---------------\n",
      "[[172  78]\n",
      " [128 122]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.69      0.63       250\n",
      "          1       0.61      0.49      0.54       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.452602---------------\n",
      "[[172  78]\n",
      " [129 121]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.69      0.62       250\n",
      "          1       0.61      0.48      0.54       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.452965---------------\n",
      "[[172  78]\n",
      " [130 120]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.69      0.62       250\n",
      "          1       0.61      0.48      0.54       250\n",
      "\n",
      "avg / total       0.59      0.58      0.58       500\n",
      "\n",
      "------------0--0.453295---------------\n",
      "[[173  77]\n",
      " [130 120]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.69      0.63       250\n",
      "          1       0.61      0.48      0.54       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.454257---------------\n",
      "[[174  76]\n",
      " [130 120]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.70      0.63       250\n",
      "          1       0.61      0.48      0.54       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.454834---------------\n",
      "[[174  76]\n",
      " [131 119]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.70      0.63       250\n",
      "          1       0.61      0.48      0.53       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.455622---------------\n",
      "[[175  75]\n",
      " [131 119]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.70      0.63       250\n",
      "          1       0.61      0.48      0.54       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.455850---------------\n",
      "[[176  74]\n",
      " [131 119]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.70      0.63       250\n",
      "          1       0.62      0.48      0.54       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.458789---------------\n",
      "[[177  73]\n",
      " [131 119]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.71      0.63       250\n",
      "          1       0.62      0.48      0.54       250\n",
      "\n",
      "avg / total       0.60      0.59      0.59       500\n",
      "\n",
      "------------0--0.459523---------------\n",
      "[[177  73]\n",
      " [132 118]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.71      0.63       250\n",
      "          1       0.62      0.47      0.54       250\n",
      "\n",
      "avg / total       0.60      0.59      0.58       500\n",
      "\n",
      "------------0--0.460604---------------\n",
      "[[177  73]\n",
      " [133 117]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.71      0.63       250\n",
      "          1       0.62      0.47      0.53       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.460654---------------\n",
      "[[177  73]\n",
      " [134 116]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.71      0.63       250\n",
      "          1       0.61      0.46      0.53       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.461149---------------\n",
      "[[178  72]\n",
      " [134 116]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.71      0.63       250\n",
      "          1       0.62      0.46      0.53       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.461456---------------\n",
      "[[179  71]\n",
      " [134 116]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.72      0.64       250\n",
      "          1       0.62      0.46      0.53       250\n",
      "\n",
      "avg / total       0.60      0.59      0.58       500\n",
      "\n",
      "------------0--0.462329---------------\n",
      "[[179  71]\n",
      " [135 115]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.72      0.63       250\n",
      "          1       0.62      0.46      0.53       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.464134---------------\n",
      "[[179  71]\n",
      " [136 114]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.72      0.63       250\n",
      "          1       0.62      0.46      0.52       250\n",
      "\n",
      "avg / total       0.59      0.59      0.58       500\n",
      "\n",
      "------------0--0.464291---------------\n",
      "[[179  71]\n",
      " [137 113]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.72      0.63       250\n",
      "          1       0.61      0.45      0.52       250\n",
      "\n",
      "avg / total       0.59      0.58      0.58       500\n",
      "\n",
      "------------0--0.464445---------------\n",
      "[[179  71]\n",
      " [138 112]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.72      0.63       250\n",
      "          1       0.61      0.45      0.52       250\n",
      "\n",
      "avg / total       0.59      0.58      0.57       500\n",
      "\n",
      "------------0--0.464871---------------\n",
      "[[179  71]\n",
      " [139 111]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.72      0.63       250\n",
      "          1       0.61      0.44      0.51       250\n",
      "\n",
      "avg / total       0.59      0.58      0.57       500\n",
      "\n",
      "------------0--0.465524---------------\n",
      "[[179  71]\n",
      " [140 110]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.72      0.63       250\n",
      "          1       0.61      0.44      0.51       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.466590---------------\n",
      "[[180  70]\n",
      " [140 110]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.72      0.63       250\n",
      "          1       0.61      0.44      0.51       250\n",
      "\n",
      "avg / total       0.59      0.58      0.57       500\n",
      "\n",
      "------------0--0.466980---------------\n",
      "[[180  70]\n",
      " [141 109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.72      0.63       250\n",
      "          1       0.61      0.44      0.51       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.467651---------------\n",
      "[[180  70]\n",
      " [142 108]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.72      0.63       250\n",
      "          1       0.61      0.43      0.50       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.469806---------------\n",
      "[[180  70]\n",
      " [143 107]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.72      0.63       250\n",
      "          1       0.60      0.43      0.50       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.469906---------------\n",
      "[[180  70]\n",
      " [144 106]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.72      0.63       250\n",
      "          1       0.60      0.42      0.50       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.473397---------------\n",
      "[[181  69]\n",
      " [144 106]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.72      0.63       250\n",
      "          1       0.61      0.42      0.50       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.474353---------------\n",
      "[[182  68]\n",
      " [144 106]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.73      0.63       250\n",
      "          1       0.61      0.42      0.50       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.474379---------------\n",
      "[[183  67]\n",
      " [144 106]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.73      0.63       250\n",
      "          1       0.61      0.42      0.50       250\n",
      "\n",
      "avg / total       0.59      0.58      0.57       500\n",
      "\n",
      "------------0--0.474919---------------\n",
      "[[183  67]\n",
      " [145 105]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.73      0.63       250\n",
      "          1       0.61      0.42      0.50       250\n",
      "\n",
      "avg / total       0.58      0.58      0.57       500\n",
      "\n",
      "------------0--0.476874---------------\n",
      "[[183  67]\n",
      " [146 104]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.73      0.63       250\n",
      "          1       0.61      0.42      0.49       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.476979---------------\n",
      "[[183  67]\n",
      " [147 103]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.73      0.63       250\n",
      "          1       0.61      0.41      0.49       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.478005---------------\n",
      "[[184  66]\n",
      " [147 103]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.74      0.63       250\n",
      "          1       0.61      0.41      0.49       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.478619---------------\n",
      "[[185  65]\n",
      " [147 103]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.74      0.64       250\n",
      "          1       0.61      0.41      0.49       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.479142---------------\n",
      "[[185  65]\n",
      " [148 102]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.74      0.63       250\n",
      "          1       0.61      0.41      0.49       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.479198---------------\n",
      "[[186  64]\n",
      " [148 102]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.74      0.64       250\n",
      "          1       0.61      0.41      0.49       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.479491---------------\n",
      "[[187  63]\n",
      " [148 102]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.75      0.64       250\n",
      "          1       0.62      0.41      0.49       250\n",
      "\n",
      "avg / total       0.59      0.58      0.57       500\n",
      "\n",
      "------------0--0.480976---------------\n",
      "[[188  62]\n",
      " [148 102]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.75      0.64       250\n",
      "          1       0.62      0.41      0.49       250\n",
      "\n",
      "avg / total       0.59      0.58      0.57       500\n",
      "\n",
      "------------0--0.481629---------------\n",
      "[[188  62]\n",
      " [149 101]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.75      0.64       250\n",
      "          1       0.62      0.40      0.49       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.482331---------------\n",
      "[[188  62]\n",
      " [150 100]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.75      0.64       250\n",
      "          1       0.62      0.40      0.49       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.483973---------------\n",
      "[[188  62]\n",
      " [151  99]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.75      0.64       250\n",
      "          1       0.61      0.40      0.48       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.485278---------------\n",
      "[[189  61]\n",
      " [151  99]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.76      0.64       250\n",
      "          1       0.62      0.40      0.48       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.486231---------------\n",
      "[[189  61]\n",
      " [152  98]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.76      0.64       250\n",
      "          1       0.62      0.39      0.48       250\n",
      "\n",
      "avg / total       0.59      0.57      0.56       500\n",
      "\n",
      "------------0--0.486818---------------\n",
      "[[190  60]\n",
      " [152  98]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.76      0.64       250\n",
      "          1       0.62      0.39      0.48       250\n",
      "\n",
      "avg / total       0.59      0.58      0.56       500\n",
      "\n",
      "------------0--0.487767---------------\n",
      "[[190  60]\n",
      " [153  97]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.76      0.64       250\n",
      "          1       0.62      0.39      0.48       250\n",
      "\n",
      "avg / total       0.59      0.57      0.56       500\n",
      "\n",
      "------------0--0.488984---------------\n",
      "[[190  60]\n",
      " [154  96]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.76      0.64       250\n",
      "          1       0.62      0.38      0.47       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.489109---------------\n",
      "[[190  60]\n",
      " [155  95]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.76      0.64       250\n",
      "          1       0.61      0.38      0.47       250\n",
      "\n",
      "avg / total       0.58      0.57      0.55       500\n",
      "\n",
      "------------0--0.489890---------------\n",
      "[[191  59]\n",
      " [155  95]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.76      0.64       250\n",
      "          1       0.62      0.38      0.47       250\n",
      "\n",
      "avg / total       0.58      0.57      0.56       500\n",
      "\n",
      "------------0--0.489974---------------\n",
      "[[191  59]\n",
      " [156  94]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.76      0.64       250\n",
      "          1       0.61      0.38      0.47       250\n",
      "\n",
      "avg / total       0.58      0.57      0.55       500\n",
      "\n",
      "------------0--0.490537---------------\n",
      "[[191  59]\n",
      " [157  93]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.76      0.64       250\n",
      "          1       0.61      0.37      0.46       250\n",
      "\n",
      "avg / total       0.58      0.57      0.55       500\n",
      "\n",
      "------------0--0.490959---------------\n",
      "[[191  59]\n",
      " [158  92]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.76      0.64       250\n",
      "          1       0.61      0.37      0.46       250\n",
      "\n",
      "avg / total       0.58      0.57      0.55       500\n",
      "\n",
      "------------0--0.492328---------------\n",
      "[[191  59]\n",
      " [159  91]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.76      0.64       250\n",
      "          1       0.61      0.36      0.46       250\n",
      "\n",
      "avg / total       0.58      0.56      0.55       500\n",
      "\n",
      "------------0--0.493795---------------\n",
      "[[191  59]\n",
      " [160  90]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.76      0.64       250\n",
      "          1       0.60      0.36      0.45       250\n",
      "\n",
      "avg / total       0.57      0.56      0.54       500\n",
      "\n",
      "------------0--0.494871---------------\n",
      "[[191  59]\n",
      " [161  89]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.76      0.63       250\n",
      "          1       0.60      0.36      0.45       250\n",
      "\n",
      "avg / total       0.57      0.56      0.54       500\n",
      "\n",
      "------------0--0.495060---------------\n",
      "[[191  59]\n",
      " [162  88]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.76      0.63       250\n",
      "          1       0.60      0.35      0.44       250\n",
      "\n",
      "avg / total       0.57      0.56      0.54       500\n",
      "\n",
      "------------0--0.496521---------------\n",
      "[[192  58]\n",
      " [162  88]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.77      0.64       250\n",
      "          1       0.60      0.35      0.44       250\n",
      "\n",
      "avg / total       0.57      0.56      0.54       500\n",
      "\n",
      "------------0--0.496631---------------\n",
      "[[193  57]\n",
      " [162  88]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.77      0.64       250\n",
      "          1       0.61      0.35      0.45       250\n",
      "\n",
      "avg / total       0.58      0.56      0.54       500\n",
      "\n",
      "------------0--0.497010---------------\n",
      "[[193  57]\n",
      " [163  87]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.77      0.64       250\n",
      "          1       0.60      0.35      0.44       250\n",
      "\n",
      "avg / total       0.57      0.56      0.54       500\n",
      "\n",
      "------------0--0.498370---------------\n",
      "[[194  56]\n",
      " [163  87]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.78      0.64       250\n",
      "          1       0.61      0.35      0.44       250\n",
      "\n",
      "avg / total       0.58      0.56      0.54       500\n",
      "\n",
      "------------0--0.503083---------------\n",
      "[[194  56]\n",
      " [164  86]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.78      0.64       250\n",
      "          1       0.61      0.34      0.44       250\n",
      "\n",
      "avg / total       0.57      0.56      0.54       500\n",
      "\n",
      "------------0--0.503836---------------\n",
      "[[194  56]\n",
      " [165  85]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.78      0.64       250\n",
      "          1       0.60      0.34      0.43       250\n",
      "\n",
      "avg / total       0.57      0.56      0.54       500\n",
      "\n",
      "------------0--0.505091---------------\n",
      "[[194  56]\n",
      " [166  84]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.78      0.64       250\n",
      "          1       0.60      0.34      0.43       250\n",
      "\n",
      "avg / total       0.57      0.56      0.53       500\n",
      "\n",
      "------------0--0.506270---------------\n",
      "[[194  56]\n",
      " [167  83]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.78      0.64       250\n",
      "          1       0.60      0.33      0.43       250\n",
      "\n",
      "avg / total       0.57      0.55      0.53       500\n",
      "\n",
      "------------0--0.506799---------------\n",
      "[[194  56]\n",
      " [168  82]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.78      0.63       250\n",
      "          1       0.59      0.33      0.42       250\n",
      "\n",
      "avg / total       0.57      0.55      0.53       500\n",
      "\n",
      "------------0--0.509913---------------\n",
      "[[194  56]\n",
      " [169  81]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.78      0.63       250\n",
      "          1       0.59      0.32      0.42       250\n",
      "\n",
      "avg / total       0.56      0.55      0.53       500\n",
      "\n",
      "------------0--0.510925---------------\n",
      "[[194  56]\n",
      " [170  80]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.78      0.63       250\n",
      "          1       0.59      0.32      0.41       250\n",
      "\n",
      "avg / total       0.56      0.55      0.52       500\n",
      "\n",
      "------------0--0.511115---------------\n",
      "[[194  56]\n",
      " [171  79]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.78      0.63       250\n",
      "          1       0.59      0.32      0.41       250\n",
      "\n",
      "avg / total       0.56      0.55      0.52       500\n",
      "\n",
      "------------0--0.511715---------------\n",
      "[[195  55]\n",
      " [171  79]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.78      0.63       250\n",
      "          1       0.59      0.32      0.41       250\n",
      "\n",
      "avg / total       0.56      0.55      0.52       500\n",
      "\n",
      "------------0--0.512462---------------\n",
      "[[195  55]\n",
      " [172  78]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.78      0.63       250\n",
      "          1       0.59      0.31      0.41       250\n",
      "\n",
      "avg / total       0.56      0.55      0.52       500\n",
      "\n",
      "------------0--0.514109---------------\n",
      "[[195  55]\n",
      " [173  77]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.78      0.63       250\n",
      "          1       0.58      0.31      0.40       250\n",
      "\n",
      "avg / total       0.56      0.54      0.52       500\n",
      "\n",
      "------------0--0.515866---------------\n",
      "[[195  55]\n",
      " [174  76]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.78      0.63       250\n",
      "          1       0.58      0.30      0.40       250\n",
      "\n",
      "avg / total       0.55      0.54      0.51       500\n",
      "\n",
      "------------0--0.517748---------------\n",
      "[[196  54]\n",
      " [174  76]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.78      0.63       250\n",
      "          1       0.58      0.30      0.40       250\n",
      "\n",
      "avg / total       0.56      0.54      0.52       500\n",
      "\n",
      "------------0--0.518288---------------\n",
      "[[197  53]\n",
      " [174  76]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.79      0.63       250\n",
      "          1       0.59      0.30      0.40       250\n",
      "\n",
      "avg / total       0.56      0.55      0.52       500\n",
      "\n",
      "------------0--0.520685---------------\n",
      "[[198  52]\n",
      " [174  76]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.79      0.64       250\n",
      "          1       0.59      0.30      0.40       250\n",
      "\n",
      "avg / total       0.56      0.55      0.52       500\n",
      "\n",
      "------------0--0.522492---------------\n",
      "[[198  52]\n",
      " [175  75]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.79      0.64       250\n",
      "          1       0.59      0.30      0.40       250\n",
      "\n",
      "avg / total       0.56      0.55      0.52       500\n",
      "\n",
      "------------0--0.522502---------------\n",
      "[[199  51]\n",
      " [175  75]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.80      0.64       250\n",
      "          1       0.60      0.30      0.40       250\n",
      "\n",
      "avg / total       0.56      0.55      0.52       500\n",
      "\n",
      "------------0--0.523029---------------\n",
      "[[200  50]\n",
      " [175  75]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.80      0.64       250\n",
      "          1       0.60      0.30      0.40       250\n",
      "\n",
      "avg / total       0.57      0.55      0.52       500\n",
      "\n",
      "------------0--0.524370---------------\n",
      "[[200  50]\n",
      " [176  74]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.80      0.64       250\n",
      "          1       0.60      0.30      0.40       250\n",
      "\n",
      "avg / total       0.56      0.55      0.52       500\n",
      "\n",
      "------------0--0.525945---------------\n",
      "[[200  50]\n",
      " [177  73]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.80      0.64       250\n",
      "          1       0.59      0.29      0.39       250\n",
      "\n",
      "avg / total       0.56      0.55      0.51       500\n",
      "\n",
      "------------0--0.527716---------------\n",
      "[[201  49]\n",
      " [177  73]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.80      0.64       250\n",
      "          1       0.60      0.29      0.39       250\n",
      "\n",
      "avg / total       0.57      0.55      0.52       500\n",
      "\n",
      "------------0--0.528114---------------\n",
      "[[201  49]\n",
      " [178  72]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.80      0.64       250\n",
      "          1       0.60      0.29      0.39       250\n",
      "\n",
      "avg / total       0.56      0.55      0.51       500\n",
      "\n",
      "------------0--0.530712---------------\n",
      "[[202  48]\n",
      " [178  72]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.81      0.64       250\n",
      "          1       0.60      0.29      0.39       250\n",
      "\n",
      "avg / total       0.57      0.55      0.52       500\n",
      "\n",
      "------------0--0.531331---------------\n",
      "[[203  47]\n",
      " [178  72]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.81      0.64       250\n",
      "          1       0.61      0.29      0.39       250\n",
      "\n",
      "avg / total       0.57      0.55      0.52       500\n",
      "\n",
      "------------0--0.534163---------------\n",
      "[[204  46]\n",
      " [178  72]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.82      0.65       250\n",
      "          1       0.61      0.29      0.39       250\n",
      "\n",
      "avg / total       0.57      0.55      0.52       500\n",
      "\n",
      "------------0--0.535661---------------\n",
      "[[205  45]\n",
      " [178  72]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.82      0.65       250\n",
      "          1       0.62      0.29      0.39       250\n",
      "\n",
      "avg / total       0.58      0.55      0.52       500\n",
      "\n",
      "------------0--0.536571---------------\n",
      "[[206  44]\n",
      " [178  72]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.82      0.65       250\n",
      "          1       0.62      0.29      0.39       250\n",
      "\n",
      "avg / total       0.58      0.56      0.52       500\n",
      "\n",
      "------------0--0.537829---------------\n",
      "[[207  43]\n",
      " [178  72]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.83      0.65       250\n",
      "          1       0.63      0.29      0.39       250\n",
      "\n",
      "avg / total       0.58      0.56      0.52       500\n",
      "\n",
      "------------0--0.538300---------------\n",
      "[[207  43]\n",
      " [179  71]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.83      0.65       250\n",
      "          1       0.62      0.28      0.39       250\n",
      "\n",
      "avg / total       0.58      0.56      0.52       500\n",
      "\n",
      "------------0--0.539271---------------\n",
      "[[208  42]\n",
      " [179  71]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.83      0.65       250\n",
      "          1       0.63      0.28      0.39       250\n",
      "\n",
      "avg / total       0.58      0.56      0.52       500\n",
      "\n",
      "------------0--0.539415---------------\n",
      "[[209  41]\n",
      " [179  71]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.84      0.66       250\n",
      "          1       0.63      0.28      0.39       250\n",
      "\n",
      "avg / total       0.59      0.56      0.52       500\n",
      "\n",
      "------------0--0.540269---------------\n",
      "[[209  41]\n",
      " [180  70]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.84      0.65       250\n",
      "          1       0.63      0.28      0.39       250\n",
      "\n",
      "avg / total       0.58      0.56      0.52       500\n",
      "\n",
      "------------0--0.540996---------------\n",
      "[[209  41]\n",
      " [181  69]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.84      0.65       250\n",
      "          1       0.63      0.28      0.38       250\n",
      "\n",
      "avg / total       0.58      0.56      0.52       500\n",
      "\n",
      "------------0--0.541806---------------\n",
      "[[210  40]\n",
      " [181  69]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.84      0.66       250\n",
      "          1       0.63      0.28      0.38       250\n",
      "\n",
      "avg / total       0.59      0.56      0.52       500\n",
      "\n",
      "------------0--0.543718---------------\n",
      "[[210  40]\n",
      " [182  68]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.84      0.65       250\n",
      "          1       0.63      0.27      0.38       250\n",
      "\n",
      "avg / total       0.58      0.56      0.52       500\n",
      "\n",
      "------------0--0.544957---------------\n",
      "[[210  40]\n",
      " [183  67]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.84      0.65       250\n",
      "          1       0.63      0.27      0.38       250\n",
      "\n",
      "avg / total       0.58      0.55      0.51       500\n",
      "\n",
      "------------0--0.545926---------------\n",
      "[[211  39]\n",
      " [183  67]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.84      0.66       250\n",
      "          1       0.63      0.27      0.38       250\n",
      "\n",
      "avg / total       0.58      0.56      0.52       500\n",
      "\n",
      "------------0--0.546446---------------\n",
      "[[211  39]\n",
      " [184  66]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.84      0.65       250\n",
      "          1       0.63      0.26      0.37       250\n",
      "\n",
      "avg / total       0.58      0.55      0.51       500\n",
      "\n",
      "------------0--0.547059---------------\n",
      "[[211  39]\n",
      " [185  65]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.84      0.65       250\n",
      "          1       0.62      0.26      0.37       250\n",
      "\n",
      "avg / total       0.58      0.55      0.51       500\n",
      "\n",
      "------------0--0.548014---------------\n",
      "[[211  39]\n",
      " [186  64]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.84      0.65       250\n",
      "          1       0.62      0.26      0.36       250\n",
      "\n",
      "avg / total       0.58      0.55      0.51       500\n",
      "\n",
      "------------0--0.548064---------------\n",
      "[[212  38]\n",
      " [186  64]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.85      0.65       250\n",
      "          1       0.63      0.26      0.36       250\n",
      "\n",
      "avg / total       0.58      0.55      0.51       500\n",
      "\n",
      "------------0--0.548372---------------\n",
      "[[213  37]\n",
      " [186  64]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.85      0.66       250\n",
      "          1       0.63      0.26      0.36       250\n",
      "\n",
      "avg / total       0.58      0.55      0.51       500\n",
      "\n",
      "------------0--0.550732---------------\n",
      "[[213  37]\n",
      " [187  63]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.85      0.66       250\n",
      "          1       0.63      0.25      0.36       250\n",
      "\n",
      "avg / total       0.58      0.55      0.51       500\n",
      "\n",
      "------------0--0.550888---------------\n",
      "[[214  36]\n",
      " [187  63]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.86      0.66       250\n",
      "          1       0.64      0.25      0.36       250\n",
      "\n",
      "avg / total       0.59      0.55      0.51       500\n",
      "\n",
      "------------0--0.551066---------------\n",
      "[[215  35]\n",
      " [187  63]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.86      0.66       250\n",
      "          1       0.64      0.25      0.36       250\n",
      "\n",
      "avg / total       0.59      0.56      0.51       500\n",
      "\n",
      "------------0--0.551896---------------\n",
      "[[215  35]\n",
      " [188  62]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.86      0.66       250\n",
      "          1       0.64      0.25      0.36       250\n",
      "\n",
      "avg / total       0.59      0.55      0.51       500\n",
      "\n",
      "------------0--0.552821---------------\n",
      "[[216  34]\n",
      " [188  62]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.86      0.66       250\n",
      "          1       0.65      0.25      0.36       250\n",
      "\n",
      "avg / total       0.59      0.56      0.51       500\n",
      "\n",
      "------------0--0.556066---------------\n",
      "[[217  33]\n",
      " [188  62]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.87      0.66       250\n",
      "          1       0.65      0.25      0.36       250\n",
      "\n",
      "avg / total       0.59      0.56      0.51       500\n",
      "\n",
      "------------0--0.556207---------------\n",
      "[[218  32]\n",
      " [188  62]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.87      0.66       250\n",
      "          1       0.66      0.25      0.36       250\n",
      "\n",
      "avg / total       0.60      0.56      0.51       500\n",
      "\n",
      "------------0--0.557336---------------\n",
      "[[218  32]\n",
      " [189  61]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.87      0.66       250\n",
      "          1       0.66      0.24      0.36       250\n",
      "\n",
      "avg / total       0.60      0.56      0.51       500\n",
      "\n",
      "------------0--0.559816---------------\n",
      "[[219  31]\n",
      " [189  61]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.88      0.67       250\n",
      "          1       0.66      0.24      0.36       250\n",
      "\n",
      "avg / total       0.60      0.56      0.51       500\n",
      "\n",
      "------------0--0.559959---------------\n",
      "[[220  30]\n",
      " [189  61]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.88      0.67       250\n",
      "          1       0.67      0.24      0.36       250\n",
      "\n",
      "avg / total       0.60      0.56      0.51       500\n",
      "\n",
      "------------0--0.560256---------------\n",
      "[[221  29]\n",
      " [189  61]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.88      0.67       250\n",
      "          1       0.68      0.24      0.36       250\n",
      "\n",
      "avg / total       0.61      0.56      0.51       500\n",
      "\n",
      "------------0--0.560317---------------\n",
      "[[222  28]\n",
      " [189  61]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.89      0.67       250\n",
      "          1       0.69      0.24      0.36       250\n",
      "\n",
      "avg / total       0.61      0.57      0.52       500\n",
      "\n",
      "------------0--0.560798---------------\n",
      "[[223  27]\n",
      " [189  61]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.89      0.67       250\n",
      "          1       0.69      0.24      0.36       250\n",
      "\n",
      "avg / total       0.62      0.57      0.52       500\n",
      "\n",
      "------------0--0.561224---------------\n",
      "[[224  26]\n",
      " [189  61]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.90      0.68       250\n",
      "          1       0.70      0.24      0.36       250\n",
      "\n",
      "avg / total       0.62      0.57      0.52       500\n",
      "\n",
      "------------0--0.562397---------------\n",
      "[[224  26]\n",
      " [190  60]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.90      0.67       250\n",
      "          1       0.70      0.24      0.36       250\n",
      "\n",
      "avg / total       0.62      0.57      0.52       500\n",
      "\n",
      "------------0--0.564304---------------\n",
      "[[225  25]\n",
      " [190  60]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.90      0.68       250\n",
      "          1       0.71      0.24      0.36       250\n",
      "\n",
      "avg / total       0.62      0.57      0.52       500\n",
      "\n",
      "------------0--0.564421---------------\n",
      "[[225  25]\n",
      " [191  59]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.90      0.68       250\n",
      "          1       0.70      0.24      0.35       250\n",
      "\n",
      "avg / total       0.62      0.57      0.51       500\n",
      "\n",
      "------------0--0.564521---------------\n",
      "[[226  24]\n",
      " [191  59]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.90      0.68       250\n",
      "          1       0.71      0.24      0.35       250\n",
      "\n",
      "avg / total       0.63      0.57      0.52       500\n",
      "\n",
      "------------0--0.566169---------------\n",
      "[[226  24]\n",
      " [192  58]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.90      0.68       250\n",
      "          1       0.71      0.23      0.35       250\n",
      "\n",
      "avg / total       0.62      0.57      0.51       500\n",
      "\n",
      "------------0--0.567503---------------\n",
      "[[227  23]\n",
      " [192  58]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.91      0.68       250\n",
      "          1       0.72      0.23      0.35       250\n",
      "\n",
      "avg / total       0.63      0.57      0.51       500\n",
      "\n",
      "------------0--0.568078---------------\n",
      "[[227  23]\n",
      " [193  57]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.91      0.68       250\n",
      "          1       0.71      0.23      0.35       250\n",
      "\n",
      "avg / total       0.63      0.57      0.51       500\n",
      "\n",
      "------------0--0.570123---------------\n",
      "[[227  23]\n",
      " [194  56]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.91      0.68       250\n",
      "          1       0.71      0.22      0.34       250\n",
      "\n",
      "avg / total       0.62      0.57      0.51       500\n",
      "\n",
      "------------0--0.571730---------------\n",
      "[[228  22]\n",
      " [194  56]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.91      0.68       250\n",
      "          1       0.72      0.22      0.34       250\n",
      "\n",
      "avg / total       0.63      0.57      0.51       500\n",
      "\n",
      "------------0--0.573054---------------\n",
      "[[229  21]\n",
      " [194  56]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.92      0.68       250\n",
      "          1       0.73      0.22      0.34       250\n",
      "\n",
      "avg / total       0.63      0.57      0.51       500\n",
      "\n",
      "------------0--0.573766---------------\n",
      "[[229  21]\n",
      " [195  55]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.92      0.68       250\n",
      "          1       0.72      0.22      0.34       250\n",
      "\n",
      "avg / total       0.63      0.57      0.51       500\n",
      "\n",
      "------------0--0.574180---------------\n",
      "[[230  20]\n",
      " [195  55]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.92      0.68       250\n",
      "          1       0.73      0.22      0.34       250\n",
      "\n",
      "avg / total       0.64      0.57      0.51       500\n",
      "\n",
      "------------0--0.575181---------------\n",
      "[[230  20]\n",
      " [196  54]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.92      0.68       250\n",
      "          1       0.73      0.22      0.33       250\n",
      "\n",
      "avg / total       0.63      0.57      0.51       500\n",
      "\n",
      "------------0--0.575598---------------\n",
      "[[230  20]\n",
      " [197  53]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.92      0.68       250\n",
      "          1       0.73      0.21      0.33       250\n",
      "\n",
      "avg / total       0.63      0.57      0.50       500\n",
      "\n",
      "------------0--0.576658---------------\n",
      "[[230  20]\n",
      " [198  52]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.92      0.68       250\n",
      "          1       0.72      0.21      0.32       250\n",
      "\n",
      "avg / total       0.63      0.56      0.50       500\n",
      "\n",
      "------------0--0.577987---------------\n",
      "[[230  20]\n",
      " [199  51]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.92      0.68       250\n",
      "          1       0.72      0.20      0.32       250\n",
      "\n",
      "avg / total       0.63      0.56      0.50       500\n",
      "\n",
      "------------0--0.579095---------------\n",
      "[[231  19]\n",
      " [199  51]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.92      0.68       250\n",
      "          1       0.73      0.20      0.32       250\n",
      "\n",
      "avg / total       0.63      0.56      0.50       500\n",
      "\n",
      "------------0--0.579766---------------\n",
      "[[232  18]\n",
      " [199  51]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.93      0.68       250\n",
      "          1       0.74      0.20      0.32       250\n",
      "\n",
      "avg / total       0.64      0.57      0.50       500\n",
      "\n",
      "------------0--0.581965---------------\n",
      "[[232  18]\n",
      " [200  50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.93      0.68       250\n",
      "          1       0.74      0.20      0.31       250\n",
      "\n",
      "avg / total       0.64      0.56      0.50       500\n",
      "\n",
      "------------0--0.585884---------------\n",
      "[[232  18]\n",
      " [201  49]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.93      0.68       250\n",
      "          1       0.73      0.20      0.31       250\n",
      "\n",
      "avg / total       0.63      0.56      0.49       500\n",
      "\n",
      "------------0--0.586307---------------\n",
      "[[233  17]\n",
      " [201  49]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.93      0.68       250\n",
      "          1       0.74      0.20      0.31       250\n",
      "\n",
      "avg / total       0.64      0.56      0.50       500\n",
      "\n",
      "------------0--0.588573---------------\n",
      "[[233  17]\n",
      " [202  48]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.93      0.68       250\n",
      "          1       0.74      0.19      0.30       250\n",
      "\n",
      "avg / total       0.64      0.56      0.49       500\n",
      "\n",
      "------------0--0.590575---------------\n",
      "[[233  17]\n",
      " [203  47]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.93      0.68       250\n",
      "          1       0.73      0.19      0.30       250\n",
      "\n",
      "avg / total       0.63      0.56      0.49       500\n",
      "\n",
      "------------0--0.591145---------------\n",
      "[[234  16]\n",
      " [203  47]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.94      0.68       250\n",
      "          1       0.75      0.19      0.30       250\n",
      "\n",
      "avg / total       0.64      0.56      0.49       500\n",
      "\n",
      "------------0--0.592839---------------\n",
      "[[234  16]\n",
      " [204  46]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.94      0.68       250\n",
      "          1       0.74      0.18      0.29       250\n",
      "\n",
      "avg / total       0.64      0.56      0.49       500\n",
      "\n",
      "------------0--0.593396---------------\n",
      "[[234  16]\n",
      " [205  45]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.94      0.68       250\n",
      "          1       0.74      0.18      0.29       250\n",
      "\n",
      "avg / total       0.64      0.56      0.48       500\n",
      "\n",
      "------------0--0.593930---------------\n",
      "[[235  15]\n",
      " [205  45]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.94      0.68       250\n",
      "          1       0.75      0.18      0.29       250\n",
      "\n",
      "avg / total       0.64      0.56      0.49       500\n",
      "\n",
      "------------0--0.594488---------------\n",
      "[[236  14]\n",
      " [205  45]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.94      0.68       250\n",
      "          1       0.76      0.18      0.29       250\n",
      "\n",
      "avg / total       0.65      0.56      0.49       500\n",
      "\n",
      "------------0--0.595052---------------\n",
      "[[236  14]\n",
      " [206  44]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.94      0.68       250\n",
      "          1       0.76      0.18      0.29       250\n",
      "\n",
      "avg / total       0.65      0.56      0.48       500\n",
      "\n",
      "------------0--0.595151---------------\n",
      "[[236  14]\n",
      " [207  43]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.94      0.68       250\n",
      "          1       0.75      0.17      0.28       250\n",
      "\n",
      "avg / total       0.64      0.56      0.48       500\n",
      "\n",
      "------------0--0.595746---------------\n",
      "[[236  14]\n",
      " [208  42]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.94      0.68       250\n",
      "          1       0.75      0.17      0.27       250\n",
      "\n",
      "avg / total       0.64      0.56      0.48       500\n",
      "\n",
      "------------0--0.596532---------------\n",
      "[[237  13]\n",
      " [208  42]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.95      0.68       250\n",
      "          1       0.76      0.17      0.28       250\n",
      "\n",
      "avg / total       0.65      0.56      0.48       500\n",
      "\n",
      "------------0--0.597187---------------\n",
      "[[237  13]\n",
      " [209  41]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.95      0.68       250\n",
      "          1       0.76      0.16      0.27       250\n",
      "\n",
      "avg / total       0.65      0.56      0.48       500\n",
      "\n",
      "------------0--0.598059---------------\n",
      "[[237  13]\n",
      " [210  40]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.95      0.68       250\n",
      "          1       0.75      0.16      0.26       250\n",
      "\n",
      "avg / total       0.64      0.55      0.47       500\n",
      "\n",
      "------------0--0.598689---------------\n",
      "[[238  12]\n",
      " [210  40]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.95      0.68       250\n",
      "          1       0.77      0.16      0.26       250\n",
      "\n",
      "avg / total       0.65      0.56      0.47       500\n",
      "\n",
      "------------0--0.598767---------------\n",
      "[[238  12]\n",
      " [211  39]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.95      0.68       250\n",
      "          1       0.76      0.16      0.26       250\n",
      "\n",
      "avg / total       0.65      0.55      0.47       500\n",
      "\n",
      "------------0--0.603576---------------\n",
      "[[238  12]\n",
      " [212  38]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.95      0.68       250\n",
      "          1       0.76      0.15      0.25       250\n",
      "\n",
      "avg / total       0.64      0.55      0.47       500\n",
      "\n",
      "------------0--0.606000---------------\n",
      "[[238  12]\n",
      " [213  37]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.95      0.68       250\n",
      "          1       0.76      0.15      0.25       250\n",
      "\n",
      "avg / total       0.64      0.55      0.46       500\n",
      "\n",
      "------------0--0.609162---------------\n",
      "[[238  12]\n",
      " [214  36]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.95      0.68       250\n",
      "          1       0.75      0.14      0.24       250\n",
      "\n",
      "avg / total       0.64      0.55      0.46       500\n",
      "\n",
      "------------0--0.610688---------------\n",
      "[[239  11]\n",
      " [214  36]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.96      0.68       250\n",
      "          1       0.77      0.14      0.24       250\n",
      "\n",
      "avg / total       0.65      0.55      0.46       500\n",
      "\n",
      "------------0--0.618975---------------\n",
      "[[239  11]\n",
      " [215  35]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.96      0.68       250\n",
      "          1       0.76      0.14      0.24       250\n",
      "\n",
      "avg / total       0.64      0.55      0.46       500\n",
      "\n",
      "------------0--0.621120---------------\n",
      "[[239  11]\n",
      " [216  34]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.96      0.68       250\n",
      "          1       0.76      0.14      0.23       250\n",
      "\n",
      "avg / total       0.64      0.55      0.45       500\n",
      "\n",
      "------------0--0.622241---------------\n",
      "[[239  11]\n",
      " [217  33]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.96      0.68       250\n",
      "          1       0.75      0.13      0.22       250\n",
      "\n",
      "avg / total       0.64      0.54      0.45       500\n",
      "\n",
      "------------0--0.628974---------------\n",
      "[[239  11]\n",
      " [218  32]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.96      0.68       250\n",
      "          1       0.74      0.13      0.22       250\n",
      "\n",
      "avg / total       0.63      0.54      0.45       500\n",
      "\n",
      "------------0--0.630704---------------\n",
      "[[240  10]\n",
      " [218  32]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.96      0.68       250\n",
      "          1       0.76      0.13      0.22       250\n",
      "\n",
      "avg / total       0.64      0.54      0.45       500\n",
      "\n",
      "------------0--0.633812---------------\n",
      "[[241   9]\n",
      " [218  32]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.96      0.68       250\n",
      "          1       0.78      0.13      0.22       250\n",
      "\n",
      "avg / total       0.65      0.55      0.45       500\n",
      "\n",
      "------------0--0.634250---------------\n",
      "[[242   8]\n",
      " [218  32]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.97      0.68       250\n",
      "          1       0.80      0.13      0.22       250\n",
      "\n",
      "avg / total       0.66      0.55      0.45       500\n",
      "\n",
      "------------0--0.634872---------------\n",
      "[[243   7]\n",
      " [218  32]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.97      0.68       250\n",
      "          1       0.82      0.13      0.22       250\n",
      "\n",
      "avg / total       0.67      0.55      0.45       500\n",
      "\n",
      "------------0--0.637300---------------\n",
      "[[243   7]\n",
      " [219  31]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.97      0.68       250\n",
      "          1       0.82      0.12      0.22       250\n",
      "\n",
      "avg / total       0.67      0.55      0.45       500\n",
      "\n",
      "------------0--0.639010---------------\n",
      "[[243   7]\n",
      " [220  30]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.97      0.68       250\n",
      "          1       0.81      0.12      0.21       250\n",
      "\n",
      "avg / total       0.67      0.55      0.45       500\n",
      "\n",
      "------------0--0.639176---------------\n",
      "[[244   6]\n",
      " [220  30]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.98      0.68       250\n",
      "          1       0.83      0.12      0.21       250\n",
      "\n",
      "avg / total       0.68      0.55      0.45       500\n",
      "\n",
      "------------0--0.649030---------------\n",
      "[[244   6]\n",
      " [221  29]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.98      0.68       250\n",
      "          1       0.83      0.12      0.20       250\n",
      "\n",
      "avg / total       0.68      0.55      0.44       500\n",
      "\n",
      "------------0--0.651896---------------\n",
      "[[245   5]\n",
      " [221  29]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.98      0.68       250\n",
      "          1       0.85      0.12      0.20       250\n",
      "\n",
      "avg / total       0.69      0.55      0.44       500\n",
      "\n",
      "------------0--0.653017---------------\n",
      "[[245   5]\n",
      " [222  28]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.98      0.68       250\n",
      "          1       0.85      0.11      0.20       250\n",
      "\n",
      "avg / total       0.69      0.55      0.44       500\n",
      "\n",
      "------------0--0.653374---------------\n",
      "[[245   5]\n",
      " [223  27]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.98      0.68       250\n",
      "          1       0.84      0.11      0.19       250\n",
      "\n",
      "avg / total       0.68      0.54      0.44       500\n",
      "\n",
      "------------0--0.662121---------------\n",
      "[[245   5]\n",
      " [224  26]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.98      0.68       250\n",
      "          1       0.84      0.10      0.19       250\n",
      "\n",
      "avg / total       0.68      0.54      0.43       500\n",
      "\n",
      "------------0--0.662706---------------\n",
      "[[246   4]\n",
      " [224  26]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.98      0.68       250\n",
      "          1       0.87      0.10      0.19       250\n",
      "\n",
      "avg / total       0.70      0.54      0.43       500\n",
      "\n",
      "------------0--0.662742---------------\n",
      "[[247   3]\n",
      " [224  26]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.99      0.69       250\n",
      "          1       0.90      0.10      0.19       250\n",
      "\n",
      "avg / total       0.71      0.55      0.44       500\n",
      "\n",
      "------------0--0.663188---------------\n",
      "[[247   3]\n",
      " [225  25]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.99      0.68       250\n",
      "          1       0.89      0.10      0.18       250\n",
      "\n",
      "avg / total       0.71      0.54      0.43       500\n",
      "\n",
      "------------0--0.665075---------------\n",
      "[[247   3]\n",
      " [226  24]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.99      0.68       250\n",
      "          1       0.89      0.10      0.17       250\n",
      "\n",
      "avg / total       0.71      0.54      0.43       500\n",
      "\n",
      "------------0--0.668014---------------\n",
      "[[247   3]\n",
      " [227  23]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.99      0.68       250\n",
      "          1       0.88      0.09      0.17       250\n",
      "\n",
      "avg / total       0.70      0.54      0.42       500\n",
      "\n",
      "------------0--0.668616---------------\n",
      "[[247   3]\n",
      " [228  22]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.99      0.68       250\n",
      "          1       0.88      0.09      0.16       250\n",
      "\n",
      "avg / total       0.70      0.54      0.42       500\n",
      "\n",
      "------------0--0.668651---------------\n",
      "[[247   3]\n",
      " [229  21]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.99      0.68       250\n",
      "          1       0.88      0.08      0.15       250\n",
      "\n",
      "avg / total       0.70      0.54      0.42       500\n",
      "\n",
      "------------0--0.669299---------------\n",
      "[[247   3]\n",
      " [230  20]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.99      0.68       250\n",
      "          1       0.87      0.08      0.15       250\n",
      "\n",
      "avg / total       0.69      0.53      0.41       500\n",
      "\n",
      "------------0--0.674367---------------\n",
      "[[247   3]\n",
      " [231  19]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.99      0.68       250\n",
      "          1       0.86      0.08      0.14       250\n",
      "\n",
      "avg / total       0.69      0.53      0.41       500\n",
      "\n",
      "------------0--0.678701---------------\n",
      "[[247   3]\n",
      " [232  18]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.99      0.68       250\n",
      "          1       0.86      0.07      0.13       250\n",
      "\n",
      "avg / total       0.69      0.53      0.41       500\n",
      "\n",
      "------------0--0.679079---------------\n",
      "[[247   3]\n",
      " [233  17]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.99      0.68       250\n",
      "          1       0.85      0.07      0.13       250\n",
      "\n",
      "avg / total       0.68      0.53      0.40       500\n",
      "\n",
      "------------0--0.680173---------------\n",
      "[[247   3]\n",
      " [234  16]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.99      0.68       250\n",
      "          1       0.84      0.06      0.12       250\n",
      "\n",
      "avg / total       0.68      0.53      0.40       500\n",
      "\n",
      "------------0--0.680990---------------\n",
      "[[247   3]\n",
      " [235  15]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.99      0.67       250\n",
      "          1       0.83      0.06      0.11       250\n",
      "\n",
      "avg / total       0.67      0.52      0.39       500\n",
      "\n",
      "------------0--0.681338---------------\n",
      "[[247   3]\n",
      " [236  14]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.99      0.67       250\n",
      "          1       0.82      0.06      0.10       250\n",
      "\n",
      "avg / total       0.67      0.52      0.39       500\n",
      "\n",
      "------------0--0.681653---------------\n",
      "[[247   3]\n",
      " [237  13]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.99      0.67       250\n",
      "          1       0.81      0.05      0.10       250\n",
      "\n",
      "avg / total       0.66      0.52      0.39       500\n",
      "\n",
      "------------0--0.683926---------------\n",
      "[[247   3]\n",
      " [238  12]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.99      0.67       250\n",
      "          1       0.80      0.05      0.09       250\n",
      "\n",
      "avg / total       0.65      0.52      0.38       500\n",
      "\n",
      "------------0--0.691433---------------\n",
      "[[247   3]\n",
      " [239  11]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.99      0.67       250\n",
      "          1       0.79      0.04      0.08       250\n",
      "\n",
      "avg / total       0.65      0.52      0.38       500\n",
      "\n",
      "------------0--0.697285---------------\n",
      "[[247   3]\n",
      " [240  10]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.99      0.67       250\n",
      "          1       0.77      0.04      0.08       250\n",
      "\n",
      "avg / total       0.64      0.51      0.37       500\n",
      "\n",
      "------------0--0.707010---------------\n",
      "[[247   3]\n",
      " [241   9]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.99      0.67       250\n",
      "          1       0.75      0.04      0.07       250\n",
      "\n",
      "avg / total       0.63      0.51      0.37       500\n",
      "\n",
      "------------0--0.707128---------------\n",
      "[[247   3]\n",
      " [242   8]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.99      0.67       250\n",
      "          1       0.73      0.03      0.06       250\n",
      "\n",
      "avg / total       0.62      0.51      0.36       500\n",
      "\n",
      "------------0--0.708341---------------\n",
      "[[247   3]\n",
      " [243   7]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.99      0.67       250\n",
      "          1       0.70      0.03      0.05       250\n",
      "\n",
      "avg / total       0.60      0.51      0.36       500\n",
      "\n",
      "------------0--0.711733---------------\n",
      "[[247   3]\n",
      " [244   6]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.99      0.67       250\n",
      "          1       0.67      0.02      0.05       250\n",
      "\n",
      "avg / total       0.58      0.51      0.36       500\n",
      "\n",
      "------------0--0.742087---------------\n",
      "[[247   3]\n",
      " [245   5]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.99      0.67       250\n",
      "          1       0.62      0.02      0.04       250\n",
      "\n",
      "avg / total       0.56      0.50      0.35       500\n",
      "\n",
      "------------0--0.773794---------------\n",
      "[[247   3]\n",
      " [246   4]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.99      0.66       250\n",
      "          1       0.57      0.02      0.03       250\n",
      "\n",
      "avg / total       0.54      0.50      0.35       500\n",
      "\n",
      "------------0--0.787806---------------\n",
      "[[247   3]\n",
      " [247   3]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.99      0.66       250\n",
      "          1       0.50      0.01      0.02       250\n",
      "\n",
      "avg / total       0.50      0.50      0.34       500\n",
      "\n",
      "------------0--0.788173---------------\n",
      "[[248   2]\n",
      " [247   3]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.99      0.67       250\n",
      "          1       0.60      0.01      0.02       250\n",
      "\n",
      "avg / total       0.55      0.50      0.34       500\n",
      "\n",
      "------------0--0.789117---------------\n",
      "[[248   2]\n",
      " [248   2]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.99      0.66       250\n",
      "          1       0.50      0.01      0.02       250\n",
      "\n",
      "avg / total       0.50      0.50      0.34       500\n",
      "\n",
      "------------0--0.832008---------------\n",
      "[[249   1]\n",
      " [248   2]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      1.00      0.67       250\n",
      "          1       0.67      0.01      0.02       250\n",
      "\n",
      "avg / total       0.58      0.50      0.34       500\n",
      "\n",
      "------------0--0.897570---------------\n",
      "[[249   1]\n",
      " [249   1]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      1.00      0.67       250\n",
      "          1       0.50      0.00      0.01       250\n",
      "\n",
      "avg / total       0.50      0.50      0.34       500\n",
      "\n",
      "------------0--0.928908---------------\n",
      "[[250   0]\n",
      " [249   1]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      1.00      0.67       250\n",
      "          1       1.00      0.00      0.01       250\n",
      "\n",
      "avg / total       0.75      0.50      0.34       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for th in (sorted(set(predictValue.squeeze()))):\n",
    "    predict=[0 if s <th else 1 for s in predictValue]\n",
    "    #predict=np.argmax(predictValue,axis=1)\n",
    "    print('------------%d--%f---------------'%(i,th))\n",
    "    print(confusion_matrix(cnnDevy,predict))\n",
    "    print(classification_report(cnnDevy,predict))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[160  90]\n",
      " [ 78 172]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.64      0.66       250\n",
      "          1       0.66      0.69      0.67       250\n",
      "\n",
      "avg / total       0.66      0.66      0.66       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict=np.argmax(predictValue,axis=1)\n",
    "print(confusion_matrix(cnnDevy,predict))\n",
    "print(classification_report(cnnDevy,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.98669726]\n",
      " [ 0.90711826]\n",
      " [ 0.68903399]\n",
      " [ 0.99862909]\n",
      " [ 0.99080515]\n",
      " [ 0.98143768]\n",
      " [ 0.83035284]\n",
      " [ 0.99558926]\n",
      " [ 0.99611318]\n",
      " [ 0.17857675]]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "0.688224\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "predictValue=crf.predict(devCnnIndex)\n",
    "print(predictValue[:10])\n",
    "print(cnnDevy[:10])\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(cnnDevy, predictValue)\n",
    "#print(thresholds)\n",
    "print(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#end of simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 300)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f159f9c99b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#word embedings\n",
    "lenDUs=[len(s) for s in rawDu]\n",
    "print(len(lenDUs))\n",
    "plt.hist(lenDUs,bins=1000)\n",
    "plt.xlim(0,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 300)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f143e6afeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lenBELs=[len(s) for s in rawBel]\n",
    "plt.hist(lenBELs,bins=1000)\n",
    "plt.xlim(0,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "56 [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 83, 95, 98, 105, 126, 171, 203]\n",
      "49 [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 60)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf35apeVpQdDVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f159f3f4748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFjtJREFUeJzt3X+s3fV93/Hna3ahSZrGJtwxZhtdd3ETOVGT0FtwlK5K\nYAMTopg/aARqi5d5tbQ6XbpFSkwrDS0JEtmq0qAmTB64gSrCYTQtViChLqGLJhWDCYRgCOUWSHwt\niG9iQ6ZGJXXy3h/n4/Tg3vu1fc6x7w+eD+nofL/v7+d7zucjDvfl7+9UFZIkzeafzXUHJEnzm0Eh\nSepkUEiSOhkUkqROBoUkqZNBIUnqZFBIkjoZFJKkTgaFJKnT0rnuwKDOPPPMGh8fn+tuSNKC8tBD\nD323qsZOZJ0FGxTj4+Ps2bNnrrshSQtKkm+d6DruepIkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJ\nnQwKSVIng0KS1MmgkCR1WrBXZks6Oca33vWT6Wevu3QOe6L5wi0KSVKnYwZFku1JDiR57Kj6byf5\nZpK9Sf57X/3qJJNJnkxycV99fatNJtnaV1+dZHerfz7JaaManCRpeMezRfFZYH1/Icm7gQ3AW6vq\nzcDvt/pa4ArgzW2dzyRZkmQJ8GngEmAtcGVrC/BJ4PqqegNwCNg07KAkSaNzzKCoqq8CB48q/0fg\nuqp6qbU50OobgB1V9VJVPQNMAue112RVPV1VPwR2ABuSBLgAuKOtfwtw2ZBjkiSN0KDHKH4e+Ndt\nl9H/SfJLrb4C2NfXbqrVZqu/Hnihqg4fVZckzRODnvW0FDgDWAf8EnB7kp8bWa9mkWQzsBngnHPO\nOdlfJ0li8C2KKeAL1fMA8GPgTGA/sKqv3cpWm63+PWBZkqVH1WdUVduqaqKqJsbGTugBTZKkAQ0a\nFH8OvBsgyc8DpwHfBXYCVyQ5PclqYA3wAPAgsKad4XQavQPeO6uqgPuAy9vnbgTuHHQwkqTRO+au\npyS3Ae8CzkwyBVwDbAe2t1NmfwhsbH/09ya5HXgcOAxsqaoftc/5IHAPsATYXlV721d8FNiR5BPA\nw8DNIxyfpA4ncnHdkbZehPfKc8ygqKorZ1n067O0vxa4dob63cDdM9SfpndWlCRpHvLKbElSJ4NC\nktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NC\nktTJoJAkdTIoJEmdjhkUSbYnOdCeZnf0sg8nqSRntvkkuSHJZJJHk5zb13Zjkqfaa2Nf/ReTfKOt\nc0OSjGpwkv7R+Na7XvZEO+l4HfMJd8BngT8Cbu0vJlkFXAR8u698Cb3nZK8BzgduBM5Pcga9R6hO\nAAU8lGRnVR1qbX4T2E3vCXjrgS8NPiRJp8qJPEpVC9cxtyiq6qvAwRkWXQ98hN4f/iM2ALdWz/3A\nsiRnAxcDu6rqYAuHXcD6tuxnq+r+9sztW4HLhhuSJGmUBjpGkWQDsL+qvn7UohXAvr75qVbrqk/N\nUJckzRPHs+vpZZK8GvhderudTqkkm4HNAOecc86p/npJekUaZIviXwGrga8neRZYCXwtyb8A9gOr\n+tqubLWu+soZ6jOqqm1VNVFVE2NjYwN0XZJ0ok44KKrqG1X1z6tqvKrG6e0uOreqngd2Ale1s5/W\nAS9W1XPAPcBFSZYnWU5va+Setuz7Sda1s52uAu4c0dgkSSNwPKfH3gb8NfDGJFNJNnU0vxt4GpgE\n/hfwWwBVdRD4OPBge32s1Whtbmrr/C2e8SRJ88oxj1FU1ZXHWD7eN13AllnabQe2z1DfA7zlWP2Q\nJM0Nr8yWJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUieD\nQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1Op5HoW5PciDJY321/5Hkm0keTfJnSZb1Lbs6yWSS\nJ5Nc3Fdf32qTSbb21Vcn2d3qn09y2igHKEkazvFsUXwWWH9UbRfwlqr6BeBvgKsBkqwFrgDe3Nb5\nTJIlSZYAnwYuAdYCV7a2AJ8Erq+qNwCHgK5nckuSTrFjBkVVfRU4eFTtL6rqcJu9H1jZpjcAO6rq\npap6BpgEzmuvyap6uqp+COwANiQJcAFwR1v/FuCyIcckSRqhURyj+PfAl9r0CmBf37KpVput/nrg\nhb7QOVKfUZLNSfYk2TM9PT2CrkuSjmXpMCsn+T3gMPC50XSnW1VtA7YBTExM1Kn4TkmDG99610+m\nn73u0jnsiYYxcFAk+XfAe4ELq+rIH+39wKq+ZitbjVnq3wOWJVnatir620uS5oGBdj0lWQ98BHhf\nVf2gb9FO4IokpydZDawBHgAeBNa0M5xOo3fAe2cLmPuAy9v6G4E7BxuKJOlkOJ7TY28D/hp4Y5Kp\nJJuAPwJeC+xK8kiS/wlQVXuB24HHgS8DW6rqR21r4YPAPcATwO2tLcBHgf+SZJLeMYubRzpCSdJQ\njrnrqaqunKE86x/zqroWuHaG+t3A3TPUn6Z3VpQkaR7yymxJUieDQpLUyaCQJHUyKCRJnQwKSVIn\ng0KS1MmgkCR1MigkSZ0MCklSJ4NCktRpqNuMS5p/vLW3Rs0tCklSJ4NCktTJoJAkdTIoJEmdDApJ\nUqfjecLd9iQHkjzWVzsjya4kT7X35a2eJDckmUzyaJJz+9bZ2No/lWRjX/0Xk3yjrXNDkox6kJKk\nwR3PFsVngfVH1bYC91bVGuDeNg9wCb3nZK8BNgM3Qi9YgGuA8+k9ze6aI+HS2vxm33pHf5ckaQ4d\nMyiq6qvAwaPKG4Bb2vQtwGV99Vur535gWZKzgYuBXVV1sKoOAbuA9W3Zz1bV/VVVwK19nyVJmgcG\nPUZxVlU916afB85q0yuAfX3tplqtqz41Q31GSTYn2ZNkz/T09IBdlySdiKEPZrctgRpBX47nu7ZV\n1URVTYyNjZ2Kr5SkV7xBg+I7bbcR7f1Aq+8HVvW1W9lqXfWVM9QlSfPEoEGxEzhy5tJG4M6++lXt\n7Kd1wIttF9U9wEVJlreD2BcB97Rl30+yrp3tdFXfZ0mS5oFj3hQwyW3Au4Azk0zRO3vpOuD2JJuA\nbwHvb83vBt4DTAI/AD4AUFUHk3wceLC1+1hVHTlA/lv0zqx6FfCl9pIkzRPHDIqqunKWRRfO0LaA\nLbN8znZg+wz1PcBbjtUPSYuDd7ddeLwyW5LUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0M\nCklSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVKnoYIiyX9OsjfJY0luS/LT\nSVYn2Z1kMsnnk5zW2p7e5ifb8vG+z7m61Z9McvFwQ5IkjdLAQZFkBfCfgImqeguwBLgC+CRwfVW9\nATgEbGqrbAIOtfr1rR1J1rb13gysBz6TZMmg/ZIkjdawu56WAq9KshR4NfAccAFwR1t+C3BZm97Q\n5mnLL0ySVt9RVS9V1TP0nrd93pD9kiSNyMBBUVX7gd8Hvk0vIF4EHgJeqKrDrdkUsKJNrwD2tXUP\nt/av76/PsM7LJNmcZE+SPdPT04N2XZJ0AobZ9bSc3tbAauBfAq+ht+vopKmqbVU1UVUTY2NjJ/Or\nJEnNMLue/g3wTFVNV9U/AF8A3gksa7uiAFYC+9v0fmAVQFv+OuB7/fUZ1pEkzbFhguLbwLokr27H\nGi4EHgfuAy5vbTYCd7bpnW2etvwrVVWtfkU7K2o1sAZ4YIh+SZJGaOmxm8ysqnYnuQP4GnAYeBjY\nBtwF7EjyiVa7ua1yM/AnSSaBg/TOdKKq9ia5nV7IHAa2VNWPBu2XpIVpfOtdADx73aVz3BMdbeCg\nAKiqa4Brjio/zQxnLVXV3wO/OsvnXAtcO0xfJEknh1dmS5I6GRSSpE4GhSSpk0EhSepkUEiSOhkU\nkqROQ50eK2luHbn2ALz+QCePWxSSpE4GhSSpk0EhSepkUEiSOhkUkqROBoUkqZNBIUnqZFBIkjoZ\nFJKkTkMFRZJlSe5I8s0kTyR5R5IzkuxK8lR7X97aJskNSSaTPJrk3L7P2djaP5Vk4+zfKEk61Ybd\novgU8OWqehPwVuAJYCtwb1WtAe5t8wCX0Hse9hpgM3AjQJIz6D0l73x6T8a75ki4SJLm3sBBkeR1\nwK/QnoldVT+sqheADcAtrdktwGVtegNwa/XcDyxLcjZwMbCrqg5W1SFgF7B+0H5JkkZrmC2K1cA0\n8MdJHk5yU5LXAGdV1XOtzfPAWW16BbCvb/2pVputLkmaB4YJiqXAucCNVfV24O/4x91MAFRVATXE\nd7xMks1J9iTZMz09PaqPlSR1GCYopoCpqtrd5u+gFxzfabuUaO8H2vL9wKq+9Ve22mz1f6KqtlXV\nRFVNjI2NDdF1SdLxGjgoqup5YF+SN7bShcDjwE7gyJlLG4E72/RO4Kp29tM64MW2i+oe4KIky9tB\n7ItaTdIr3PjWu37y0twZ9sFFvw18LslpwNPAB+iFz+1JNgHfAt7f2t4NvAeYBH7Q2lJVB5N8HHiw\ntftYVR0csl+SpBEZKiiq6hFgYoZFF87QtoAts3zOdmD7MH2RJJ0cXpktSepkUEiSOhkUkqROBoUk\nqZNBIUnqZFBIkjoZFJKkTgaFJKmTQSFJ6mRQSJI6GRSSpE4GhSSpk0EhSepkUEiSOg37PApJOqX6\nH2L07HWXzmFPXjncopAkdRo6KJIsSfJwki+2+dVJdieZTPL59vQ7kpze5ifb8vG+z7i61Z9McvGw\nfZIkjc4otig+BDzRN/9J4PqqegNwCNjU6puAQ61+fWtHkrXAFcCbgfXAZ5IsGUG/JEkjMFRQJFkJ\nXArc1OYDXADc0ZrcAlzWpje0edryC1v7DcCOqnqpqp6h90zt84bplyRpdIbdovhD4CPAj9v864EX\nqupwm58CVrTpFcA+gLb8xdb+J/UZ1pEkzbGBgyLJe4EDVfXQCPtzrO/cnGRPkj3T09On6msl6RVt\nmC2KdwLvS/IssIPeLqdPAcuSHDntdiWwv03vB1YBtOWvA77XX59hnZepqm1VNVFVE2NjY0N0XZJ0\nvAYOiqq6uqpWVtU4vYPRX6mqXwPuAy5vzTYCd7bpnW2etvwrVVWtfkU7K2o1sAZ4YNB+SZJG62Rc\ncPdRYEeSTwAPAze3+s3AnySZBA7SCxeqam+S24HHgcPAlqr60Unol7SgHbnQzIvMdKqNJCiq6q+A\nv2rTTzPDWUtV9ffAr86y/rXAtaPoiyRptLwyW5LUyaCQJHUyKCRJnQwKSVInbzMuaVHw9uMnj1sU\nkqROBoUkqZNBIUnqZFBIkjoZFJKkTgaFJKmTQSFJ6mRQSJI6GRSSpE4GhSSpk0EhSeo0cFAkWZXk\nviSPJ9mb5EOtfkaSXUmeau/LWz1JbkgymeTRJOf2fdbG1v6pJBtn+05JOhHjW+/6yUuDG2aL4jDw\n4apaC6wDtiRZC2wF7q2qNcC9bR7gEnrPw14DbAZuhF6wANcA59N7Mt41R8JFkjT3Bg6Kqnquqr7W\npv8f8ASwAtgA3NKa3QJc1qY3ALdWz/3AsiRnAxcDu6rqYFUdAnYB6wftlyRptEZyjCLJOPB2YDdw\nVlU91xY9D5zVplcA+/pWm2q12eqSpHlg6KBI8jPAnwK/U1Xf719WVQXUsN/R912bk+xJsmd6enpU\nHytJ6jBUUCT5KXoh8bmq+kIrf6ftUqK9H2j1/cCqvtVXttps9X+iqrZV1URVTYyNjQ3TdUnScRrm\nrKcANwNPVNUf9C3aCRw5c2kjcGdf/ap29tM64MW2i+oe4KIky9tB7ItaTZI0DwzzKNR3Ar8BfCPJ\nI632u8B1wO1JNgHfAt7flt0NvAeYBH4AfACgqg4m+TjwYGv3sao6OES/JEkjNHBQVNX/BTLL4gtn\naF/Allk+azuwfdC+SNKJOHJdhc/WPj5emS1J6mRQSJI6GRSSpE4GhSSp0zBnPUk6CfpvYOfBVs0H\nblFIkjq5RSFJuCXXxS0KSVIng0KS1MmgkCR1MigkSZ08mC1Js/AAd49bFJKkTgaFJKmTQSFJJ2h8\n610v2y212BkUkqRO8+ZgdpL1wKeAJcBNVXXdHHdJOul8gM7isZgPfM+LoEiyBPg08G+BKeDBJDur\n6vG57ZkkjcZCDpJ5ERTAecBkVT0NkGQHsAEwKCQtWrOFx3zb0pwvQbEC2Nc3PwWcP0d9kUZuIf9r\nUvPDTL+hEwmaYQ6+p6oGXnlUklwOrK+q/9DmfwM4v6o+eFS7zcDmNvsW4LFT2tFT60zgu3PdiZNk\nMY8NHN9Ct9jH98aqeu2JrDBftij2A6v65le22stU1TZgG0CSPVU1cWq6d+ot5vEt5rGB41voXgnj\nO9F15svpsQ8Ca5KsTnIacAWwc477JElinmxRVNXhJB8E7qF3euz2qto7x92SJDFPggKgqu4G7j6B\nVbadrL7ME4t5fIt5bOD4FjrHd5R5cTBbkjR/zZdjFJKkeWrBBUWS9UmeTDKZZOtc92dYSbYnOZDk\nsb7aGUl2JXmqvS+fyz4OI8mqJPcleTzJ3iQfavVFMcYkP53kgSRfb+P7b62+Osnu9jv9fDtJY0FK\nsiTJw0m+2OYX09ieTfKNJI8cORtosfw2AZIsS3JHkm8meSLJOwYZ34IKir5bfVwCrAWuTLJ2bns1\ntM8C64+qbQXurao1wL1tfqE6DHy4qtYC64At7b/ZYhnjS8AFVfVW4G3A+iTrgE8C11fVG4BDwKY5\n7OOwPgQ80Te/mMYG8O6qelvfKbGL5bcJvfvnfbmq3gS8ld5/xxMfX1UtmBfwDuCevvmrgavnul8j\nGNc48Fjf/JPA2W36bODJue7jCMd6J717ei26MQKvBr5G764C3wWWtvrLfrcL6UXvmqZ7gQuALwJZ\nLGNr/X8WOPOo2qL4bQKvA56hHYseZnwLaouCmW/1sWKO+nIynVVVz7Xp54Gz5rIzo5JkHHg7sJtF\nNMa2a+YR4ACwC/hb4IWqOtyaLOTf6R8CHwF+3OZfz+IZG0ABf5HkoXbnB1g8v83VwDTwx23X4U1J\nXsMA41toQfGKU73YX/CnpiX5GeBPgd+pqu/3L1voY6yqH1XV2+j96/s84E1z3KWRSPJe4EBVPTTX\nfTmJfrmqzqW3O3tLkl/pX7jAf5tLgXOBG6vq7cDfcdRupuMd30ILiuO61cci8J0kZwO09wNz3J+h\nJPkpeiHxuar6QisvqjECVNULwH30dscsS3LkOqWF+jt9J/C+JM8CO+jtfvoUi2NsAFTV/vZ+APgz\nekG/WH6bU8BUVe1u83fQC44THt9CC4pXyq0+dgIb2/RGevv1F6QkAW4GnqiqP+hbtCjGmGQsybI2\n/Sp6x1+eoBcYl7dmC3J8VXV1Va2sqnF6/699pap+jUUwNoAkr0ny2iPTwEX0bjS6KH6bVfU8sC/J\nG1vpQnqPbjjx8c31AZcBDtC8B/gbevuBf2+u+zOC8dwGPAf8A71/AWyitx/4XuAp4C+BM+a6n0OM\n75fpbdo+CjzSXu9ZLGMEfgF4uI3vMeC/tvrPAQ8Ak8D/Bk6f674OOc53AV9cTGNr4/h6e+098vdk\nsfw221jeBuxpv88/B5YPMj6vzJYkdVpou54kSaeYQSFJ6mRQSJI6GRSSpE4GhSSpk0EhSepkUEiS\nOhkUkqRO/x9vscvZuUDpAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f159f06a438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE09JREFUeJzt3X+sX/V93/HnqyakWdLWJniWZTsybaxFTrUYahFHiaaU\nqGBgmqmURaCpWJFVV6qREinSajpptEmQyB8NK1KC5A4vZsriMJIMK3Hrei5S1T8AX4IDGJdxSxxh\ny2AnhtAtGpnJe398P1a/ce793Ov7w/cHz4d09D3nfT7nfD8f8YUX53PO93tTVUiSNJ5fmusOSJLm\nN4NCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpK7L5roDU3XllVfW2rVr57obkrSg\nPPnkkz+squUXc8yCDYq1a9cyMjIy192QpAUlyQ8u9hinniRJXQaFJKnLoJAkdRkUkqQug0KS1GVQ\nSJK6DApJUpdBIUnqMigkSV0L9pvZkmbH2p3fmVS74/fcPMs90Xwx4RVFkl9O8kSS7yU5muRPW/2q\nJI8nGU3y9SSXt/rb2/Zo27926Fx3tvrzSW4Yqm9utdEkO2d+mJKkqZrM1NMbwHVV9QFgA7A5ySbg\nC8C9VfVe4FVgW2u/DXi11e9t7UiyHrgVeD+wGfhykiVJlgBfAm4E1gO3tbaSpHlgwqCogf/dNt/W\nlgKuAx5u9T3ALW19S9um7f9YkrT63qp6o6q+D4wC17ZltKperKqfAntbW0nSPDCpm9nt//yPAKeB\ng8A/AK9V1bnW5ASwqq2vAl4CaPt/DLx7uH7BMePVJUnzwKRuZlfVm8CGJEuBbwHvm9VejSPJdmA7\nwHve85656IK0oE32RrU07KIej62q14BHgQ8BS5OcD5rVwMm2fhJYA9D2/xrwo+H6BceMVx/r/XdV\n1caq2rh8+UX93Q1J0hRN5qmn5e1KgiTvAH4HOMYgMD7emm0FHmnr+9o2bf/fVFW1+q3tqairgHXA\nE8BhYF17iupyBje8983E4CRJ0zeZqaeVwJ72dNIvAQ9V1beTPAfsTfJ54Cnggdb+AeC/JhkFzjL4\nDz9VdTTJQ8BzwDlgR5vSIskdwAFgCbC7qo7O2AglSdMyYVBU1dPA1WPUX2TwxNKF9f8L/NtxznU3\ncPcY9f3A/kn0V5J0ifkTHpKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6D\nQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigk\nSV0GhSSpy6CQJHUZFJKkrgmDIsmaJI8meS7J0SSfavU/SXIyyZG23DR0zJ1JRpM8n+SGofrmVhtN\nsnOoflWSx1v960kun+mBSpKmZjJXFOeAz1TVemATsCPJ+rbv3qra0Jb9AG3frcD7gc3Al5MsSbIE\n+BJwI7AeuG3oPF9o53ov8CqwbYbGJ0mapgmDoqpOVdV32/o/AseAVZ1DtgB7q+qNqvo+MApc25bR\nqnqxqn4K7AW2JAlwHfBwO34PcMtUByRJmlkXdY8iyVrgauDxVrojydNJdidZ1mqrgJeGDjvRauPV\n3w28VlXnLqhLkuaBSQdFkncB3wA+XVWvA/cDvwFsAE4BfzYrPfz5PmxPMpJk5MyZM7P9dpIkJhkU\nSd7GICS+WlXfBKiqV6rqzar6GfAXDKaWAE4Ca4YOX91q49V/BCxNctkF9V9QVbuqamNVbVy+fPlk\nui5JmqbJPPUU4AHgWFV9cai+cqjZ7wLPtvV9wK1J3p7kKmAd8ARwGFjXnnC6nMEN731VVcCjwMfb\n8VuBR6Y3LEnSTLls4iZ8GPg94JkkR1rtjxk8tbQBKOA48AcAVXU0yUPAcwyemNpRVW8CJLkDOAAs\nAXZX1dF2vj8C9ib5PPAUg2CSNI+t3fmdSbU7fs/Ns9wTzbYJg6Kq/g7IGLv2d465G7h7jPr+sY6r\nqhf5p6krSdI84jezJUldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnL\noJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwK\nSVKXQSFJ6jIoJEldEwZFkjVJHk3yXJKjST7V6lckOZjkhfa6rNWT5L4ko0meTnLN0Lm2tvYvJNk6\nVP+tJM+0Y+5LktkYrCTp4k3miuIc8JmqWg9sAnYkWQ/sBA5V1TrgUNsGuBFY15btwP0wCBbgLuCD\nwLXAXefDpbX5/aHjNk9/aJKkmTBhUFTVqar6blv/R+AYsArYAuxpzfYAt7T1LcCDNfAYsDTJSuAG\n4GBVna2qV4GDwOa271er6rGqKuDBoXNJkubYRd2jSLIWuBp4HFhRVafarpeBFW19FfDS0GEnWq1X\nPzFGfaz3355kJMnImTNnLqbrkqQpmnRQJHkX8A3g01X1+vC+diVQM9y3X1BVu6pqY1VtXL58+Wy/\nnSSJSQZFkrcxCImvVtU3W/mVNm1Eez3d6ieBNUOHr261Xn31GHVJ0jwwmaeeAjwAHKuqLw7t2gec\nf3JpK/DIUP329vTTJuDHbYrqAHB9kmXtJvb1wIG27/Ukm9p73T50LknSHLtsEm0+DPwe8EySI632\nx8A9wENJtgE/AD7R9u0HbgJGgZ8AnwSoqrNJPgccbu0+W1Vn2/ofAl8B3gH8ZVskSfPAhEFRVX8H\njPe9ho+N0b6AHeOcazewe4z6CPCbE/VFknTp+c1sSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6D\nQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigk\nSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSuiYMiiS7k5xO8uxQ7U+SnExypC03De27M8lokueT3DBU\n39xqo0l2DtWvSvJ4q389yeUzOUBJ0vRM5oriK8DmMer3VtWGtuwHSLIeuBV4fzvmy0mWJFkCfAm4\nEVgP3NbaAnyhneu9wKvAtukMSJI0syYMiqr6W+DsJM+3BdhbVW9U1feBUeDatoxW1YtV9VNgL7Al\nSYDrgIfb8XuAWy5yDJKkWTSdexR3JHm6TU0ta7VVwEtDbU602nj1dwOvVdW5C+qSpHliqkFxP/Ab\nwAbgFPBnM9ajjiTbk4wkGTlz5syleEtJesubUlBU1StV9WZV/Qz4CwZTSwAngTVDTVe32nj1HwFL\nk1x2QX28991VVRurauPy5cun0nVJ0kWaUlAkWTm0+bvA+Sei9gG3Jnl7kquAdcATwGFgXXvC6XIG\nN7z3VVUBjwIfb8dvBR6ZSp8kSbPjsokaJPka8FHgyiQngLuAjybZABRwHPgDgKo6muQh4DngHLCj\nqt5s57kDOAAsAXZX1dH2Fn8E7E3yeeAp4IEZG50kadomDIqqum2M8rj/Ma+qu4G7x6jvB/aPUX+R\nf5q6kiTNM34zW5LUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6JvwJ\nD0mabWt3fmdS7Y7fc/Ms90Rj8YpCktRlUEiSugwKSVKX9yikecy5e80HXlFIkroMCklSl0EhSery\nHoW0CEz2XoY0FV5RSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHVNGBRJdic5neTZodoVSQ4meaG9\nLmv1JLkvyWiSp5NcM3TM1tb+hSRbh+q/leSZdsx9STLTg5QkTd1krii+Amy+oLYTOFRV64BDbRvg\nRmBdW7YD98MgWIC7gA8C1wJ3nQ+X1ub3h4678L0kSXNowqCoqr8Fzl5Q3gLsaet7gFuG6g/WwGPA\n0iQrgRuAg1V1tqpeBQ4Cm9u+X62qx6qqgAeHziVJmgemeo9iRVWdausvAyva+irgpaF2J1qtVz8x\nRl2SNE9M+2Z2uxKoGejLhJJsTzKSZOTMmTOX4i0l6S1vqkHxSps2or2ebvWTwJqhdqtbrVdfPUZ9\nTFW1q6o2VtXG5cuXT7HrkqSLMdWg2Aecf3JpK/DIUP329vTTJuDHbYrqAHB9kmXtJvb1wIG27/Uk\nm9rTTrcPnUuSNA9M+OuxSb4GfBS4MskJBk8v3QM8lGQb8APgE635fuAmYBT4CfBJgKo6m+RzwOHW\n7rNVdf4G+R8yeLLqHcBftkWSNE9MGBRVdds4uz42RtsCdoxznt3A7jHqI8BvTtQPSdLc8JvZkqQu\n/3CRpFnlH1Va+LyikCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdfnNbEkL\nxmS/5X38nptnuSdvLV5RSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwK\nSVKXQSFJ6jIoJEldBoUkqWtaQZHkeJJnkhxJMtJqVyQ5mOSF9rqs1ZPkviSjSZ5Ocs3Qeba29i8k\n2Tq9IUmSZtJMXFH8dlVtqKqNbXsncKiq1gGH2jbAjcC6tmwH7odBsAB3AR8ErgXuOh8ukqS5NxtT\nT1uAPW19D3DLUP3BGngMWJpkJXADcLCqzlbVq8BBYPMs9EuSNAXTDYoC/jrJk0m2t9qKqjrV1l8G\nVrT1VcBLQ8eeaLXx6r8gyfYkI0lGzpw5M82uS5ImY7p/4e4jVXUyyT8HDib5++GdVVVJaprvMXy+\nXcAugI0bN87YeSVJ45vWFUVVnWyvp4FvMbjH8EqbUqK9nm7NTwJrhg5f3Wrj1SVJ88CUgyLJO5P8\nyvl14HrgWWAfcP7Jpa3AI219H3B7e/ppE/DjNkV1ALg+ybJ2E/v6VpMkzQPTmXpaAXwryfnz/Leq\n+qskh4GHkmwDfgB8orXfD9wEjAI/AT4JUFVnk3wOONzafbaqzk6jX5KkGZSqhTnVv3HjxhoZGZnr\nbkizau3O78x1Fxat4/fcPNddmBNJnhz6OsOk+M1sSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6D\nQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqSu6f4pVElT5E+Ia6HwikKS1OUVhSR1\nTPbKbzH/ISSvKCRJXQaFJKnLqSdJb0k+TDB5XlFIkroMCklSl1NPkjQDJjOVtVCfjPKKQpLUNW+C\nIsnmJM8nGU2yc677I0kamBdTT0mWAF8Cfgc4ARxOsq+qnpvbnknSzJnpJ60u1VTWvAgK4FpgtKpe\nBEiyF9gCGBSSNI5LdV9kvgTFKuCloe0TwAfnqC+StGjMxFXMfAmKSUmyHdjeNt9I8uxc9meWXQn8\ncK47MUsW89jA8S10i318/+JiD5gvQXESWDO0vbrVfk5V7QJ2ASQZqaqNl6Z7l95iHt9iHhs4voXu\nrTC+iz1mvjz1dBhYl+SqJJcDtwL75rhPkiTmyRVFVZ1LcgdwAFgC7K6qo3PcLUkS8yQoAKpqP7D/\nIg7ZNVt9mScW8/gW89jA8S10ju8CqarZ6IgkaZGYL/coJEnz1IILisX2Ux9Jdic5Pfyob5IrkhxM\n8kJ7XTaXfZyOJGuSPJrkuSRHk3yq1RfFGJP8cpInknyvje9PW/2qJI+3z+nX20MaC1KSJUmeSvLt\ntr2YxnY8yTNJjpx/GmixfDYBkixN8nCSv09yLMmHpjK+BRUUQz/1cSOwHrgtyfq57dW0fQXYfEFt\nJ3CoqtYBh9r2QnUO+ExVrQc2ATvaP7PFMsY3gOuq6gPABmBzkk3AF4B7q+q9wKvAtjns43R9Cjg2\ntL2Yxgbw21W1YeiR2MXy2QT4c+Cvqup9wAcY/HO8+PFV1YJZgA8BB4a27wTunOt+zcC41gLPDm0/\nD6xs6yuB5+e6jzM41kcY/KbXohsj8M+A7zL4VYEfApe1+s99bhfSwuA7TYeA64BvA1ksY2v9Pw5c\neUFtUXw2gV8Dvk+7Fz2d8S2oKwrG/qmPVXPUl9m0oqpOtfWXgRVz2ZmZkmQtcDXwOItojG1q5ghw\nGjgI/APwWlWda00W8uf0PwH/HvhZ2343i2dsAAX8dZIn2y8/wOL5bF4FnAH+S5s6/M9J3skUxrfQ\nguItpwaxv+AfTUvyLuAbwKer6vXhfQt9jFX1ZlVtYPB/39cC75vjLs2IJP8aOF1VT851X2bRR6rq\nGgbT2TuS/KvhnQv8s3kZcA1wf1VdDfwfLphmmuz4FlpQTOqnPhaBV5KsBGivp+e4P9OS5G0MQuKr\nVfXNVl5UYwSoqteARxlMxyxNcv57Sgv1c/ph4N8kOQ7sZTD99OcsjrEBUFUn2+tp4FsMgn6xfDZP\nACeq6vG2/TCD4Ljo8S20oHir/NTHPmBrW9/KYF5/QUoS4AHgWFV9cWjXohhjkuVJlrb1dzC4/3KM\nQWB8vDVbkOOrqjuranVVrWXw79rfVNW/YxGMDSDJO5P8yvl14HrgWRbJZ7OqXgZeSnL+RwA/xuBP\nN1z8+Ob6hssUbtDcBPwvBvPA/2Gu+zMD4/kacAr4fwz+D2Abg3ngQ8ALwP8Erpjrfk5jfB9hcGn7\nNHCkLTctljEC/xJ4qo3vWeA/tvqvA08Ao8B/B94+132d5jg/Cnx7MY2tjeN7bTl6/r8ni+Wz2cay\nARhpn8//ASybyvj8ZrYkqWuhTT1Jki4xg0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHX9\nf35apeVpQdDVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f159f3f4be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lenDUs=[len(s.split()) for s in rawDu]\n",
    "print(len(lenDUs))\n",
    "print(len(set(lenDUs)),sorted(set(lenDUs)))\n",
    "plt.figure()\n",
    "plt.hist(lenDUs,bins=100)\n",
    "plt.xlim(0,60)\n",
    "lenBELs=[len(s.split()) for s in rawBel]\n",
    "print(len(set(lenBELs)),sorted(set(lenBELs)))\n",
    "plt.figure()\n",
    "plt.hist(lenBELs,bins=100)\n",
    "plt.xlim(0,60)\n",
    "plt.figure()\n",
    "plt.hist(lenDUs,bins=100)\n",
    "plt.xlim(0,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alldu=' '.join(rawDu)\n",
    "allbel=' '.join(rawBel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157725 183736\n",
      "690629 708076\n",
      "118236 136742\n",
      "1450 110\n"
     ]
    }
   ],
   "source": [
    "for s in [',','.','?','!']:\n",
    "    print(alldu.count(s),allbel.count(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastText import train_unsupervised\n",
    "modelDUT=train_unsupervised(rawdutFile,model='skipgram')\n",
    "modelDUT.save_model('./data/model.dut')\n",
    "modelBEL=train_unsupervised(rawbelFile,model='skipgram')\n",
    "modelBEL.save_model('./data/model.bel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.49957323 -0.22804885 -0.32661757 -0.135037   -0.4285343   0.05748361\n",
      " -0.30789647  0.35027662 -0.7199924   0.24762928 -0.55427676 -0.28550458\n",
      "  0.41219109  0.36858508  0.17004927  0.18557267 -0.00182276 -0.20368603\n",
      "  0.8597337   0.35776418 -0.06100864  0.51243538  0.0867442  -0.16818188\n",
      " -0.19422162 -0.6114198   0.77921373  0.2121985   0.5454917  -0.19107008\n",
      " -0.26988694  0.24004583 -0.62325007  0.54945314 -0.31511647  0.11806463\n",
      "  0.17451186  0.26779199  0.02646391 -0.23503387 -0.12956455  0.26389471\n",
      " -0.04559888  0.23596525 -0.48916444 -0.83028793  0.06031936 -0.21574034\n",
      "  0.037954    0.32742545 -0.49152359 -0.27880195 -0.27608162 -0.04392534\n",
      "  0.35020268  0.56100959  0.16326417 -0.08740969  0.04876515  0.2692546\n",
      "  0.24521498  0.62413239  0.15785673 -0.19375294  0.28960162  0.09436969\n",
      "  0.64159173  0.21011961  0.29346746 -0.16046356  0.05896864  0.36044773\n",
      "  0.14098091  0.35768515 -0.61971235  0.06191366  0.24490903  0.15729178\n",
      " -0.03790499  0.24350733 -0.15024269  0.02303209  0.30472615  0.28534439\n",
      " -0.11266743 -0.16487183  0.48667526 -0.38432565  0.3746562   0.82389194\n",
      "  0.13495055 -0.41781145  0.43461412 -0.05038543  0.29790401 -0.26823178\n",
      " -0.30726585 -0.46906385  0.31592247 -0.05950416]\n"
     ]
    }
   ],
   "source": [
    "print(modelDUT.get_word_vector('king'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 200)\n"
     ]
    }
   ],
   "source": [
    "def raw2MeanVector2(modela,modelb,texts):\n",
    "    vss=[]\n",
    "    if type(texts)!=list:\n",
    "        texts=[texts]\n",
    "    for t in texts:\n",
    "        v1s,v2s=[],[]\n",
    "        for w in t.split():\n",
    "            v1s.append(modela.get_word_vector(w))\n",
    "            v2s.append(modelb.get_word_vector(w))\n",
    "        v1s=np.mean(np.array(v1s),axis=0)   \n",
    "        v2s=np.mean(np.array(v2s),axis=0)\n",
    "        vss.append( np.hstack((v1s,v2s)))\n",
    "    return np.array(vss)\n",
    "\n",
    "def raw2MeanVector(modela,modelb,texts):\n",
    "    vss=[]\n",
    "    if type(texts)!=list:\n",
    "        texts=[texts]\n",
    "    for t in texts:\n",
    "        v1s,v2s=modela.get_sentence_vector(t),modelb.get_sentence_vector(t)        \n",
    "        vss.append( np.hstack((v1s,v2s)))\n",
    "    return np.array(vss)\n",
    "\n",
    "\n",
    "meanVector=raw2MeanVector(modelDUT,modelBEL,trainxRaw)\n",
    "print (meanVector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07397734,  0.03471294, -0.04198583, -0.12459175,  0.16131328,\n",
       "        0.03778868,  0.00948046, -0.10236049, -0.07283383,  0.17764598,\n",
       "       -0.09355266,  0.11038069,  0.00420704, -0.06964095, -0.00615061,\n",
       "       -0.10761622,  0.04966148,  0.01723301,  0.07783248,  0.05593039,\n",
       "       -0.04921625,  0.1767507 ,  0.04861021,  0.05147902, -0.04940193,\n",
       "       -0.04682404,  0.09164505,  0.01948317,  0.08209245,  0.05857224,\n",
       "        0.01429278, -0.09955533, -0.05513987, -0.09302238, -0.07820313,\n",
       "       -0.00562016,  0.02670372, -0.00882121, -0.06828126,  0.10187826,\n",
       "       -0.06575949, -0.02952009,  0.0105079 ,  0.02630074, -0.05022471,\n",
       "       -0.09049001, -0.02762979, -0.20462756, -0.02991704,  0.05746111,\n",
       "       -0.02401965, -0.11073597, -0.14145273,  0.02010405, -0.00620565,\n",
       "        0.0348949 ,  0.06959935,  0.09804574,  0.0394443 ,  0.10496492,\n",
       "       -0.01532972,  0.06962698,  0.01059645,  0.00873911, -0.03730577,\n",
       "        0.02768884, -0.02374511, -0.06597299, -0.01245592, -0.05339469,\n",
       "       -0.05274904,  0.05468456,  0.0716517 ,  0.02884822,  0.02676515,\n",
       "        0.03160527,  0.11429434,  0.06498222, -0.08606604,  0.07088203,\n",
       "        0.00380158, -0.00235419,  0.10425511, -0.00478619, -0.00113169,\n",
       "        0.00142664,  0.06929459, -0.03860496, -0.02487712, -0.01238533,\n",
       "        0.13724701, -0.07312158, -0.01490224, -0.03905623, -0.0340205 ,\n",
       "       -0.06036157,  0.00594125, -0.03187634, -0.00210756, -0.03773125], dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelDUT.get_sentence_vector(trainxRaw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "devMeanVector=raw2MeanVector(modelDUT,modelBEL,devxRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 200) (500, 200)\n"
     ]
    }
   ],
   "source": [
    "print(meanVector.shape,devMeanVector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[153  97]\n",
      " [103 147]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        BEL       0.60      0.61      0.60       250\n",
      "        DUT       0.60      0.59      0.60       250\n",
      "\n",
      "avg / total       0.60      0.60      0.60       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf=LinearDiscriminantAnalysis()\n",
    "\n",
    "clf.fit(meanVector,trainy)\n",
    "predictValue=clf.predict(devMeanVector)\n",
    "print(confusion_matrix(evaly,predictValue))\n",
    "print(classification_report(evaly,predictValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.691244\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.689616\n",
      "[3]\tvalid_0's binary_logloss: 0.687869\n",
      "[4]\tvalid_0's binary_logloss: 0.687216\n",
      "[5]\tvalid_0's binary_logloss: 0.686048\n",
      "[6]\tvalid_0's binary_logloss: 0.685488\n",
      "[7]\tvalid_0's binary_logloss: 0.684943\n",
      "[8]\tvalid_0's binary_logloss: 0.684723\n",
      "[9]\tvalid_0's binary_logloss: 0.683401\n",
      "[10]\tvalid_0's binary_logloss: 0.684135\n",
      "[11]\tvalid_0's binary_logloss: 0.683675\n",
      "[12]\tvalid_0's binary_logloss: 0.683015\n",
      "[13]\tvalid_0's binary_logloss: 0.682669\n",
      "[14]\tvalid_0's binary_logloss: 0.68068\n",
      "[15]\tvalid_0's binary_logloss: 0.679922\n",
      "[16]\tvalid_0's binary_logloss: 0.679483\n",
      "[17]\tvalid_0's binary_logloss: 0.678263\n",
      "[18]\tvalid_0's binary_logloss: 0.677897\n",
      "[19]\tvalid_0's binary_logloss: 0.677443\n",
      "[20]\tvalid_0's binary_logloss: 0.676216\n",
      "[21]\tvalid_0's binary_logloss: 0.675587\n",
      "[22]\tvalid_0's binary_logloss: 0.674694\n",
      "[23]\tvalid_0's binary_logloss: 0.673446\n",
      "[24]\tvalid_0's binary_logloss: 0.673874\n",
      "[25]\tvalid_0's binary_logloss: 0.673582\n",
      "[26]\tvalid_0's binary_logloss: 0.673689\n",
      "[27]\tvalid_0's binary_logloss: 0.672702\n",
      "[28]\tvalid_0's binary_logloss: 0.671695\n",
      "[29]\tvalid_0's binary_logloss: 0.671629\n",
      "[30]\tvalid_0's binary_logloss: 0.671092\n",
      "[31]\tvalid_0's binary_logloss: 0.670833\n",
      "[32]\tvalid_0's binary_logloss: 0.670489\n",
      "[33]\tvalid_0's binary_logloss: 0.669749\n",
      "[34]\tvalid_0's binary_logloss: 0.668946\n",
      "[35]\tvalid_0's binary_logloss: 0.668568\n",
      "[36]\tvalid_0's binary_logloss: 0.667806\n",
      "[37]\tvalid_0's binary_logloss: 0.668183\n",
      "[38]\tvalid_0's binary_logloss: 0.668217\n",
      "[39]\tvalid_0's binary_logloss: 0.667829\n",
      "[40]\tvalid_0's binary_logloss: 0.667982\n",
      "[41]\tvalid_0's binary_logloss: 0.666827\n",
      "[42]\tvalid_0's binary_logloss: 0.666888\n",
      "[43]\tvalid_0's binary_logloss: 0.665485\n",
      "[44]\tvalid_0's binary_logloss: 0.66551\n",
      "[45]\tvalid_0's binary_logloss: 0.664717\n",
      "[46]\tvalid_0's binary_logloss: 0.663839\n",
      "[47]\tvalid_0's binary_logloss: 0.663904\n",
      "[48]\tvalid_0's binary_logloss: 0.663941\n",
      "[49]\tvalid_0's binary_logloss: 0.663348\n",
      "[50]\tvalid_0's binary_logloss: 0.662647\n",
      "[51]\tvalid_0's binary_logloss: 0.6619\n",
      "[52]\tvalid_0's binary_logloss: 0.661353\n",
      "[53]\tvalid_0's binary_logloss: 0.660714\n",
      "[54]\tvalid_0's binary_logloss: 0.659373\n",
      "[55]\tvalid_0's binary_logloss: 0.659397\n",
      "[56]\tvalid_0's binary_logloss: 0.659797\n",
      "[57]\tvalid_0's binary_logloss: 0.659339\n",
      "[58]\tvalid_0's binary_logloss: 0.658837\n",
      "[59]\tvalid_0's binary_logloss: 0.658387\n",
      "[60]\tvalid_0's binary_logloss: 0.658355\n",
      "[61]\tvalid_0's binary_logloss: 0.657775\n",
      "[62]\tvalid_0's binary_logloss: 0.657027\n",
      "[63]\tvalid_0's binary_logloss: 0.656768\n",
      "[64]\tvalid_0's binary_logloss: 0.65671\n",
      "[65]\tvalid_0's binary_logloss: 0.656678\n",
      "[66]\tvalid_0's binary_logloss: 0.656685\n",
      "[67]\tvalid_0's binary_logloss: 0.65581\n",
      "[68]\tvalid_0's binary_logloss: 0.65552\n",
      "[69]\tvalid_0's binary_logloss: 0.65472\n",
      "[70]\tvalid_0's binary_logloss: 0.654167\n",
      "[71]\tvalid_0's binary_logloss: 0.653523\n",
      "[72]\tvalid_0's binary_logloss: 0.653015\n",
      "[73]\tvalid_0's binary_logloss: 0.652903\n",
      "[74]\tvalid_0's binary_logloss: 0.652462\n",
      "[75]\tvalid_0's binary_logloss: 0.652559\n",
      "[76]\tvalid_0's binary_logloss: 0.652361\n",
      "[77]\tvalid_0's binary_logloss: 0.651478\n",
      "[78]\tvalid_0's binary_logloss: 0.651593\n",
      "[79]\tvalid_0's binary_logloss: 0.651144\n",
      "[80]\tvalid_0's binary_logloss: 0.650186\n",
      "[81]\tvalid_0's binary_logloss: 0.650088\n",
      "[82]\tvalid_0's binary_logloss: 0.65078\n",
      "[83]\tvalid_0's binary_logloss: 0.651023\n",
      "[84]\tvalid_0's binary_logloss: 0.65067\n",
      "[85]\tvalid_0's binary_logloss: 0.649691\n",
      "[86]\tvalid_0's binary_logloss: 0.649869\n",
      "[87]\tvalid_0's binary_logloss: 0.649247\n",
      "[88]\tvalid_0's binary_logloss: 0.649039\n",
      "[89]\tvalid_0's binary_logloss: 0.648298\n",
      "[90]\tvalid_0's binary_logloss: 0.648593\n",
      "[91]\tvalid_0's binary_logloss: 0.64793\n",
      "[92]\tvalid_0's binary_logloss: 0.64742\n",
      "[93]\tvalid_0's binary_logloss: 0.647268\n",
      "[94]\tvalid_0's binary_logloss: 0.647185\n",
      "[95]\tvalid_0's binary_logloss: 0.64689\n",
      "[96]\tvalid_0's binary_logloss: 0.647045\n",
      "[97]\tvalid_0's binary_logloss: 0.646839\n",
      "[98]\tvalid_0's binary_logloss: 0.647159\n",
      "[99]\tvalid_0's binary_logloss: 0.647519\n",
      "[100]\tvalid_0's binary_logloss: 0.647138\n",
      "[101]\tvalid_0's binary_logloss: 0.647196\n",
      "[102]\tvalid_0's binary_logloss: 0.64778\n",
      "[103]\tvalid_0's binary_logloss: 0.647188\n",
      "[104]\tvalid_0's binary_logloss: 0.647328\n",
      "[105]\tvalid_0's binary_logloss: 0.64706\n",
      "[106]\tvalid_0's binary_logloss: 0.646513\n",
      "[107]\tvalid_0's binary_logloss: 0.6465\n",
      "[108]\tvalid_0's binary_logloss: 0.647004\n",
      "[109]\tvalid_0's binary_logloss: 0.646374\n",
      "[110]\tvalid_0's binary_logloss: 0.646223\n",
      "[111]\tvalid_0's binary_logloss: 0.646255\n",
      "[112]\tvalid_0's binary_logloss: 0.646828\n",
      "[113]\tvalid_0's binary_logloss: 0.646251\n",
      "[114]\tvalid_0's binary_logloss: 0.64612\n",
      "[115]\tvalid_0's binary_logloss: 0.645591\n",
      "[116]\tvalid_0's binary_logloss: 0.646088\n",
      "[117]\tvalid_0's binary_logloss: 0.645371\n",
      "[118]\tvalid_0's binary_logloss: 0.645392\n",
      "[119]\tvalid_0's binary_logloss: 0.645111\n",
      "[120]\tvalid_0's binary_logloss: 0.64458\n",
      "[121]\tvalid_0's binary_logloss: 0.643751\n",
      "[122]\tvalid_0's binary_logloss: 0.644231\n",
      "[123]\tvalid_0's binary_logloss: 0.642984\n",
      "[124]\tvalid_0's binary_logloss: 0.643142\n",
      "[125]\tvalid_0's binary_logloss: 0.64366\n",
      "[126]\tvalid_0's binary_logloss: 0.642976\n",
      "[127]\tvalid_0's binary_logloss: 0.642986\n",
      "[128]\tvalid_0's binary_logloss: 0.643119\n",
      "[129]\tvalid_0's binary_logloss: 0.643292\n",
      "[130]\tvalid_0's binary_logloss: 0.642762\n",
      "[131]\tvalid_0's binary_logloss: 0.642665\n",
      "[132]\tvalid_0's binary_logloss: 0.642295\n",
      "[133]\tvalid_0's binary_logloss: 0.641583\n",
      "[134]\tvalid_0's binary_logloss: 0.642005\n",
      "[135]\tvalid_0's binary_logloss: 0.64159\n",
      "[136]\tvalid_0's binary_logloss: 0.641318\n",
      "[137]\tvalid_0's binary_logloss: 0.640796\n",
      "[138]\tvalid_0's binary_logloss: 0.639633\n",
      "[139]\tvalid_0's binary_logloss: 0.639242\n",
      "[140]\tvalid_0's binary_logloss: 0.638463\n",
      "[141]\tvalid_0's binary_logloss: 0.638151\n",
      "[142]\tvalid_0's binary_logloss: 0.638265\n",
      "[143]\tvalid_0's binary_logloss: 0.638518\n",
      "[144]\tvalid_0's binary_logloss: 0.638993\n",
      "[145]\tvalid_0's binary_logloss: 0.638703\n",
      "[146]\tvalid_0's binary_logloss: 0.638843\n",
      "[147]\tvalid_0's binary_logloss: 0.638893\n",
      "[148]\tvalid_0's binary_logloss: 0.63764\n",
      "[149]\tvalid_0's binary_logloss: 0.637872\n",
      "[150]\tvalid_0's binary_logloss: 0.637626\n",
      "[151]\tvalid_0's binary_logloss: 0.637525\n",
      "[152]\tvalid_0's binary_logloss: 0.636735\n",
      "[153]\tvalid_0's binary_logloss: 0.635974\n",
      "[154]\tvalid_0's binary_logloss: 0.63586\n",
      "[155]\tvalid_0's binary_logloss: 0.635926\n",
      "[156]\tvalid_0's binary_logloss: 0.635353\n",
      "[157]\tvalid_0's binary_logloss: 0.63529\n",
      "[158]\tvalid_0's binary_logloss: 0.634693\n",
      "[159]\tvalid_0's binary_logloss: 0.634138\n",
      "[160]\tvalid_0's binary_logloss: 0.634048\n",
      "[161]\tvalid_0's binary_logloss: 0.633886\n",
      "[162]\tvalid_0's binary_logloss: 0.633367\n",
      "[163]\tvalid_0's binary_logloss: 0.633445\n",
      "[164]\tvalid_0's binary_logloss: 0.633619\n",
      "[165]\tvalid_0's binary_logloss: 0.633318\n",
      "[166]\tvalid_0's binary_logloss: 0.633184\n",
      "[167]\tvalid_0's binary_logloss: 0.633558\n",
      "[168]\tvalid_0's binary_logloss: 0.632798\n",
      "[169]\tvalid_0's binary_logloss: 0.63228\n",
      "[170]\tvalid_0's binary_logloss: 0.632033\n",
      "[171]\tvalid_0's binary_logloss: 0.631747\n",
      "[172]\tvalid_0's binary_logloss: 0.631216\n",
      "[173]\tvalid_0's binary_logloss: 0.631731\n",
      "[174]\tvalid_0's binary_logloss: 0.632216\n",
      "[175]\tvalid_0's binary_logloss: 0.631725\n",
      "[176]\tvalid_0's binary_logloss: 0.63162\n",
      "[177]\tvalid_0's binary_logloss: 0.631671\n",
      "[178]\tvalid_0's binary_logloss: 0.631036\n",
      "[179]\tvalid_0's binary_logloss: 0.631015\n",
      "[180]\tvalid_0's binary_logloss: 0.63072\n",
      "[181]\tvalid_0's binary_logloss: 0.630825\n",
      "[182]\tvalid_0's binary_logloss: 0.630581\n",
      "[183]\tvalid_0's binary_logloss: 0.630255\n",
      "[184]\tvalid_0's binary_logloss: 0.630169\n",
      "[185]\tvalid_0's binary_logloss: 0.629727\n",
      "[186]\tvalid_0's binary_logloss: 0.629521\n",
      "[187]\tvalid_0's binary_logloss: 0.628634\n",
      "[188]\tvalid_0's binary_logloss: 0.628831\n",
      "[189]\tvalid_0's binary_logloss: 0.628737\n",
      "[190]\tvalid_0's binary_logloss: 0.62884\n",
      "[191]\tvalid_0's binary_logloss: 0.628757\n",
      "[192]\tvalid_0's binary_logloss: 0.628912\n",
      "[193]\tvalid_0's binary_logloss: 0.628743\n",
      "[194]\tvalid_0's binary_logloss: 0.628957\n",
      "[195]\tvalid_0's binary_logloss: 0.628975\n",
      "[196]\tvalid_0's binary_logloss: 0.629073\n",
      "[197]\tvalid_0's binary_logloss: 0.629165\n",
      "[198]\tvalid_0's binary_logloss: 0.628948\n",
      "[199]\tvalid_0's binary_logloss: 0.629124\n",
      "[200]\tvalid_0's binary_logloss: 0.629206\n",
      "[201]\tvalid_0's binary_logloss: 0.629262\n",
      "[202]\tvalid_0's binary_logloss: 0.628534\n",
      "[203]\tvalid_0's binary_logloss: 0.628407\n",
      "[204]\tvalid_0's binary_logloss: 0.628228\n",
      "[205]\tvalid_0's binary_logloss: 0.628001\n",
      "[206]\tvalid_0's binary_logloss: 0.627638\n",
      "[207]\tvalid_0's binary_logloss: 0.627728\n",
      "[208]\tvalid_0's binary_logloss: 0.626906\n",
      "[209]\tvalid_0's binary_logloss: 0.627375\n",
      "[210]\tvalid_0's binary_logloss: 0.62742\n",
      "[211]\tvalid_0's binary_logloss: 0.62667\n",
      "[212]\tvalid_0's binary_logloss: 0.626033\n",
      "[213]\tvalid_0's binary_logloss: 0.626051\n",
      "[214]\tvalid_0's binary_logloss: 0.625722\n",
      "[215]\tvalid_0's binary_logloss: 0.626203\n",
      "[216]\tvalid_0's binary_logloss: 0.62641\n",
      "[217]\tvalid_0's binary_logloss: 0.62713\n",
      "[218]\tvalid_0's binary_logloss: 0.62694\n",
      "[219]\tvalid_0's binary_logloss: 0.626791\n",
      "[220]\tvalid_0's binary_logloss: 0.627053\n",
      "[221]\tvalid_0's binary_logloss: 0.627645\n",
      "[222]\tvalid_0's binary_logloss: 0.627108\n",
      "[223]\tvalid_0's binary_logloss: 0.62666\n",
      "[224]\tvalid_0's binary_logloss: 0.625897\n",
      "[225]\tvalid_0's binary_logloss: 0.625941\n",
      "[226]\tvalid_0's binary_logloss: 0.625828\n",
      "[227]\tvalid_0's binary_logloss: 0.625903\n",
      "[228]\tvalid_0's binary_logloss: 0.625698\n",
      "[229]\tvalid_0's binary_logloss: 0.625783\n",
      "[230]\tvalid_0's binary_logloss: 0.625778\n",
      "[231]\tvalid_0's binary_logloss: 0.625358\n",
      "[232]\tvalid_0's binary_logloss: 0.624935\n",
      "[233]\tvalid_0's binary_logloss: 0.624488\n",
      "[234]\tvalid_0's binary_logloss: 0.624752\n",
      "[235]\tvalid_0's binary_logloss: 0.625279\n",
      "[236]\tvalid_0's binary_logloss: 0.624893\n",
      "[237]\tvalid_0's binary_logloss: 0.624834\n",
      "[238]\tvalid_0's binary_logloss: 0.624931\n",
      "[239]\tvalid_0's binary_logloss: 0.624962\n",
      "[240]\tvalid_0's binary_logloss: 0.625201\n",
      "[241]\tvalid_0's binary_logloss: 0.62541\n",
      "[242]\tvalid_0's binary_logloss: 0.625025\n",
      "[243]\tvalid_0's binary_logloss: 0.625438\n",
      "[244]\tvalid_0's binary_logloss: 0.625167\n",
      "[245]\tvalid_0's binary_logloss: 0.625015\n",
      "[246]\tvalid_0's binary_logloss: 0.624778\n",
      "[247]\tvalid_0's binary_logloss: 0.624164\n",
      "[248]\tvalid_0's binary_logloss: 0.623648\n",
      "[249]\tvalid_0's binary_logloss: 0.624098\n",
      "[250]\tvalid_0's binary_logloss: 0.623818\n",
      "[251]\tvalid_0's binary_logloss: 0.623147\n",
      "[252]\tvalid_0's binary_logloss: 0.622694\n",
      "[253]\tvalid_0's binary_logloss: 0.622452\n",
      "[254]\tvalid_0's binary_logloss: 0.62256\n",
      "[255]\tvalid_0's binary_logloss: 0.621927\n",
      "[256]\tvalid_0's binary_logloss: 0.622013\n",
      "[257]\tvalid_0's binary_logloss: 0.62275\n",
      "[258]\tvalid_0's binary_logloss: 0.622202\n",
      "[259]\tvalid_0's binary_logloss: 0.622129\n",
      "[260]\tvalid_0's binary_logloss: 0.622358\n",
      "[261]\tvalid_0's binary_logloss: 0.622261\n",
      "[262]\tvalid_0's binary_logloss: 0.622011\n",
      "[263]\tvalid_0's binary_logloss: 0.621691\n",
      "[264]\tvalid_0's binary_logloss: 0.62142\n",
      "[265]\tvalid_0's binary_logloss: 0.621658\n",
      "[266]\tvalid_0's binary_logloss: 0.621864\n",
      "[267]\tvalid_0's binary_logloss: 0.621817\n",
      "[268]\tvalid_0's binary_logloss: 0.622019\n",
      "[269]\tvalid_0's binary_logloss: 0.621888\n",
      "[270]\tvalid_0's binary_logloss: 0.62144\n",
      "[271]\tvalid_0's binary_logloss: 0.621723\n",
      "[272]\tvalid_0's binary_logloss: 0.621752\n",
      "[273]\tvalid_0's binary_logloss: 0.622137\n",
      "[274]\tvalid_0's binary_logloss: 0.622041\n",
      "[275]\tvalid_0's binary_logloss: 0.62148\n",
      "[276]\tvalid_0's binary_logloss: 0.621431\n",
      "[277]\tvalid_0's binary_logloss: 0.621784\n",
      "[278]\tvalid_0's binary_logloss: 0.621632\n",
      "[279]\tvalid_0's binary_logloss: 0.621748\n",
      "[280]\tvalid_0's binary_logloss: 0.621958\n",
      "[281]\tvalid_0's binary_logloss: 0.621612\n",
      "[282]\tvalid_0's binary_logloss: 0.621534\n",
      "[283]\tvalid_0's binary_logloss: 0.621375\n",
      "[284]\tvalid_0's binary_logloss: 0.621565\n",
      "[285]\tvalid_0's binary_logloss: 0.62151\n",
      "[286]\tvalid_0's binary_logloss: 0.621626\n",
      "[287]\tvalid_0's binary_logloss: 0.621066\n",
      "[288]\tvalid_0's binary_logloss: 0.620672\n",
      "[289]\tvalid_0's binary_logloss: 0.620876\n",
      "[290]\tvalid_0's binary_logloss: 0.620817\n",
      "[291]\tvalid_0's binary_logloss: 0.620926\n",
      "[292]\tvalid_0's binary_logloss: 0.621663\n",
      "[293]\tvalid_0's binary_logloss: 0.621516\n",
      "[294]\tvalid_0's binary_logloss: 0.621751\n",
      "[295]\tvalid_0's binary_logloss: 0.621482\n",
      "[296]\tvalid_0's binary_logloss: 0.62164\n",
      "[297]\tvalid_0's binary_logloss: 0.621228\n",
      "[298]\tvalid_0's binary_logloss: 0.621122\n",
      "[299]\tvalid_0's binary_logloss: 0.621013\n",
      "[300]\tvalid_0's binary_logloss: 0.620143\n",
      "[301]\tvalid_0's binary_logloss: 0.620132\n",
      "[302]\tvalid_0's binary_logloss: 0.620233\n",
      "[303]\tvalid_0's binary_logloss: 0.620345\n",
      "[304]\tvalid_0's binary_logloss: 0.619983\n",
      "[305]\tvalid_0's binary_logloss: 0.619513\n",
      "[306]\tvalid_0's binary_logloss: 0.619986\n",
      "[307]\tvalid_0's binary_logloss: 0.619592\n",
      "[308]\tvalid_0's binary_logloss: 0.619807\n",
      "[309]\tvalid_0's binary_logloss: 0.619845\n",
      "[310]\tvalid_0's binary_logloss: 0.61938\n",
      "[311]\tvalid_0's binary_logloss: 0.61961\n",
      "[312]\tvalid_0's binary_logloss: 0.618956\n",
      "[313]\tvalid_0's binary_logloss: 0.618568\n",
      "[314]\tvalid_0's binary_logloss: 0.61889\n",
      "[315]\tvalid_0's binary_logloss: 0.618562\n",
      "[316]\tvalid_0's binary_logloss: 0.618538\n",
      "[317]\tvalid_0's binary_logloss: 0.618737\n",
      "[318]\tvalid_0's binary_logloss: 0.618834\n",
      "[319]\tvalid_0's binary_logloss: 0.618453\n",
      "[320]\tvalid_0's binary_logloss: 0.618213\n",
      "[321]\tvalid_0's binary_logloss: 0.61767\n",
      "[322]\tvalid_0's binary_logloss: 0.617236\n",
      "[323]\tvalid_0's binary_logloss: 0.6168\n",
      "[324]\tvalid_0's binary_logloss: 0.617183\n",
      "[325]\tvalid_0's binary_logloss: 0.616765\n",
      "[326]\tvalid_0's binary_logloss: 0.616699\n",
      "[327]\tvalid_0's binary_logloss: 0.616464\n",
      "[328]\tvalid_0's binary_logloss: 0.616451\n",
      "[329]\tvalid_0's binary_logloss: 0.616871\n",
      "[330]\tvalid_0's binary_logloss: 0.616698\n",
      "[331]\tvalid_0's binary_logloss: 0.617026\n",
      "[332]\tvalid_0's binary_logloss: 0.617368\n",
      "[333]\tvalid_0's binary_logloss: 0.617355\n",
      "[334]\tvalid_0's binary_logloss: 0.617374\n",
      "[335]\tvalid_0's binary_logloss: 0.617118\n",
      "[336]\tvalid_0's binary_logloss: 0.617732\n",
      "[337]\tvalid_0's binary_logloss: 0.61748\n",
      "[338]\tvalid_0's binary_logloss: 0.617616\n",
      "[339]\tvalid_0's binary_logloss: 0.618248\n",
      "[340]\tvalid_0's binary_logloss: 0.617814\n",
      "[341]\tvalid_0's binary_logloss: 0.618826\n",
      "[342]\tvalid_0's binary_logloss: 0.618744\n",
      "[343]\tvalid_0's binary_logloss: 0.618672\n",
      "[344]\tvalid_0's binary_logloss: 0.618711\n",
      "[345]\tvalid_0's binary_logloss: 0.619312\n",
      "[346]\tvalid_0's binary_logloss: 0.618519\n",
      "[347]\tvalid_0's binary_logloss: 0.618525\n",
      "[348]\tvalid_0's binary_logloss: 0.618239\n",
      "Early stopping, best iteration is:\n",
      "[328]\tvalid_0's binary_logloss: 0.616451\n",
      "[[160  90]\n",
      " [ 89 161]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        BEL       0.64      0.64      0.64       250\n",
      "        DUT       0.64      0.64      0.64       250\n",
      "\n",
      "avg / total       0.64      0.64      0.64       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf=lgb.LGBMClassifier(num_leaves=150,n_jobs=12,\n",
    "                                learning_rate=0.1,n_estimators=1000,silent=False)\n",
    "\n",
    "clf.fit(meanVector,trainy,early_stopping_rounds=20,\n",
    "        eval_set=(devMeanVector,evaly),\n",
    "        verbose=True)\n",
    "predictValue=clf.predict(devMeanVector,) #clf.best_iteration_\n",
    "print(confusion_matrix(evaly,predictValue))\n",
    "print(classification_report(evaly,predictValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.689864\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.687639\n",
      "[3]\tvalid_0's binary_logloss: 0.685551\n",
      "[4]\tvalid_0's binary_logloss: 0.684079\n",
      "[5]\tvalid_0's binary_logloss: 0.68207\n",
      "[6]\tvalid_0's binary_logloss: 0.681018\n",
      "[7]\tvalid_0's binary_logloss: 0.679126\n",
      "[8]\tvalid_0's binary_logloss: 0.678645\n",
      "[9]\tvalid_0's binary_logloss: 0.677536\n",
      "[10]\tvalid_0's binary_logloss: 0.676601\n",
      "[11]\tvalid_0's binary_logloss: 0.675288\n",
      "[12]\tvalid_0's binary_logloss: 0.674611\n",
      "[13]\tvalid_0's binary_logloss: 0.673239\n",
      "[14]\tvalid_0's binary_logloss: 0.672608\n",
      "[15]\tvalid_0's binary_logloss: 0.67165\n",
      "[16]\tvalid_0's binary_logloss: 0.671322\n",
      "[17]\tvalid_0's binary_logloss: 0.671675\n",
      "[18]\tvalid_0's binary_logloss: 0.670968\n",
      "[19]\tvalid_0's binary_logloss: 0.670367\n",
      "[20]\tvalid_0's binary_logloss: 0.669897\n",
      "[21]\tvalid_0's binary_logloss: 0.669244\n",
      "[22]\tvalid_0's binary_logloss: 0.668914\n",
      "[23]\tvalid_0's binary_logloss: 0.66893\n",
      "[24]\tvalid_0's binary_logloss: 0.668694\n",
      "[25]\tvalid_0's binary_logloss: 0.668929\n",
      "[26]\tvalid_0's binary_logloss: 0.668176\n",
      "[27]\tvalid_0's binary_logloss: 0.667369\n",
      "[28]\tvalid_0's binary_logloss: 0.667434\n",
      "[29]\tvalid_0's binary_logloss: 0.667385\n",
      "[30]\tvalid_0's binary_logloss: 0.667033\n",
      "[31]\tvalid_0's binary_logloss: 0.666595\n",
      "[32]\tvalid_0's binary_logloss: 0.665762\n",
      "[33]\tvalid_0's binary_logloss: 0.665907\n",
      "[34]\tvalid_0's binary_logloss: 0.665444\n",
      "[35]\tvalid_0's binary_logloss: 0.665068\n",
      "[36]\tvalid_0's binary_logloss: 0.664742\n",
      "[37]\tvalid_0's binary_logloss: 0.664822\n",
      "[38]\tvalid_0's binary_logloss: 0.664735\n",
      "[39]\tvalid_0's binary_logloss: 0.663972\n",
      "[40]\tvalid_0's binary_logloss: 0.664266\n",
      "[41]\tvalid_0's binary_logloss: 0.662944\n",
      "[42]\tvalid_0's binary_logloss: 0.662567\n",
      "[43]\tvalid_0's binary_logloss: 0.661829\n",
      "[44]\tvalid_0's binary_logloss: 0.661103\n",
      "[45]\tvalid_0's binary_logloss: 0.660435\n",
      "[46]\tvalid_0's binary_logloss: 0.659938\n",
      "[47]\tvalid_0's binary_logloss: 0.660069\n",
      "[48]\tvalid_0's binary_logloss: 0.65949\n",
      "[49]\tvalid_0's binary_logloss: 0.659353\n",
      "[50]\tvalid_0's binary_logloss: 0.658509\n",
      "[51]\tvalid_0's binary_logloss: 0.65745\n",
      "[52]\tvalid_0's binary_logloss: 0.657419\n",
      "[53]\tvalid_0's binary_logloss: 0.656992\n",
      "[54]\tvalid_0's binary_logloss: 0.656647\n",
      "[55]\tvalid_0's binary_logloss: 0.655788\n",
      "[56]\tvalid_0's binary_logloss: 0.655324\n",
      "[57]\tvalid_0's binary_logloss: 0.655066\n",
      "[58]\tvalid_0's binary_logloss: 0.655104\n",
      "[59]\tvalid_0's binary_logloss: 0.65489\n",
      "[60]\tvalid_0's binary_logloss: 0.654462\n",
      "[61]\tvalid_0's binary_logloss: 0.653864\n",
      "[62]\tvalid_0's binary_logloss: 0.653522\n",
      "[63]\tvalid_0's binary_logloss: 0.65311\n",
      "[64]\tvalid_0's binary_logloss: 0.653072\n",
      "[65]\tvalid_0's binary_logloss: 0.652538\n",
      "[66]\tvalid_0's binary_logloss: 0.652536\n",
      "[67]\tvalid_0's binary_logloss: 0.652427\n",
      "[68]\tvalid_0's binary_logloss: 0.652254\n",
      "[69]\tvalid_0's binary_logloss: 0.651939\n",
      "[70]\tvalid_0's binary_logloss: 0.651709\n",
      "[71]\tvalid_0's binary_logloss: 0.651584\n",
      "[72]\tvalid_0's binary_logloss: 0.65157\n",
      "[73]\tvalid_0's binary_logloss: 0.65127\n",
      "[74]\tvalid_0's binary_logloss: 0.650951\n",
      "[75]\tvalid_0's binary_logloss: 0.650815\n",
      "[76]\tvalid_0's binary_logloss: 0.650394\n",
      "[77]\tvalid_0's binary_logloss: 0.651304\n",
      "[78]\tvalid_0's binary_logloss: 0.651392\n",
      "[79]\tvalid_0's binary_logloss: 0.650732\n",
      "[80]\tvalid_0's binary_logloss: 0.650873\n",
      "[81]\tvalid_0's binary_logloss: 0.651172\n",
      "[82]\tvalid_0's binary_logloss: 0.651021\n",
      "[83]\tvalid_0's binary_logloss: 0.650961\n",
      "[84]\tvalid_0's binary_logloss: 0.650793\n",
      "[85]\tvalid_0's binary_logloss: 0.650921\n",
      "[86]\tvalid_0's binary_logloss: 0.651139\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's binary_logloss: 0.650394\n",
      "[[163  87]\n",
      " [102 148]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        BEL       0.62      0.65      0.63       250\n",
      "        DUT       0.63      0.59      0.61       250\n",
      "\n",
      "avg / total       0.62      0.62      0.62       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf=lgb.LGBMClassifier(num_leaves=150,n_jobs=12,\n",
    "                                learning_rate=0.1,n_estimators=1000,silent=False)\n",
    "\n",
    "clf.fit(trainx,trainy,early_stopping_rounds=20,\n",
    "        eval_set=(evalx,evaly),\n",
    "        verbose=True)\n",
    "predictValue=clf.predict(evalx)\n",
    "print(confusion_matrix(evaly,predictValue))\n",
    "print(classification_report(evaly,predictValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29317, 100) 29317\n",
      "(27665, 100) 27665\n"
     ]
    }
   ],
   "source": [
    "methods=[method_name for method_name in dir(modelDUT)]\n",
    "dutMatrix=modelDUT.get_output_matrix()\n",
    "dutWords=modelDUT.get_words()\n",
    "print(dutMatrix.shape,len(dutWords))\n",
    "belMatrix=modelBEL.get_output_matrix()\n",
    "belWords=modelBEL.get_words()\n",
    "print(belMatrix.shape,len(belWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 70)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 70)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 70, 100)       2931700     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 70, 100)       2766500     input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 66, 32)        16032       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 66, 32)        16032       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 16, 32)        0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 16, 32)        0           conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 12, 32)        5152        max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 12, 32)        5152        max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 3, 32)         0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)   (None, 3, 32)         0           conv1d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 32)            8320        max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 32)            8320        max_pooling1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 64)            0           lstm_1[0][0]                     \n",
      "                                                                   lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 512)           33280       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             513         dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 1)             0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 5,791,001\n",
      "Trainable params: 92,801\n",
      "Non-trainable params: 5,698,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"885pt\" viewBox=\"0.00 0.00 910.00 885.00\" width=\"910pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 881)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-881 906,-881 906,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139729311762080 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139729311762080</title>\n",
       "<polygon fill=\"none\" points=\"61,-830.5 61,-876.5 381,-876.5 381,-830.5 61,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"141\" y=\"-849.8\">input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"221,-830.5 221,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"221,-853.5 289,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"289,-830.5 289,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"335\" y=\"-861.3\">(None, 70)</text>\n",
       "<polyline fill=\"none\" points=\"289,-853.5 381,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"335\" y=\"-838.3\">(None, 70)</text>\n",
       "</g>\n",
       "<!-- 139729311762248 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139729311762248</title>\n",
       "<polygon fill=\"none\" points=\"21.5,-747.5 21.5,-793.5 420.5,-793.5 420.5,-747.5 21.5,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-766.8\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"224.5,-747.5 224.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258.5\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"224.5,-770.5 292.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258.5\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"292.5,-747.5 292.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"356.5\" y=\"-778.3\">(None, 70)</text>\n",
       "<polyline fill=\"none\" points=\"292.5,-770.5 420.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"356.5\" y=\"-755.3\">(None, 70, 100)</text>\n",
       "</g>\n",
       "<!-- 139729311762080&#45;&gt;139729311762248 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139729311762080-&gt;139729311762248</title>\n",
       "<path d=\"M221,-830.366C221,-822.152 221,-812.658 221,-803.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-803.607 221,-793.607 217.5,-803.607 224.5,-803.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729521773256 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139729521773256</title>\n",
       "<polygon fill=\"none\" points=\"521,-830.5 521,-876.5 841,-876.5 841,-830.5 521,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"601\" y=\"-849.8\">input_2: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"681,-830.5 681,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"715\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"681,-853.5 749,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"715\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"749,-830.5 749,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"795\" y=\"-861.3\">(None, 70)</text>\n",
       "<polyline fill=\"none\" points=\"749,-853.5 841,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"795\" y=\"-838.3\">(None, 70)</text>\n",
       "</g>\n",
       "<!-- 139729661676512 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139729661676512</title>\n",
       "<polygon fill=\"none\" points=\"481.5,-747.5 481.5,-793.5 880.5,-793.5 880.5,-747.5 481.5,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"583\" y=\"-766.8\">embedding_2: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"684.5,-747.5 684.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718.5\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"684.5,-770.5 752.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718.5\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"752.5,-747.5 752.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"816.5\" y=\"-778.3\">(None, 70)</text>\n",
       "<polyline fill=\"none\" points=\"752.5,-770.5 880.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"816.5\" y=\"-755.3\">(None, 70, 100)</text>\n",
       "</g>\n",
       "<!-- 139729521773256&#45;&gt;139729661676512 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139729521773256-&gt;139729661676512</title>\n",
       "<path d=\"M681,-830.366C681,-822.152 681,-812.658 681,-803.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"684.5,-803.607 681,-793.607 677.5,-803.607 684.5,-803.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729311763984 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139729311763984</title>\n",
       "<polygon fill=\"none\" points=\"47.5,-664.5 47.5,-710.5 394.5,-710.5 394.5,-664.5 47.5,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-683.8\">conv1d_1: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"198.5,-664.5 198.5,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232.5\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"198.5,-687.5 266.5,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232.5\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"266.5,-664.5 266.5,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.5\" y=\"-695.3\">(None, 70, 100)</text>\n",
       "<polyline fill=\"none\" points=\"266.5,-687.5 394.5,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.5\" y=\"-672.3\">(None, 66, 32)</text>\n",
       "</g>\n",
       "<!-- 139729311762248&#45;&gt;139729311763984 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139729311762248-&gt;139729311763984</title>\n",
       "<path d=\"M221,-747.366C221,-739.152 221,-729.658 221,-720.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-720.607 221,-710.607 217.5,-720.607 224.5,-720.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729293937128 -->\n",
       "<g class=\"node\" id=\"node6\"><title>139729293937128</title>\n",
       "<polygon fill=\"none\" points=\"507.5,-664.5 507.5,-710.5 854.5,-710.5 854.5,-664.5 507.5,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"583\" y=\"-683.8\">conv1d_3: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"658.5,-664.5 658.5,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"692.5\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"658.5,-687.5 726.5,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"692.5\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"726.5,-664.5 726.5,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"790.5\" y=\"-695.3\">(None, 70, 100)</text>\n",
       "<polyline fill=\"none\" points=\"726.5,-687.5 854.5,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"790.5\" y=\"-672.3\">(None, 66, 32)</text>\n",
       "</g>\n",
       "<!-- 139729661676512&#45;&gt;139729293937128 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139729661676512-&gt;139729293937128</title>\n",
       "<path d=\"M681,-747.366C681,-739.152 681,-729.658 681,-720.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"684.5,-720.607 681,-710.607 677.5,-720.607 684.5,-720.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729311763144 -->\n",
       "<g class=\"node\" id=\"node7\"><title>139729311763144</title>\n",
       "<polygon fill=\"none\" points=\"0,-581.5 0,-627.5 442,-627.5 442,-581.5 0,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5\" y=\"-600.8\">max_pooling1d_1: MaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"255,-581.5 255,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"289\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"255,-604.5 323,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"289\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"323,-581.5 323,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382.5\" y=\"-612.3\">(None, 66, 32)</text>\n",
       "<polyline fill=\"none\" points=\"323,-604.5 442,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382.5\" y=\"-589.3\">(None, 16, 32)</text>\n",
       "</g>\n",
       "<!-- 139729311763984&#45;&gt;139729311763144 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>139729311763984-&gt;139729311763144</title>\n",
       "<path d=\"M221,-664.366C221,-656.152 221,-646.658 221,-637.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-637.607 221,-627.607 217.5,-637.607 224.5,-637.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729662303704 -->\n",
       "<g class=\"node\" id=\"node8\"><title>139729662303704</title>\n",
       "<polygon fill=\"none\" points=\"460,-581.5 460,-627.5 902,-627.5 902,-581.5 460,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"587.5\" y=\"-600.8\">max_pooling1d_3: MaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"715,-581.5 715,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"749\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"715,-604.5 783,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"749\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"783,-581.5 783,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"842.5\" y=\"-612.3\">(None, 66, 32)</text>\n",
       "<polyline fill=\"none\" points=\"783,-604.5 902,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"842.5\" y=\"-589.3\">(None, 16, 32)</text>\n",
       "</g>\n",
       "<!-- 139729293937128&#45;&gt;139729662303704 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>139729293937128-&gt;139729662303704</title>\n",
       "<path d=\"M681,-664.366C681,-656.152 681,-646.658 681,-637.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"684.5,-637.607 681,-627.607 677.5,-637.607 684.5,-637.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729521722704 -->\n",
       "<g class=\"node\" id=\"node9\"><title>139729521722704</title>\n",
       "<polygon fill=\"none\" points=\"52,-498.5 52,-544.5 390,-544.5 390,-498.5 52,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5\" y=\"-517.8\">conv1d_2: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"203,-498.5 203,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"203,-521.5 271,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"271,-498.5 271,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.5\" y=\"-529.3\">(None, 16, 32)</text>\n",
       "<polyline fill=\"none\" points=\"271,-521.5 390,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.5\" y=\"-506.3\">(None, 12, 32)</text>\n",
       "</g>\n",
       "<!-- 139729311763144&#45;&gt;139729521722704 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>139729311763144-&gt;139729521722704</title>\n",
       "<path d=\"M221,-581.366C221,-573.152 221,-563.658 221,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-554.607 221,-544.607 217.5,-554.607 224.5,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729669107784 -->\n",
       "<g class=\"node\" id=\"node10\"><title>139729669107784</title>\n",
       "<polygon fill=\"none\" points=\"512,-498.5 512,-544.5 850,-544.5 850,-498.5 512,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"587.5\" y=\"-517.8\">conv1d_4: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"663,-498.5 663,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"697\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"663,-521.5 731,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"697\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"731,-498.5 731,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"790.5\" y=\"-529.3\">(None, 16, 32)</text>\n",
       "<polyline fill=\"none\" points=\"731,-521.5 850,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"790.5\" y=\"-506.3\">(None, 12, 32)</text>\n",
       "</g>\n",
       "<!-- 139729662303704&#45;&gt;139729669107784 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>139729662303704-&gt;139729669107784</title>\n",
       "<path d=\"M681,-581.366C681,-573.152 681,-563.658 681,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"684.5,-554.607 681,-544.607 677.5,-554.607 684.5,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729521722144 -->\n",
       "<g class=\"node\" id=\"node11\"><title>139729521722144</title>\n",
       "<polygon fill=\"none\" points=\"0,-415.5 0,-461.5 442,-461.5 442,-415.5 0,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5\" y=\"-434.8\">max_pooling1d_2: MaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"255,-415.5 255,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"289\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"255,-438.5 323,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"289\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"323,-415.5 323,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382.5\" y=\"-446.3\">(None, 12, 32)</text>\n",
       "<polyline fill=\"none\" points=\"323,-438.5 442,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382.5\" y=\"-423.3\">(None, 3, 32)</text>\n",
       "</g>\n",
       "<!-- 139729521722704&#45;&gt;139729521722144 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>139729521722704-&gt;139729521722144</title>\n",
       "<path d=\"M221,-498.366C221,-490.152 221,-480.658 221,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-471.607 221,-461.607 217.5,-471.607 224.5,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729669108736 -->\n",
       "<g class=\"node\" id=\"node12\"><title>139729669108736</title>\n",
       "<polygon fill=\"none\" points=\"460,-415.5 460,-461.5 902,-461.5 902,-415.5 460,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"587.5\" y=\"-434.8\">max_pooling1d_4: MaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"715,-415.5 715,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"749\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"715,-438.5 783,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"749\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"783,-415.5 783,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"842.5\" y=\"-446.3\">(None, 12, 32)</text>\n",
       "<polyline fill=\"none\" points=\"783,-438.5 902,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"842.5\" y=\"-423.3\">(None, 3, 32)</text>\n",
       "</g>\n",
       "<!-- 139729669107784&#45;&gt;139729669108736 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>139729669107784-&gt;139729669108736</title>\n",
       "<path d=\"M681,-498.366C681,-490.152 681,-480.658 681,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"684.5,-471.607 681,-461.607 677.5,-471.607 684.5,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729657012968 -->\n",
       "<g class=\"node\" id=\"node13\"><title>139729657012968</title>\n",
       "<polygon fill=\"none\" points=\"148,-332.5 148,-378.5 442,-378.5 442,-332.5 148,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"206\" y=\"-351.8\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"264,-332.5 264,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"264,-355.5 332,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"332,-332.5 332,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387\" y=\"-363.3\">(None, 3, 32)</text>\n",
       "<polyline fill=\"none\" points=\"332,-355.5 442,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387\" y=\"-340.3\">(None, 32)</text>\n",
       "</g>\n",
       "<!-- 139729521722144&#45;&gt;139729657012968 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>139729521722144-&gt;139729657012968</title>\n",
       "<path d=\"M241.221,-415.366C249.46,-406.348 259.107,-395.788 267.938,-386.121\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"270.643,-388.35 274.804,-378.607 265.475,-383.629 270.643,-388.35\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729550452720 -->\n",
       "<g class=\"node\" id=\"node14\"><title>139729550452720</title>\n",
       "<polygon fill=\"none\" points=\"497,-332.5 497,-378.5 791,-378.5 791,-332.5 497,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"555\" y=\"-351.8\">lstm_2: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"613,-332.5 613,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"647\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"613,-355.5 681,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"647\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"681,-332.5 681,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"736\" y=\"-363.3\">(None, 3, 32)</text>\n",
       "<polyline fill=\"none\" points=\"681,-355.5 791,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"736\" y=\"-340.3\">(None, 32)</text>\n",
       "</g>\n",
       "<!-- 139729669108736&#45;&gt;139729550452720 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>139729669108736-&gt;139729550452720</title>\n",
       "<path d=\"M670.89,-415.366C667.015,-406.884 662.517,-397.037 658.322,-387.853\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"661.437,-386.249 654.098,-378.607 655.07,-389.157 661.437,-386.249\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729551505728 -->\n",
       "<g class=\"node\" id=\"node15\"><title>139729551505728</title>\n",
       "<polygon fill=\"none\" points=\"230.5,-249.5 230.5,-295.5 707.5,-295.5 707.5,-249.5 230.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"341\" y=\"-268.8\">concatenate_1: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"451.5,-249.5 451.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"485.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"451.5,-272.5 519.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"485.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"519.5,-249.5 519.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"613.5\" y=\"-280.3\">[(None, 32), (None, 32)]</text>\n",
       "<polyline fill=\"none\" points=\"519.5,-272.5 707.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"613.5\" y=\"-257.3\">(None, 64)</text>\n",
       "</g>\n",
       "<!-- 139729657012968&#45;&gt;139729551505728 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>139729657012968-&gt;139729551505728</title>\n",
       "<path d=\"M342.546,-332.366C364.125,-322.321 389.812,-310.363 412.38,-299.858\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"413.923,-303 421.511,-295.607 410.968,-296.654 413.923,-303\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729550452720&#45;&gt;139729551505728 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>139729550452720-&gt;139729551505728</title>\n",
       "<path d=\"M596.18,-332.366C574.478,-322.321 548.643,-310.363 525.946,-299.858\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"527.307,-296.631 516.762,-295.607 524.367,-302.984 527.307,-296.631\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729551506456 -->\n",
       "<g class=\"node\" id=\"node16\"><title>139729551506456</title>\n",
       "<polygon fill=\"none\" points=\"320.5,-166.5 320.5,-212.5 617.5,-212.5 617.5,-166.5 320.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"384.5\" y=\"-185.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"448.5,-166.5 448.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"482.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"448.5,-189.5 516.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"482.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"516.5,-166.5 516.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"567\" y=\"-197.3\">(None, 64)</text>\n",
       "<polyline fill=\"none\" points=\"516.5,-189.5 617.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"567\" y=\"-174.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 139729551505728&#45;&gt;139729551506456 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>139729551505728-&gt;139729551506456</title>\n",
       "<path d=\"M469,-249.366C469,-241.152 469,-231.658 469,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"472.5,-222.607 469,-212.607 465.5,-222.607 472.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729551953872 -->\n",
       "<g class=\"node\" id=\"node17\"><title>139729551953872</title>\n",
       "<polygon fill=\"none\" points=\"320.5,-83.5 320.5,-129.5 617.5,-129.5 617.5,-83.5 320.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"384.5\" y=\"-102.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"448.5,-83.5 448.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"482.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"448.5,-106.5 516.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"482.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"516.5,-83.5 516.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"567\" y=\"-114.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"516.5,-106.5 617.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"567\" y=\"-91.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 139729551506456&#45;&gt;139729551953872 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>139729551506456-&gt;139729551953872</title>\n",
       "<path d=\"M469,-166.366C469,-158.152 469,-148.658 469,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"472.5,-139.607 469,-129.607 465.5,-139.607 472.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139729551668672 -->\n",
       "<g class=\"node\" id=\"node18\"><title>139729551668672</title>\n",
       "<polygon fill=\"none\" points=\"300,-0.5 300,-46.5 638,-46.5 638,-0.5 300,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"393.5\" y=\"-19.8\">activation_1: Activation</text>\n",
       "<polyline fill=\"none\" points=\"487,-0.5 487,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"521\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"487,-23.5 555,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"521\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"555,-0.5 555,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"596.5\" y=\"-31.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"555,-23.5 638,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"596.5\" y=\"-8.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 139729551953872&#45;&gt;139729551668672 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>139729551953872-&gt;139729551668672</title>\n",
       "<path d=\"M469,-83.3664C469,-75.1516 469,-65.6579 469,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"472.5,-56.6068 469,-46.6068 465.5,-56.6069 472.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#CNN\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "\n",
    "#https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py \n",
    "\n",
    "def getCNN():\n",
    "    input1=keras.layers.Input(shape=(70,))    \n",
    "    LengthOfInputSequences=70\n",
    "    kernel_size = 5\n",
    "    filters = 32\n",
    "    pool_size = 4\n",
    "    lstm_output_size=32\n",
    "    x1=Embedding(input_dim=dutMatrix.shape[0],\n",
    "                input_length=LengthOfInputSequences,\n",
    "                output_dim=dutMatrix.shape[1],\n",
    "                weights=[dutMatrix],\n",
    "                trainable=False\n",
    "               )(input1) \n",
    "    #x1=Dropout(0.2)(x1)\n",
    "    x1=Conv1D(filters=filters,kernel_size=kernel_size)(x1)\n",
    "    x1=MaxPooling1D(pool_size=pool_size)(x1)\n",
    "    x1=Conv1D(filters=filters,kernel_size=kernel_size)(x1)\n",
    "    x1=MaxPooling1D(pool_size=pool_size)(x1)\n",
    "    #x1=Dropout(0.2)(x1)\n",
    "    x1=LSTM(lstm_output_size)(x1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    input2=keras.layers.Input(shape=(70,))\n",
    "    SizeOfTheVocabulary=len(dutWords)\n",
    "    LengthOfInputSequences=70\n",
    "    x2=Embedding(input_dim=belMatrix.shape[0],\n",
    "                input_length=LengthOfInputSequences,\n",
    "                output_dim=belMatrix.shape[1],\n",
    "                weights=[belMatrix],\n",
    "                trainable=False\n",
    "               )(input2)\n",
    "    #x2=Dropout(0.2)(x2)\n",
    "    x2=Conv1D(filters=filters,kernel_size=kernel_size)(x2)\n",
    "    x2=MaxPooling1D(pool_size=pool_size)(x2)\n",
    "    x2=Conv1D(filters=filters,kernel_size=kernel_size)(x2)\n",
    "    x2=MaxPooling1D(pool_size=pool_size)(x2)\n",
    "    #x2=Dropout(0.2)(x2)\n",
    "    x2=LSTM(lstm_output_size)(x2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    addLayer=keras.layers.Concatenate()([x1,x2])\n",
    "    x1=Dropout(0.2)(x1)\n",
    "    addLayer=Dense(512)(addLayer)\n",
    "    output=Dense(1)(addLayer)\n",
    "    output=Activation('sigmoid')(output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    seq=keras.models.Model(inputs=[input1,input2],outputs=output)\n",
    "    seq.summary()\n",
    "    seq.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return seq\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "crf=getCNN()\n",
    "\n",
    "#plot_model(crf, to_file='lstm-mlp.png',show_shapes=True)\n",
    "SVG(model_to_dot(crf, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". .\n"
     ]
    }
   ],
   "source": [
    "print(dutWords[0],belWords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 3}\n",
      "{4}\n",
      "{1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "sa=set([1,2,3])\n",
    "sb=set([2,4])\n",
    "print(sa-sb)\n",
    "print(sb-sa)\n",
    "print(sa|sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7442\n",
      "5790\n",
      "35107\n"
     ]
    }
   ],
   "source": [
    "print(len(set(dutWords)-set(belWords)))\n",
    "print(len(set(belWords)-set(dutWords)))\n",
    "print(len(set(belWords)|set(dutWords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2vDim=100\n",
    "def textToIndex(wordList,texts,maxlen=70):\n",
    "    rv=[]\n",
    "    wordDict={w:windex for windex,w in enumerate(wordList)}\n",
    "    for t in texts:\n",
    "        lenWords=[]\n",
    "        for w in t.split():\n",
    "            \n",
    "            if w in wordDict:\n",
    "                lenWords.append(wordDict[w])\n",
    "            #else:\n",
    "            #    lenWords.append(0)\n",
    "        while(len(lenWords)<maxlen):\n",
    "            lenWords.append(0)\n",
    "        rv.append(lenWords[:maxlen])\n",
    "    return np.array(rv)\n",
    "\n",
    "trainDuIndex=textToIndex(dutWords,trainxRaw)\n",
    "trainBelIndex=textToIndex(dutWords,trainxRaw)\n",
    "                \n",
    "devDuIndex=textToIndex(dutWords,devxRaw)\n",
    "devBelIndex=textToIndex(dutWords,devxRaw)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 70) [   10   158     1   488    12     0     1    41     9  1449   290    26\n",
      "     6  8840    14    31   402     0   109     9    20 10556  4030   511\n",
      " 13922     0     8    10    71    59    12     7     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(trainDuIndex.shape,trainDuIndex[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnnTrainy=[0 if s=='DUT' else 1 for s in trainy]\n",
    "cnnDevy=[0 if s=='DUT' else 1 for s in evaly]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print (cnnTrainy[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300000 samples, validate on 500 samples\n",
      "Epoch 1/9\n",
      "300000/300000 [==============================] - 19s - loss: 0.6739 - acc: 0.5762 - val_loss: 0.6729 - val_acc: 0.5680\n",
      "Epoch 2/9\n",
      "300000/300000 [==============================] - 18s - loss: 0.6569 - acc: 0.6054 - val_loss: 0.6708 - val_acc: 0.5720\n",
      "Epoch 3/9\n",
      "300000/300000 [==============================] - 18s - loss: 0.6409 - acc: 0.6260 - val_loss: 0.6608 - val_acc: 0.5860\n",
      "Epoch 4/9\n",
      "300000/300000 [==============================] - 18s - loss: 0.6256 - acc: 0.6442 - val_loss: 0.6635 - val_acc: 0.6060\n",
      "Epoch 5/9\n",
      "300000/300000 [==============================] - 18s - loss: 0.6110 - acc: 0.6584 - val_loss: 0.6857 - val_acc: 0.5800\n",
      "Epoch 6/9\n",
      "300000/300000 [==============================] - 18s - loss: 0.5992 - acc: 0.6711 - val_loss: 0.6826 - val_acc: 0.5720\n",
      "Epoch 7/9\n",
      "300000/300000 [==============================] - 18s - loss: 0.5890 - acc: 0.6805 - val_loss: 0.6881 - val_acc: 0.5800\n",
      "Epoch 8/9\n",
      "300000/300000 [==============================] - 18s - loss: 0.5811 - acc: 0.6883 - val_loss: 0.6965 - val_acc: 0.5980\n",
      "Epoch 9/9\n",
      "300000/300000 [==============================] - 18s - loss: 0.5721 - acc: 0.6955 - val_loss: 0.6857 - val_acc: 0.6100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1543fde278>"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.fit([trainDuIndex,trainBelIndex],\n",
    "        cnnTrainy,\n",
    "        validation_data=([devDuIndex,devBelIndex],cnnDevy),epochs=9,batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.58024412]\n",
      " [ 0.43893683]\n",
      " [ 0.61475945]\n",
      " [ 0.2647222 ]\n",
      " [ 0.28319511]\n",
      " [ 0.71227461]\n",
      " [ 0.60813922]\n",
      " [ 0.67268556]\n",
      " [ 0.91106439]\n",
      " [ 0.72659934]]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "0.645648\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "predictValue=crf.predict([devDuIndex,devBelIndex])\n",
    "print(predictValue[:10])\n",
    "print(cnnDevy[:10])\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(cnnDevy, predictValue)\n",
    "#print(thresholds)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "#print(confusion_matrix(cnnDevy,predictValue))\n",
    "#print(classification_report(cnnDevy,predictValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 52.,  53.,  58.,  55.,  50.,  48.,  64.,  45.,  46.,  29.]),\n",
       " array([  5.80069609e-05,   9.99945090e-02,   1.99931011e-01,\n",
       "          2.99867513e-01,   3.99804015e-01,   4.99740517e-01,\n",
       "          5.99677019e-01,   6.99613521e-01,   7.99550023e-01,\n",
       "          8.99486525e-01,   9.99423027e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ny3oqhj/YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f15930e8438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictValue)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
